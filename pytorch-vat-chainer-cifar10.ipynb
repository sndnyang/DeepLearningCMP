{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "gpu = \"1\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ExpUtils import *\n",
    "\n",
    "from torch_func.utils import set_framework_seed, weights_init_uniform, adjust_learning_rate\n",
    "from torch_func.evaluate import evaluate_classifier\n",
    "from torch_func.load_dataset import load_dataset\n",
    "import torch_func.CNN as CNN\n",
    "from torch_func.vat import VAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_func/CNN.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nfunc\n",
    "\n",
    "from torch_func.utils import call_bn\n",
    "\n",
    "\n",
    "class CNN9c(nn.Module):\n",
    "    \"\"\"\n",
    "    Ref: [VAT Chainer](https://github.com/takerum/vat_chainer/blob/master/models/cnn.py)\n",
    "    [VAT TF[(https://github.com/takerum/vat_tf/blob/master/cnn.py)\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(CNN9c, self).__init__()\n",
    "        input_shape = (3, 32, 32)\n",
    "        num_conv = 128\n",
    "        affine = args.affine\n",
    "        self.top_bn = args.top_bn\n",
    "        self.dropout = args.drop\n",
    "        # VAT Chainer CNN use bias, TF don't use bias\n",
    "        self.c1 = nn.Conv2d(input_shape[0], num_conv, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(num_conv, num_conv, 3, 1, 1)\n",
    "        self.c3 = nn.Conv2d(num_conv, num_conv, 3, 1, 1)\n",
    "        self.c4 = nn.Conv2d(num_conv, num_conv * 2, 3, 1, 1)\n",
    "        self.c5 = nn.Conv2d(num_conv * 2, num_conv * 2, 3, 1, 1)\n",
    "        self.c6 = nn.Conv2d(num_conv * 2, num_conv * 2, 3, 1, 1)\n",
    "        self.c7 = nn.Conv2d(num_conv * 2, num_conv * 4, 3, 1, 0)\n",
    "        self.c8 = nn.Conv2d(num_conv * 4, num_conv * 2, 1, 1, 0)\n",
    "        self.c9 = nn.Conv2d(num_conv * 2, 128, 1, 1, 0)\n",
    "        # Chainer default eps=2e-05 [Chainer bn](https://docs.chainer.org/en/stable/reference/generated/chainer.links.BatchNormalization.html)\n",
    "        self.bn1 = nn.BatchNorm2d(num_conv, affine=affine, eps=2e-05)\n",
    "        self.bn2 = nn.BatchNorm2d(num_conv, affine=affine, eps=2e-05)\n",
    "        self.bn3 = nn.BatchNorm2d(num_conv, affine=affine, eps=2e-05)\n",
    "        self.bn4 = nn.BatchNorm2d(num_conv * 2, affine=affine, eps=2e-05)\n",
    "        self.bn5 = nn.BatchNorm2d(num_conv * 2, affine=affine, eps=2e-05)\n",
    "        self.bn6 = nn.BatchNorm2d(num_conv * 2, affine=affine, eps=2e-05)\n",
    "        self.bn7 = nn.BatchNorm2d(num_conv * 4, affine=affine, eps=2e-05)\n",
    "        self.bn8 = nn.BatchNorm2d(num_conv * 2, affine=affine, eps=2e-05)\n",
    "        self.bn9 = nn.BatchNorm2d(num_conv, affine=affine, eps=2e-05)\n",
    "        self.mp1 = nn.MaxPool2d(2, 2)\n",
    "        self.mp2 = nn.MaxPool2d(2, 2)\n",
    "        # Global average pooling, [batch_size, num_conv, ?, ?] -> [batch_size, num_conv, 1, 1]\n",
    "        self.aap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(128, 10)\n",
    "\n",
    "        if self.dropout > 0:\n",
    "            # make sure it's Dropout, not Dropout2d\n",
    "            self.dp1 = nn.Dropout(self.dropout)\n",
    "            self.dp2 = nn.Dropout(self.dropout)\n",
    "        if self.top_bn:\n",
    "            self.bnf = nn.BatchNorm1d(10, affine=affine, eps=2e-05)\n",
    "\n",
    "    def forward(self, x, update_batch_stats=True, return_h=False):\n",
    "        h = x\n",
    "        endpoints = {}\n",
    "        h = self.c1(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn1, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn2, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn3, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.mp1(h)\n",
    "        if self.dropout:\n",
    "            h = self.dp1(h)\n",
    "        endpoints[\"conv_layer0\"] = h\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn4, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn5, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn6, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.mp2(h)\n",
    "        if self.dropout:\n",
    "            h = self.dp2(h)\n",
    "        endpoints[\"conv_layer1\"] = h\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn7, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn8, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn9, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.aap(h)\n",
    "        endpoints[\"fc_layer0\"] = h\n",
    "        output = self.linear(h.view(-1, 128))\n",
    "\n",
    "        if self.top_bn:\n",
    "            output = call_bn(self.bnf, output, update_batch_stats=update_batch_stats)\n",
    "        if return_h:\n",
    "            return output, endpoints\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    set_framework_seed(args.seed, args.debug)\n",
    "\n",
    "    train_l, train_ul, test_set = load_dataset(args.data_dir, valid=False, dataset_seed=args.seed)\n",
    "    wlog(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "    print(\"train data\", train_l.data.sum())\n",
    "    test_set = TensorDataset(torch.FloatTensor(test_set.data), torch.LongTensor(test_set.label))\n",
    "    test_loader = DataLoader(test_set, 100, False)\n",
    "\n",
    "    arch = getattr(CNN, args.arch)\n",
    "    model = arch(args)\n",
    "    if args.debug:\n",
    "        # weights init is based on numpy, so only need np.random.seed()\n",
    "        np.random.seed(args.seed)\n",
    "        model.apply(weights_init_uniform)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    model = model.to(args.device)\n",
    "    model.train()\n",
    "\n",
    "    accs_test = np.zeros(args.num_epochs)\n",
    "    # Define losses.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    vat_criterion = VAT(args.device, eps=args.eps, xi=args.xi, use_ent_min=args.use_entmin, debug=args.debug)\n",
    "    np.random.seed(1)\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for it in range(args.num_batch_it):\n",
    "\n",
    "            x, t = train_l.get(args.batchsize, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "\n",
    "            images = torch.FloatTensor(x).to(args.device)\n",
    "            labels = torch.LongTensor(t).to(args.device)\n",
    "            logits = model(images)\n",
    "\n",
    "            sup_loss = 0\n",
    "            ul_loss = 0\n",
    "\n",
    "            # supervised loss\n",
    "            ce_loss = criterion(logits, labels)\n",
    "            sup_loss += ce_loss\n",
    "\n",
    "            if args.trainer == \"mle\":\n",
    "                total_loss = sup_loss\n",
    "            else:\n",
    "                x_u, _ = train_ul.get(args.batchsize_ul, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "                ul_images = torch.FloatTensor(x_u).to(args.device)\n",
    "                ul_loss = vat_criterion(model, ul_images)\n",
    "                total_loss = sup_loss + ul_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % 1 == 0 and it == args.num_batch_it - 1:\n",
    "            print(\"x: %.5f\" % x.sum())\n",
    "            print(\"pytorch label loss %.7f\" % sup_loss)\n",
    "            n_err, test_loss = evaluate_classifier(model, test_loader, args.device)\n",
    "            acc = 1 - n_err / len(test_set)\n",
    "            print(\"Epoch %d, test acc: %.5f\" % (epoch, acc))\n",
    "            accs_test[epoch] = acc\n",
    "    return accs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"cifar10\"\n",
    "    args.trainer = \"mle\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"CNN9c\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    # a large xi like 1e-4 will hurt the performance on MNIST\n",
    "    # but even xi is 1e-1, the accuracy of CIFAR10 keeps the same\n",
    "    args.xi = 1e-6\n",
    "    args.eps = 10\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.reg_lamb = 1\n",
    "    \n",
    "    args.gpu_id = \"1\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 1\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 32\n",
    "    args.batchsize_ul = 128\n",
    "    args.batchsize_eval = 100\n",
    "    \n",
    "    args.num_epochs = 50\n",
    "    args.num_iter_per_epoch = 125\n",
    "    args.num_batch_it = 125\n",
    "    args.epoch_decay_start = 120\n",
    "    args.method = \"ce\"\n",
    "    args.epsilon = 10\n",
    "    args.extra_lamb = 1\n",
    "    \n",
    "    args.drop = 0.5\n",
    "    args.top_bn = False\n",
    "    args.vis = False\n",
    "    \n",
    "    args.data_dir = os.path.join(\"./data/%s\" % args.dataset)\n",
    "\n",
    "    args.debug = True\n",
    "    args.log_interval = 1\n",
    "    args.affine = False\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    args.device = device\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-14 09:54:51,254 - <ipython-input-3-9b330eb74da9>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: 92.79172\n",
      "pytorch label loss 1.7741661\n",
      "Epoch 0, test acc: 0.34040\n",
      "x: -543.73975\n",
      "pytorch label loss 1.2745868\n",
      "Epoch 1, test acc: 0.45910\n",
      "x: -60.15443\n",
      "pytorch label loss 1.2494316\n",
      "Epoch 2, test acc: 0.46230\n",
      "x: 143.20457\n",
      "pytorch label loss 0.9590910\n",
      "Epoch 3, test acc: 0.42930\n",
      "x: -101.98663\n",
      "pytorch label loss 0.9446954\n",
      "Epoch 4, test acc: 0.51030\n",
      "x: 17.58744\n",
      "pytorch label loss 0.9082096\n",
      "Epoch 5, test acc: 0.53870\n",
      "x: 161.68625\n",
      "pytorch label loss 0.6502841\n",
      "Epoch 6, test acc: 0.59160\n",
      "x: 182.77206\n",
      "pytorch label loss 0.9259822\n",
      "Epoch 7, test acc: 0.61790\n",
      "x: 584.95361\n",
      "pytorch label loss 0.5094450\n",
      "Epoch 8, test acc: 0.56890\n",
      "x: -52.26637\n",
      "pytorch label loss 0.6592411\n",
      "Epoch 9, test acc: 0.55960\n",
      "x: 33.64616\n",
      "pytorch label loss 0.8124924\n",
      "Epoch 10, test acc: 0.60680\n",
      "x: 27.12246\n",
      "pytorch label loss 0.6108854\n",
      "Epoch 11, test acc: 0.61990\n",
      "x: 461.10016\n",
      "pytorch label loss 0.5733246\n",
      "Epoch 12, test acc: 0.61520\n",
      "x: -724.32178\n",
      "pytorch label loss 0.3515632\n",
      "Epoch 13, test acc: 0.61320\n",
      "x: -455.98608\n",
      "pytorch label loss 0.7946752\n",
      "Epoch 14, test acc: 0.65180\n",
      "x: -226.17506\n",
      "pytorch label loss 0.3525383\n",
      "Epoch 15, test acc: 0.65890\n",
      "x: 156.10573\n",
      "pytorch label loss 0.5980788\n",
      "Epoch 16, test acc: 0.57690\n",
      "x: -390.85257\n",
      "pytorch label loss 0.5642402\n",
      "Epoch 17, test acc: 0.64130\n",
      "x: 158.77869\n",
      "pytorch label loss 0.5155116\n",
      "Epoch 18, test acc: 0.67480\n",
      "x: -97.40959\n",
      "pytorch label loss 0.4022542\n",
      "Epoch 19, test acc: 0.66830\n",
      "x: -96.21394\n",
      "pytorch label loss 0.3930346\n",
      "Epoch 20, test acc: 0.65660\n",
      "x: 5.27058\n",
      "pytorch label loss 0.2830288\n",
      "Epoch 21, test acc: 0.66900\n",
      "x: -96.27946\n",
      "pytorch label loss 0.4464243\n",
      "Epoch 22, test acc: 0.63390\n",
      "x: 268.87546\n",
      "pytorch label loss 0.1858309\n",
      "Epoch 23, test acc: 0.60190\n",
      "x: -95.28915\n",
      "pytorch label loss 0.3463184\n",
      "Epoch 24, test acc: 0.63290\n",
      "x: -42.58689\n",
      "pytorch label loss 0.2897654\n",
      "Epoch 25, test acc: 0.64840\n",
      "x: -235.31183\n",
      "pytorch label loss 0.1557859\n",
      "Epoch 26, test acc: 0.61010\n",
      "x: 261.02332\n",
      "pytorch label loss 0.1916292\n",
      "Epoch 27, test acc: 0.65560\n",
      "x: -194.43774\n",
      "pytorch label loss 0.1716989\n",
      "Epoch 28, test acc: 0.67160\n",
      "x: -10.37340\n",
      "pytorch label loss 0.1618700\n",
      "Epoch 29, test acc: 0.68170\n",
      "x: -394.28952\n",
      "pytorch label loss 0.4524801\n",
      "Epoch 30, test acc: 0.67630\n",
      "x: 350.75348\n",
      "pytorch label loss 0.1337716\n",
      "Epoch 31, test acc: 0.68780\n",
      "x: -184.29578\n",
      "pytorch label loss 0.4063381\n",
      "Epoch 32, test acc: 0.67220\n",
      "x: 269.24139\n",
      "pytorch label loss 0.1517863\n",
      "Epoch 33, test acc: 0.67330\n",
      "x: -187.72755\n",
      "pytorch label loss 0.1532056\n",
      "Epoch 34, test acc: 0.65320\n",
      "x: 58.61833\n",
      "pytorch label loss 0.2938914\n",
      "Epoch 35, test acc: 0.62200\n",
      "x: -711.13745\n",
      "pytorch label loss 0.1194347\n",
      "Epoch 36, test acc: 0.69380\n",
      "x: -211.80272\n",
      "pytorch label loss 0.1213789\n",
      "Epoch 37, test acc: 0.70600\n",
      "x: 474.07065\n",
      "pytorch label loss 0.3102584\n",
      "Epoch 38, test acc: 0.65020\n",
      "x: -125.10830\n",
      "pytorch label loss 0.2360323\n",
      "Epoch 39, test acc: 0.69320\n",
      "x: -82.51530\n",
      "pytorch label loss 0.1243458\n",
      "Epoch 40, test acc: 0.71330\n",
      "x: 447.39508\n",
      "pytorch label loss 0.1036129\n",
      "Epoch 41, test acc: 0.67310\n",
      "x: -112.36443\n",
      "pytorch label loss 0.1096173\n",
      "Epoch 42, test acc: 0.66200\n",
      "x: 125.27754\n",
      "pytorch label loss 0.2609760\n",
      "Epoch 43, test acc: 0.64620\n",
      "x: -278.60928\n",
      "pytorch label loss 0.0967185\n",
      "Epoch 44, test acc: 0.65730\n",
      "x: -578.50470\n",
      "pytorch label loss 0.2411626\n",
      "Epoch 45, test acc: 0.67530\n",
      "x: -147.82173\n",
      "pytorch label loss 0.1065237\n",
      "Epoch 46, test acc: 0.66920\n",
      "x: -6.22253\n",
      "pytorch label loss 0.0374555\n",
      "Epoch 47, test acc: 0.69440\n",
      "x: 364.45578\n",
      "pytorch label loss 0.2672820\n",
      "Epoch 48, test acc: 0.67430\n",
      "x: -307.44006\n",
      "pytorch label loss 0.2680858\n",
      "Epoch 49, test acc: 0.69290\n"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "arg = parse_args()\n",
    "arg.drop = 0.5\n",
    "acc_list = train(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-14 10:02:46,946 - <ipython-input-3-9b330eb74da9>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: 92.79172\n",
      "pytorch label loss 1.5453452\n",
      "Epoch 0, test acc: 0.33040\n",
      "x: -543.73975\n",
      "pytorch label loss 1.1830873\n",
      "Epoch 1, test acc: 0.30080\n",
      "x: -60.15443\n",
      "pytorch label loss 1.1114335\n",
      "Epoch 2, test acc: 0.45430\n",
      "x: 143.20457\n",
      "pytorch label loss 0.8631876\n",
      "Epoch 3, test acc: 0.52250\n",
      "x: -101.98663\n",
      "pytorch label loss 0.7341555\n",
      "Epoch 4, test acc: 0.57190\n",
      "x: 17.58744\n",
      "pytorch label loss 0.5021304\n",
      "Epoch 5, test acc: 0.52320\n",
      "x: 161.68625\n",
      "pytorch label loss 0.4496548\n",
      "Epoch 6, test acc: 0.42930\n",
      "x: 182.77206\n",
      "pytorch label loss 0.7085162\n",
      "Epoch 7, test acc: 0.51320\n",
      "x: 584.95361\n",
      "pytorch label loss 0.2585516\n",
      "Epoch 8, test acc: 0.58410\n",
      "x: -52.26637\n",
      "pytorch label loss 0.3343198\n",
      "Epoch 9, test acc: 0.54150\n",
      "x: 33.64616\n",
      "pytorch label loss 0.2910812\n",
      "Epoch 10, test acc: 0.55910\n",
      "x: 27.12246\n",
      "pytorch label loss 0.2502673\n",
      "Epoch 11, test acc: 0.51170\n",
      "x: 461.10016\n",
      "pytorch label loss 0.1484381\n",
      "Epoch 12, test acc: 0.57940\n",
      "x: -724.32178\n",
      "pytorch label loss 0.0978856\n",
      "Epoch 13, test acc: 0.61950\n",
      "x: -455.98608\n",
      "pytorch label loss 0.2597702\n",
      "Epoch 14, test acc: 0.45020\n",
      "x: -226.17506\n",
      "pytorch label loss 0.1020753\n",
      "Epoch 15, test acc: 0.57590\n",
      "x: 156.10573\n",
      "pytorch label loss 0.0697889\n",
      "Epoch 16, test acc: 0.58020\n",
      "x: -390.85257\n",
      "pytorch label loss 0.1942552\n",
      "Epoch 17, test acc: 0.62180\n",
      "x: 158.77869\n",
      "pytorch label loss 0.1123758\n",
      "Epoch 18, test acc: 0.54250\n",
      "x: -97.40959\n",
      "pytorch label loss 0.0648898\n",
      "Epoch 19, test acc: 0.56710\n",
      "x: -96.21394\n",
      "pytorch label loss 0.0440814\n",
      "Epoch 20, test acc: 0.58650\n",
      "x: 5.27058\n",
      "pytorch label loss 0.0844173\n",
      "Epoch 21, test acc: 0.38250\n",
      "x: -96.27946\n",
      "pytorch label loss 0.0497079\n",
      "Epoch 22, test acc: 0.57150\n",
      "x: 268.87546\n",
      "pytorch label loss 0.0628709\n",
      "Epoch 23, test acc: 0.45970\n",
      "x: -95.28915\n",
      "pytorch label loss 0.0698111\n",
      "Epoch 24, test acc: 0.60990\n",
      "x: -42.58689\n",
      "pytorch label loss 0.0426205\n",
      "Epoch 25, test acc: 0.57610\n",
      "x: -235.31183\n",
      "pytorch label loss 0.1150138\n",
      "Epoch 26, test acc: 0.62300\n",
      "x: 261.02332\n",
      "pytorch label loss 0.0390713\n",
      "Epoch 27, test acc: 0.65480\n",
      "x: -194.43774\n",
      "pytorch label loss 0.0299536\n",
      "Epoch 28, test acc: 0.59540\n",
      "x: -10.37340\n",
      "pytorch label loss 0.0392655\n",
      "Epoch 29, test acc: 0.59720\n",
      "x: -394.28952\n",
      "pytorch label loss 0.0469495\n",
      "Epoch 30, test acc: 0.58500\n",
      "x: 350.75348\n",
      "pytorch label loss 0.0267291\n",
      "Epoch 31, test acc: 0.55340\n",
      "x: -184.29578\n",
      "pytorch label loss 0.0333391\n",
      "Epoch 32, test acc: 0.62740\n",
      "x: 269.24139\n",
      "pytorch label loss 0.0173930\n",
      "Epoch 33, test acc: 0.55960\n",
      "x: -187.72755\n",
      "pytorch label loss 0.0945657\n",
      "Epoch 34, test acc: 0.63310\n",
      "x: 58.61833\n",
      "pytorch label loss 0.0819154\n",
      "Epoch 35, test acc: 0.54760\n",
      "x: -711.13745\n",
      "pytorch label loss 0.0480759\n",
      "Epoch 36, test acc: 0.56010\n",
      "x: -211.80272\n",
      "pytorch label loss 0.0400231\n",
      "Epoch 37, test acc: 0.58440\n",
      "x: 474.07065\n",
      "pytorch label loss 0.0890699\n",
      "Epoch 38, test acc: 0.54700\n",
      "x: -125.10830\n",
      "pytorch label loss 0.0070468\n",
      "Epoch 39, test acc: 0.66680\n",
      "x: -82.51530\n",
      "pytorch label loss 0.0048476\n",
      "Epoch 40, test acc: 0.59490\n",
      "x: 447.39508\n",
      "pytorch label loss 0.0029023\n",
      "Epoch 41, test acc: 0.66640\n",
      "x: -112.36443\n",
      "pytorch label loss 0.0110159\n",
      "Epoch 42, test acc: 0.57260\n",
      "x: 125.27754\n",
      "pytorch label loss 0.1439381\n",
      "Epoch 43, test acc: 0.65500\n",
      "x: -278.60928\n",
      "pytorch label loss 0.0236681\n",
      "Epoch 44, test acc: 0.52400\n",
      "x: -578.50470\n",
      "pytorch label loss 0.0086063\n",
      "Epoch 45, test acc: 0.55950\n",
      "x: -147.82173\n",
      "pytorch label loss 0.0262766\n",
      "Epoch 46, test acc: 0.51140\n",
      "x: -6.22253\n",
      "pytorch label loss 0.0179845\n",
      "Epoch 47, test acc: 0.64060\n",
      "x: 364.45578\n",
      "pytorch label loss 0.0499844\n",
      "Epoch 48, test acc: 0.51630\n",
      "x: -307.44006\n",
      "pytorch label loss 0.0532723\n",
      "Epoch 49, test acc: 0.61780\n"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "arg.drop = 0.0\n",
    "acc_05_list = train(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 10:50:43,892 - <ipython-input-15-19c32b36ef1a>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: -562.97064, x_u: 143.50818\n",
      "pytorch label loss 2.2606215\n",
      "x: 35.14433, x_u: 135.20184\n",
      "pytorch label loss 2.4189808\n",
      "x: -17.62558, x_u: 601.27460\n",
      "pytorch label loss 2.2952368\n",
      "x: -532.37256, x_u: 1750.79065\n",
      "pytorch label loss 2.4121597\n",
      "x: -70.22945, x_u: 347.33923\n",
      "pytorch label loss 2.3319638\n",
      "x: 56.61665, x_u: -1213.71118\n",
      "pytorch label loss 2.3965092\n",
      "x: -627.13055, x_u: -55.53556\n",
      "pytorch label loss 2.4148233\n",
      "x: -178.30162, x_u: -360.91476\n",
      "pytorch label loss 2.4614964\n",
      "x: -289.32187, x_u: 915.18994\n",
      "pytorch label loss 2.4101825\n",
      "x: -111.70967, x_u: -422.41815\n",
      "pytorch label loss 2.3997018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 10:50:47,094 - <ipython-input-15-19c32b36ef1a>[line:61]: Epoch: 0 Train Loss: 2.3997 ce: 2.39970, vat: 0.00000, test loss: 2.40626, test acc: 0.1102\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 10:52:04,452 - <ipython-input-15-19c32b36ef1a>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: -562.97064, x_u: 143.50818\n",
      "pytorch label loss 2.2750621\n",
      "x: 35.14433, x_u: 135.20184\n",
      "pytorch label loss 2.3824730\n",
      "x: -17.62558, x_u: 601.27460\n",
      "pytorch label loss 2.2932196\n",
      "x: -532.37256, x_u: 1750.79065\n",
      "pytorch label loss 2.3744545\n",
      "x: -70.22945, x_u: 347.33923\n",
      "pytorch label loss 2.3137186\n",
      "x: 56.61665, x_u: -1213.71118\n",
      "pytorch label loss 2.3670661\n",
      "x: -627.13055, x_u: -55.53556\n",
      "pytorch label loss 2.4136870\n",
      "x: -178.30162, x_u: -360.91476\n",
      "pytorch label loss 2.4105940\n",
      "x: -289.32187, x_u: 915.18994\n",
      "pytorch label loss 2.4110556\n",
      "x: -111.70967, x_u: -422.41815\n",
      "pytorch label loss 2.3749356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 10:52:07,670 - <ipython-input-15-19c32b36ef1a>[line:61]: Epoch: 0 Train Loss: 2.3749 ce: 2.37494, vat: 0.00000, test loss: 2.41538, test acc: 0.1142\n"
     ]
    }
   ],
   "source": [
    "arg.drop = 0.5\n",
    "\n",
    "acc_list = train(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 11:13:04,190 - <ipython-input-18-a06c9cede2be>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: -562.97064, x_u: 143.50818\n",
      "pytorch label loss 2.4099069\n",
      "x: 35.14433, x_u: 135.20184\n",
      "pytorch label loss 2.2265427\n",
      "x: -17.62558, x_u: 601.27460\n",
      "pytorch label loss 2.1924956\n",
      "x: -532.37256, x_u: 1750.79065\n",
      "pytorch label loss 2.3408909\n",
      "x: -70.22945, x_u: 347.33923\n",
      "pytorch label loss 2.2128272\n",
      "x: 56.61665, x_u: -1213.71118\n",
      "pytorch label loss 2.3133676\n",
      "x: -627.13055, x_u: -55.53556\n",
      "pytorch label loss 2.2681317\n",
      "x: -178.30162, x_u: -360.91476\n",
      "pytorch label loss 2.4872472\n",
      "x: -289.32187, x_u: 915.18994\n",
      "pytorch label loss 2.4417503\n",
      "x: -111.70967, x_u: -422.41815\n",
      "pytorch label loss 2.3514400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 11:13:06,699 - <ipython-input-18-a06c9cede2be>[line:61]: Epoch: 0 Train Loss: 2.3514 ce: 2.35144, vat: 0.00000, test loss: 2.31827, test acc: 0.1229\n"
     ]
    }
   ],
   "source": [
    "arg.drop = 0.5\n",
    "\n",
    "acc_list = train(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 11:44:08,047 - <ipython-input-37-ba4cbffe6c01>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: -562.97064, x_u: 143.50818\n",
      "pytorch label loss 2.2750621\n",
      "test acc: 0.10000\n",
      "x: -126.36263, x_u: 345.36444\n",
      "pytorch label loss 2.1643209\n",
      "test acc: 0.09940\n",
      "x: 399.73303, x_u: -8.43586\n",
      "pytorch label loss 2.1929793\n",
      "test acc: 0.19810\n",
      "x: 226.79512, x_u: 424.78540\n",
      "pytorch label loss 2.0780790\n",
      "test acc: 0.17450\n",
      "x: 94.59266, x_u: -622.94928\n",
      "pytorch label loss 1.5659134\n",
      "test acc: 0.21560\n",
      "x: 94.60011, x_u: -644.31439\n",
      "pytorch label loss 1.4967724\n",
      "test acc: 0.28310\n",
      "x: 196.49091, x_u: 61.31622\n",
      "pytorch label loss 1.7410563\n",
      "test acc: 0.19080\n",
      "x: -178.64716, x_u: -353.65878\n",
      "pytorch label loss 1.5666623\n",
      "test acc: 0.17340\n",
      "x: 320.46967, x_u: -612.20245\n",
      "pytorch label loss 1.8934345\n",
      "test acc: 0.36530\n",
      "x: 19.11314, x_u: -784.69800\n",
      "pytorch label loss 1.7507118\n",
      "test acc: 0.26760\n"
     ]
    }
   ],
   "source": [
    "arg.drop = 0.5\n",
    "acc_list = train(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 11:44:50,717 - <ipython-input-37-ba4cbffe6c01>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n",
      "x: -562.97064, x_u: 143.50818\n",
      "pytorch label loss 2.2606215\n",
      "test acc: 0.10000\n",
      "x: -126.36263, x_u: 345.36444\n",
      "pytorch label loss 2.1652429\n",
      "test acc: 0.13800\n",
      "x: 399.73303, x_u: -8.43586\n",
      "pytorch label loss 2.0234582\n",
      "test acc: 0.15870\n",
      "x: 226.79512, x_u: 424.78540\n",
      "pytorch label loss 1.9177381\n",
      "test acc: 0.27860\n",
      "x: 94.59266, x_u: -622.94928\n",
      "pytorch label loss 1.6000139\n",
      "test acc: 0.12220\n",
      "x: 94.60011, x_u: -644.31439\n",
      "pytorch label loss 1.5202812\n",
      "test acc: 0.31760\n",
      "x: 196.49091, x_u: 61.31622\n",
      "pytorch label loss 1.5974302\n",
      "test acc: 0.30010\n",
      "x: -178.64716, x_u: -353.65878\n",
      "pytorch label loss 1.5880890\n",
      "test acc: 0.22190\n",
      "x: 320.46967, x_u: -612.20245\n",
      "pytorch label loss 1.8056109\n",
      "test acc: 0.33720\n",
      "x: 19.11314, x_u: -784.69800\n",
      "pytorch label loss 1.6236304\n",
      "test acc: 0.30490\n"
     ]
    }
   ],
   "source": [
    "arg.drop = 0.0\n",
    "acc_list = train(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    set_framework_seed(args.seed, args.debug)\n",
    "\n",
    "    train_l, train_ul, test_set = load_dataset(args.data_dir, valid=False, dataset_seed=args.seed)\n",
    "    wlog(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "    print(\"train data\", train_l.data.sum())\n",
    "    test_set = TensorDataset(torch.FloatTensor(test_set.data), torch.LongTensor(test_set.label))\n",
    "    test_loader = DataLoader(test_set, 128, False)\n",
    "\n",
    "    arch = getattr(CNN, args.arch)\n",
    "    model = arch(args)\n",
    "    if args.debug:\n",
    "        # weights init is based on numpy, so only need np.random.seed()\n",
    "        np.random.seed(args.seed)\n",
    "        model.apply(weights_init_normal)\n",
    "\n",
    "    optimizer = optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    model = model.to(args.device)\n",
    "    model.train()\n",
    "\n",
    "    # Define losses.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    vat_criterion = VAT(args.device, eps=args.eps, xi=args.xi, use_ent_min=args.ent_min, debug=args.debug)\n",
    "\n",
    "    start = time.time()\n",
    "    acc_list = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "\n",
    "        sum_loss_l = 0\n",
    "        sum_loss_ul = 0\n",
    "        for it in range(args.num_batch_it):\n",
    "            x, t = train_l.get(args.batchsize, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            x_u, _ = train_ul.get(args.batchsize_ul, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            \n",
    "            images = torch.FloatTensor(x).to(args.device)\n",
    "            labels = torch.LongTensor(t).to(args.device)\n",
    "            ul_images = torch.FloatTensor(x_u).to(args.device)\n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            sup_loss = 0\n",
    "            ul_loss = 0\n",
    "\n",
    "            # supervised loss\n",
    "            ce_loss = criterion(logits, labels)\n",
    "            sup_loss += ce_loss\n",
    "\n",
    "            if args.trainer == \"mle\":\n",
    "                total_loss = sup_loss\n",
    "            else:\n",
    "                ul_loss = vat_criterion(model, ul_images)\n",
    "                total_loss = sup_loss + ul_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((epoch % args.log_interval) == 0 and it == args.num_batch_it - 1) or (args.debug and it < 50):\n",
    "                n_err, test_loss = evaluate_classifier(model, test_loader, args.device)\n",
    "                acc = 1 - n_err / len(test_set)\n",
    "                wlog(\"Epoch: %d Train Loss: %.4f ce: %.5f, vat: %.5f, test loss: %.5f, test acc: %.4f\" % (epoch, total_loss, ce_loss, ul_loss, test_loss, acc))\n",
    "\n",
    "                pred_y = torch.max(logits, dim=1)[1]\n",
    "                train_acc = 1.0 * torch.sum(pred_y == labels).item() / pred_y.shape[0]\n",
    "                acc_list.append(acc)\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, args)\n",
    "        if (epoch % args.log_interval) == 0:\n",
    "            wlog(\"learning rate %f\" % lr)\n",
    "            if args.vis:\n",
    "                args.writer.add_scalar(\"optimizer/learning_rate\", lr, epoch)\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train with dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:35:29,387 - <ipython-input-9-672506e2b4d7>[line:59]: args in this experiment ('affine', False)\n",
      "('alpha', 1)\n",
      "('arch', 'CNN9c')\n",
      "('aug_flip', False)\n",
      "('aug_trans', False)\n",
      "('batchsize', 32)\n",
      "('batchsize_eval', 100)\n",
      "('batchsize_ul', 128)\n",
      "('cuda', True)\n",
      "('data_dir', './data/cifar10')\n",
      "('dataset', 'cifar10')\n",
      "('dataset_seed', 1)\n",
      "('debug', False)\n",
      "('device', device(type='cuda'))\n",
      "('drop', 0.5)\n",
      "('ent_min', False)\n",
      "('epoch_decay_start', 120)\n",
      "('eps', 10)\n",
      "('epsilon', 10)\n",
      "('eval_freq', 1)\n",
      "('exp', 'avg')\n",
      "('extra_lamb', 1)\n",
      "('gpu_id', '1')\n",
      "('iterations', 1000)\n",
      "('k', 1)\n",
      "('log_dir', 'log')\n",
      "('log_interval', 1)\n",
      "('lr', 0.001)\n",
      "('method', 'vat')\n",
      "('mom1', 0.9)\n",
      "('mom2', 0.5)\n",
      "('n_categories', 10)\n",
      "('no_cuda', False)\n",
      "('num_batch_it', 400)\n",
      "('num_epochs', 200)\n",
      "('num_iter_per_epoch', 400)\n",
      "('reg_lamb', 1)\n",
      "('seed', 1)\n",
      "('size', 100)\n",
      "('snapshot_freq', 20)\n",
      "('top_bn', False)\n",
      "('trainer', 'mle')\n",
      "('use_entmin', False)\n",
      "('validation', False)\n",
      "('vis', False)\n",
      "('xi', 1e-06)\n",
      "2019-05-12 22:35:31,136 - <ipython-input-7-bdc23bc4b139>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:35:43,542 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 0 Train Loss: 1.6884 ce: 1.68835, vat: 0.00000, test loss: 1.78299, test acc: 0.3209\n",
      "2019-05-12 22:35:43,544 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:35:50,309 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 1 Train Loss: 1.5572 ce: 1.55722, vat: 0.00000, test loss: 1.42607, test acc: 0.4810\n",
      "2019-05-12 22:35:50,310 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:35:57,267 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 2 Train Loss: 1.1066 ce: 1.10659, vat: 0.00000, test loss: 1.31845, test acc: 0.5136\n",
      "2019-05-12 22:35:57,269 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:04,277 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 3 Train Loss: 1.0479 ce: 1.04791, vat: 0.00000, test loss: 1.26523, test acc: 0.5492\n",
      "2019-05-12 22:36:04,282 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:11,156 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 4 Train Loss: 0.7223 ce: 0.72234, vat: 0.00000, test loss: 1.10647, test acc: 0.6124\n",
      "2019-05-12 22:36:11,158 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:17,620 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 5 Train Loss: 0.5529 ce: 0.55289, vat: 0.00000, test loss: 1.20353, test acc: 0.5884\n",
      "2019-05-12 22:36:17,621 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:24,010 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 6 Train Loss: 0.4490 ce: 0.44897, vat: 0.00000, test loss: 1.34984, test acc: 0.5405\n",
      "2019-05-12 22:36:24,012 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:30,488 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 7 Train Loss: 0.5783 ce: 0.57825, vat: 0.00000, test loss: 1.18581, test acc: 0.6139\n",
      "2019-05-12 22:36:30,490 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:36,928 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 8 Train Loss: 0.7474 ce: 0.74737, vat: 0.00000, test loss: 1.79960, test acc: 0.4972\n",
      "2019-05-12 22:36:36,929 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:43,565 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 9 Train Loss: 0.3126 ce: 0.31263, vat: 0.00000, test loss: 1.75643, test acc: 0.5378\n",
      "2019-05-12 22:36:43,566 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:49,933 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 10 Train Loss: 0.3407 ce: 0.34070, vat: 0.00000, test loss: 1.69152, test acc: 0.5737\n",
      "2019-05-12 22:36:49,935 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:36:56,336 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 11 Train Loss: 0.2807 ce: 0.28066, vat: 0.00000, test loss: 1.48304, test acc: 0.5769\n",
      "2019-05-12 22:36:56,338 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:02,847 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 12 Train Loss: 0.2042 ce: 0.20421, vat: 0.00000, test loss: 1.49769, test acc: 0.6024\n",
      "2019-05-12 22:37:02,849 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:09,267 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 13 Train Loss: 0.3643 ce: 0.36426, vat: 0.00000, test loss: 1.48214, test acc: 0.6169\n",
      "2019-05-12 22:37:09,271 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:15,829 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 14 Train Loss: 0.1837 ce: 0.18367, vat: 0.00000, test loss: 1.29856, test acc: 0.6453\n",
      "2019-05-12 22:37:15,830 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:22,185 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 15 Train Loss: 0.1152 ce: 0.11521, vat: 0.00000, test loss: 1.21995, test acc: 0.6691\n",
      "2019-05-12 22:37:22,186 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:28,625 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 16 Train Loss: 0.1275 ce: 0.12754, vat: 0.00000, test loss: 1.47347, test acc: 0.6379\n",
      "2019-05-12 22:37:28,627 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:35,111 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 17 Train Loss: 0.0970 ce: 0.09702, vat: 0.00000, test loss: 1.73676, test acc: 0.6025\n",
      "2019-05-12 22:37:35,113 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:41,692 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 18 Train Loss: 0.1703 ce: 0.17027, vat: 0.00000, test loss: 1.38081, test acc: 0.6579\n",
      "2019-05-12 22:37:41,693 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:48,167 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 19 Train Loss: 0.0877 ce: 0.08766, vat: 0.00000, test loss: 1.35100, test acc: 0.6560\n",
      "2019-05-12 22:37:48,169 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:37:54,728 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 20 Train Loss: 0.1428 ce: 0.14282, vat: 0.00000, test loss: 1.48115, test acc: 0.6479\n",
      "2019-05-12 22:37:54,730 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:01,339 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 21 Train Loss: 0.1785 ce: 0.17848, vat: 0.00000, test loss: 1.65950, test acc: 0.6300\n",
      "2019-05-12 22:38:01,340 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:07,860 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 22 Train Loss: 0.0631 ce: 0.06307, vat: 0.00000, test loss: 1.46700, test acc: 0.6688\n",
      "2019-05-12 22:38:07,862 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:14,484 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 23 Train Loss: 0.0402 ce: 0.04023, vat: 0.00000, test loss: 1.41330, test acc: 0.6663\n",
      "2019-05-12 22:38:14,485 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:20,913 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 24 Train Loss: 0.1258 ce: 0.12579, vat: 0.00000, test loss: 2.02934, test acc: 0.5870\n",
      "2019-05-12 22:38:20,915 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:27,290 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 25 Train Loss: 0.0402 ce: 0.04025, vat: 0.00000, test loss: 1.66161, test acc: 0.6450\n",
      "2019-05-12 22:38:27,291 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:33,757 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 26 Train Loss: 0.0820 ce: 0.08200, vat: 0.00000, test loss: 1.64255, test acc: 0.6615\n",
      "2019-05-12 22:38:33,758 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:40,187 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 27 Train Loss: 0.0689 ce: 0.06887, vat: 0.00000, test loss: 1.90571, test acc: 0.6322\n",
      "2019-05-12 22:38:40,188 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:46,621 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 28 Train Loss: 0.0301 ce: 0.03005, vat: 0.00000, test loss: 1.54445, test acc: 0.6759\n",
      "2019-05-12 22:38:46,622 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:52,918 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 29 Train Loss: 0.0951 ce: 0.09515, vat: 0.00000, test loss: 1.46985, test acc: 0.6801\n",
      "2019-05-12 22:38:52,920 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:38:59,156 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 30 Train Loss: 0.0264 ce: 0.02635, vat: 0.00000, test loss: 1.67203, test acc: 0.6471\n",
      "2019-05-12 22:38:59,157 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:05,425 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 31 Train Loss: 0.0210 ce: 0.02103, vat: 0.00000, test loss: 1.60988, test acc: 0.6550\n",
      "2019-05-12 22:39:05,427 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:11,706 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 32 Train Loss: 0.0507 ce: 0.05065, vat: 0.00000, test loss: 1.56508, test acc: 0.6661\n",
      "2019-05-12 22:39:11,708 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:39:17,963 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 33 Train Loss: 0.0163 ce: 0.01634, vat: 0.00000, test loss: 1.79781, test acc: 0.6529\n",
      "2019-05-12 22:39:17,965 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:24,244 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 34 Train Loss: 0.0167 ce: 0.01667, vat: 0.00000, test loss: 1.55125, test acc: 0.6869\n",
      "2019-05-12 22:39:24,245 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:30,508 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 35 Train Loss: 0.0232 ce: 0.02317, vat: 0.00000, test loss: 2.08963, test acc: 0.6192\n",
      "2019-05-12 22:39:30,509 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:36,834 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 36 Train Loss: 0.0839 ce: 0.08388, vat: 0.00000, test loss: 1.53061, test acc: 0.6839\n",
      "2019-05-12 22:39:36,835 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:43,154 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 37 Train Loss: 0.0872 ce: 0.08719, vat: 0.00000, test loss: 1.68889, test acc: 0.6610\n",
      "2019-05-12 22:39:43,155 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:49,460 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 38 Train Loss: 0.1762 ce: 0.17619, vat: 0.00000, test loss: 1.57093, test acc: 0.6709\n",
      "2019-05-12 22:39:49,461 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:39:55,764 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 39 Train Loss: 0.0170 ce: 0.01699, vat: 0.00000, test loss: 1.70916, test acc: 0.6676\n",
      "2019-05-12 22:39:55,765 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:02,041 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 40 Train Loss: 0.1492 ce: 0.14920, vat: 0.00000, test loss: 1.70447, test acc: 0.6589\n",
      "2019-05-12 22:40:02,042 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:08,384 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 41 Train Loss: 0.1162 ce: 0.11617, vat: 0.00000, test loss: 1.84980, test acc: 0.6524\n",
      "2019-05-12 22:40:08,385 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:14,756 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 42 Train Loss: 0.0426 ce: 0.04257, vat: 0.00000, test loss: 1.76031, test acc: 0.6683\n",
      "2019-05-12 22:40:14,757 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:21,082 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 43 Train Loss: 0.0028 ce: 0.00277, vat: 0.00000, test loss: 1.54665, test acc: 0.6914\n",
      "2019-05-12 22:40:21,083 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:27,426 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 44 Train Loss: 0.0327 ce: 0.03270, vat: 0.00000, test loss: 2.19028, test acc: 0.6354\n",
      "2019-05-12 22:40:27,427 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:33,772 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 45 Train Loss: 0.0026 ce: 0.00257, vat: 0.00000, test loss: 1.59114, test acc: 0.6834\n",
      "2019-05-12 22:40:33,774 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:40,161 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 46 Train Loss: 0.0168 ce: 0.01684, vat: 0.00000, test loss: 1.84782, test acc: 0.6511\n",
      "2019-05-12 22:40:40,163 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:46,557 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 47 Train Loss: 0.0046 ce: 0.00458, vat: 0.00000, test loss: 1.68379, test acc: 0.6798\n",
      "2019-05-12 22:40:46,558 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:52,924 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 48 Train Loss: 0.0372 ce: 0.03724, vat: 0.00000, test loss: 1.65225, test acc: 0.6686\n",
      "2019-05-12 22:40:52,926 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:40:59,306 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 49 Train Loss: 0.0842 ce: 0.08423, vat: 0.00000, test loss: 1.65707, test acc: 0.6664\n",
      "2019-05-12 22:40:59,307 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:05,566 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 50 Train Loss: 0.0041 ce: 0.00408, vat: 0.00000, test loss: 1.58041, test acc: 0.6922\n",
      "2019-05-12 22:41:05,567 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:12,019 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 51 Train Loss: 0.3980 ce: 0.39804, vat: 0.00000, test loss: 1.86323, test acc: 0.6668\n",
      "2019-05-12 22:41:12,020 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:18,364 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 52 Train Loss: 0.0314 ce: 0.03140, vat: 0.00000, test loss: 1.72877, test acc: 0.6869\n",
      "2019-05-12 22:41:18,366 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:24,706 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 53 Train Loss: 0.0066 ce: 0.00658, vat: 0.00000, test loss: 1.64601, test acc: 0.6883\n",
      "2019-05-12 22:41:24,708 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:31,060 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 54 Train Loss: 0.0056 ce: 0.00559, vat: 0.00000, test loss: 1.71410, test acc: 0.6799\n",
      "2019-05-12 22:41:31,061 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:37,504 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 55 Train Loss: 0.0023 ce: 0.00231, vat: 0.00000, test loss: 1.91888, test acc: 0.6613\n",
      "2019-05-12 22:41:37,505 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:43,915 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 56 Train Loss: 0.0350 ce: 0.03504, vat: 0.00000, test loss: 1.79639, test acc: 0.6641\n",
      "2019-05-12 22:41:43,916 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:50,317 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 57 Train Loss: 0.0068 ce: 0.00678, vat: 0.00000, test loss: 1.85190, test acc: 0.6638\n",
      "2019-05-12 22:41:50,318 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:41:56,632 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 58 Train Loss: 0.0309 ce: 0.03085, vat: 0.00000, test loss: 1.82626, test acc: 0.6638\n",
      "2019-05-12 22:41:56,634 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:02,914 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 59 Train Loss: 0.0290 ce: 0.02903, vat: 0.00000, test loss: 1.71100, test acc: 0.6902\n",
      "2019-05-12 22:42:02,916 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:09,257 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 60 Train Loss: 0.0279 ce: 0.02791, vat: 0.00000, test loss: 1.71756, test acc: 0.6774\n",
      "2019-05-12 22:42:09,259 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:15,701 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 61 Train Loss: 0.0097 ce: 0.00966, vat: 0.00000, test loss: 1.85396, test acc: 0.6797\n",
      "2019-05-12 22:42:15,703 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:22,135 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 62 Train Loss: 0.0801 ce: 0.08008, vat: 0.00000, test loss: 1.87942, test acc: 0.6683\n",
      "2019-05-12 22:42:22,136 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:28,485 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 63 Train Loss: 0.0262 ce: 0.02620, vat: 0.00000, test loss: 1.84652, test acc: 0.6718\n",
      "2019-05-12 22:42:28,486 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:34,888 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 64 Train Loss: 0.0374 ce: 0.03736, vat: 0.00000, test loss: 1.79662, test acc: 0.6801\n",
      "2019-05-12 22:42:34,889 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:41,235 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 65 Train Loss: 0.0118 ce: 0.01176, vat: 0.00000, test loss: 1.62461, test acc: 0.6973\n",
      "2019-05-12 22:42:41,237 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:42:47,626 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 66 Train Loss: 0.0045 ce: 0.00445, vat: 0.00000, test loss: 1.88496, test acc: 0.6691\n",
      "2019-05-12 22:42:47,627 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:42:54,012 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 67 Train Loss: 0.0033 ce: 0.00326, vat: 0.00000, test loss: 1.70542, test acc: 0.6961\n",
      "2019-05-12 22:42:54,014 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:00,281 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 68 Train Loss: 0.0025 ce: 0.00249, vat: 0.00000, test loss: 1.95200, test acc: 0.6754\n",
      "2019-05-12 22:43:00,283 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:06,666 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 69 Train Loss: 0.0445 ce: 0.04455, vat: 0.00000, test loss: 1.78753, test acc: 0.6803\n",
      "2019-05-12 22:43:06,667 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:13,060 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 70 Train Loss: 0.0460 ce: 0.04601, vat: 0.00000, test loss: 1.75122, test acc: 0.6868\n",
      "2019-05-12 22:43:13,062 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:19,414 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 71 Train Loss: 0.0463 ce: 0.04631, vat: 0.00000, test loss: 1.85622, test acc: 0.6680\n",
      "2019-05-12 22:43:19,416 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:25,835 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 72 Train Loss: 0.0317 ce: 0.03175, vat: 0.00000, test loss: 2.11861, test acc: 0.6588\n",
      "2019-05-12 22:43:25,837 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:32,178 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 73 Train Loss: 0.0167 ce: 0.01667, vat: 0.00000, test loss: 1.73374, test acc: 0.6875\n",
      "2019-05-12 22:43:32,180 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:38,556 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 74 Train Loss: 0.0094 ce: 0.00944, vat: 0.00000, test loss: 1.83730, test acc: 0.6774\n",
      "2019-05-12 22:43:38,558 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:44,912 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 75 Train Loss: 0.0205 ce: 0.02047, vat: 0.00000, test loss: 1.78273, test acc: 0.6828\n",
      "2019-05-12 22:43:44,913 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:51,248 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 76 Train Loss: 0.0023 ce: 0.00232, vat: 0.00000, test loss: 2.00095, test acc: 0.6652\n",
      "2019-05-12 22:43:51,250 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:43:57,657 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 77 Train Loss: 0.0333 ce: 0.03325, vat: 0.00000, test loss: 1.74743, test acc: 0.6839\n",
      "2019-05-12 22:43:57,658 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:04,099 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 78 Train Loss: 0.0013 ce: 0.00132, vat: 0.00000, test loss: 1.76362, test acc: 0.6917\n",
      "2019-05-12 22:44:04,101 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:10,411 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 79 Train Loss: 0.0272 ce: 0.02718, vat: 0.00000, test loss: 1.73842, test acc: 0.6915\n",
      "2019-05-12 22:44:10,414 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:16,729 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 80 Train Loss: 0.0193 ce: 0.01927, vat: 0.00000, test loss: 1.84304, test acc: 0.6722\n",
      "2019-05-12 22:44:16,730 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:23,118 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 81 Train Loss: 0.1434 ce: 0.14344, vat: 0.00000, test loss: 1.92424, test acc: 0.6735\n",
      "2019-05-12 22:44:23,119 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:29,454 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 82 Train Loss: 0.0015 ce: 0.00148, vat: 0.00000, test loss: 1.80570, test acc: 0.6868\n",
      "2019-05-12 22:44:29,455 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:35,774 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 83 Train Loss: 0.0460 ce: 0.04605, vat: 0.00000, test loss: 1.79368, test acc: 0.6927\n",
      "2019-05-12 22:44:35,776 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:42,104 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 84 Train Loss: 0.0014 ce: 0.00138, vat: 0.00000, test loss: 1.76942, test acc: 0.7020\n",
      "2019-05-12 22:44:42,105 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:48,486 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 85 Train Loss: 0.0087 ce: 0.00871, vat: 0.00000, test loss: 1.90486, test acc: 0.6903\n",
      "2019-05-12 22:44:48,487 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:44:54,817 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 86 Train Loss: 0.0204 ce: 0.02038, vat: 0.00000, test loss: 1.86670, test acc: 0.6820\n",
      "2019-05-12 22:44:54,818 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:01,126 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 87 Train Loss: 0.0085 ce: 0.00853, vat: 0.00000, test loss: 1.94901, test acc: 0.6831\n",
      "2019-05-12 22:45:01,127 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:07,476 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 88 Train Loss: 0.1083 ce: 0.10830, vat: 0.00000, test loss: 1.88747, test acc: 0.6852\n",
      "2019-05-12 22:45:07,477 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:13,963 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 89 Train Loss: 0.0230 ce: 0.02302, vat: 0.00000, test loss: 1.80701, test acc: 0.6914\n",
      "2019-05-12 22:45:13,964 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:20,217 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 90 Train Loss: 0.1133 ce: 0.11332, vat: 0.00000, test loss: 1.89399, test acc: 0.6787\n",
      "2019-05-12 22:45:20,218 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:26,617 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 91 Train Loss: 0.0418 ce: 0.04183, vat: 0.00000, test loss: 2.10767, test acc: 0.6821\n",
      "2019-05-12 22:45:26,619 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:32,972 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 92 Train Loss: 0.0205 ce: 0.02049, vat: 0.00000, test loss: 1.84456, test acc: 0.6879\n",
      "2019-05-12 22:45:32,974 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:39,360 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 93 Train Loss: 0.0021 ce: 0.00211, vat: 0.00000, test loss: 1.76942, test acc: 0.6950\n",
      "2019-05-12 22:45:39,362 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:45,704 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 94 Train Loss: 0.0115 ce: 0.01153, vat: 0.00000, test loss: 1.74776, test acc: 0.6976\n",
      "2019-05-12 22:45:45,706 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:52,025 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 95 Train Loss: 0.0016 ce: 0.00165, vat: 0.00000, test loss: 1.84148, test acc: 0.6992\n",
      "2019-05-12 22:45:52,027 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:45:58,355 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 96 Train Loss: 0.0057 ce: 0.00569, vat: 0.00000, test loss: 1.91094, test acc: 0.6823\n",
      "2019-05-12 22:45:58,357 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:04,724 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 97 Train Loss: 0.0072 ce: 0.00724, vat: 0.00000, test loss: 1.89471, test acc: 0.6786\n",
      "2019-05-12 22:46:04,725 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:10,969 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 98 Train Loss: 0.0027 ce: 0.00269, vat: 0.00000, test loss: 1.82046, test acc: 0.6873\n",
      "2019-05-12 22:46:10,970 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:46:17,100 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 99 Train Loss: 0.0468 ce: 0.04678, vat: 0.00000, test loss: 1.95112, test acc: 0.6864\n",
      "2019-05-12 22:46:17,102 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:23,438 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 100 Train Loss: 0.0021 ce: 0.00206, vat: 0.00000, test loss: 1.82572, test acc: 0.7011\n",
      "2019-05-12 22:46:23,440 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:29,833 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 101 Train Loss: 0.0020 ce: 0.00200, vat: 0.00000, test loss: 1.94584, test acc: 0.6791\n",
      "2019-05-12 22:46:29,835 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:36,238 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 102 Train Loss: 0.0063 ce: 0.00632, vat: 0.00000, test loss: 1.77183, test acc: 0.6997\n",
      "2019-05-12 22:46:36,239 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:42,666 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 103 Train Loss: 0.0318 ce: 0.03178, vat: 0.00000, test loss: 1.80916, test acc: 0.6954\n",
      "2019-05-12 22:46:42,668 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:49,032 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 104 Train Loss: 0.0119 ce: 0.01188, vat: 0.00000, test loss: 1.91528, test acc: 0.6782\n",
      "2019-05-12 22:46:49,033 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:46:55,107 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 105 Train Loss: 0.0005 ce: 0.00054, vat: 0.00000, test loss: 1.74456, test acc: 0.7027\n",
      "2019-05-12 22:46:55,109 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:01,334 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 106 Train Loss: 0.0030 ce: 0.00299, vat: 0.00000, test loss: 1.68462, test acc: 0.6989\n",
      "2019-05-12 22:47:01,335 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:07,540 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 107 Train Loss: 0.0174 ce: 0.01741, vat: 0.00000, test loss: 1.77101, test acc: 0.6988\n",
      "2019-05-12 22:47:07,541 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:13,855 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 108 Train Loss: 0.0194 ce: 0.01944, vat: 0.00000, test loss: 2.08378, test acc: 0.6708\n",
      "2019-05-12 22:47:13,857 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:20,067 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 109 Train Loss: 0.0031 ce: 0.00311, vat: 0.00000, test loss: 1.95904, test acc: 0.6804\n",
      "2019-05-12 22:47:20,068 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:26,308 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 110 Train Loss: 0.0094 ce: 0.00937, vat: 0.00000, test loss: 1.85120, test acc: 0.6869\n",
      "2019-05-12 22:47:26,309 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:32,618 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 111 Train Loss: 0.0033 ce: 0.00328, vat: 0.00000, test loss: 1.84764, test acc: 0.6929\n",
      "2019-05-12 22:47:32,619 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:38,844 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 112 Train Loss: 0.0030 ce: 0.00297, vat: 0.00000, test loss: 1.86211, test acc: 0.7009\n",
      "2019-05-12 22:47:38,845 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:45,153 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 113 Train Loss: 0.0389 ce: 0.03891, vat: 0.00000, test loss: 1.76797, test acc: 0.7019\n",
      "2019-05-12 22:47:45,154 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:51,400 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 114 Train Loss: 0.0102 ce: 0.01023, vat: 0.00000, test loss: 2.12412, test acc: 0.6659\n",
      "2019-05-12 22:47:51,402 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:47:57,616 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 115 Train Loss: 0.0249 ce: 0.02494, vat: 0.00000, test loss: 1.96447, test acc: 0.6861\n",
      "2019-05-12 22:47:57,618 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:03,817 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 116 Train Loss: 0.0031 ce: 0.00311, vat: 0.00000, test loss: 1.87942, test acc: 0.6938\n",
      "2019-05-12 22:48:03,819 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:10,046 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 117 Train Loss: 0.0058 ce: 0.00583, vat: 0.00000, test loss: 1.83219, test acc: 0.6988\n",
      "2019-05-12 22:48:10,047 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:16,351 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 118 Train Loss: 0.0144 ce: 0.01442, vat: 0.00000, test loss: 1.92427, test acc: 0.6849\n",
      "2019-05-12 22:48:16,353 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:22,588 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 119 Train Loss: 0.0048 ce: 0.00481, vat: 0.00000, test loss: 1.87036, test acc: 0.6850\n",
      "2019-05-12 22:48:22,589 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:28,800 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 120 Train Loss: 0.0019 ce: 0.00195, vat: 0.00000, test loss: 1.87431, test acc: 0.6920\n",
      "2019-05-12 22:48:28,801 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 22:48:35,025 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 121 Train Loss: 0.0055 ce: 0.00548, vat: 0.00000, test loss: 1.87719, test acc: 0.7017\n",
      "2019-05-12 22:48:35,026 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000988\n",
      "2019-05-12 22:48:41,348 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 122 Train Loss: 0.0023 ce: 0.00229, vat: 0.00000, test loss: 2.09602, test acc: 0.6777\n",
      "2019-05-12 22:48:41,350 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000975\n",
      "2019-05-12 22:48:47,600 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 123 Train Loss: 0.0016 ce: 0.00155, vat: 0.00000, test loss: 1.97570, test acc: 0.6854\n",
      "2019-05-12 22:48:47,601 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000963\n",
      "2019-05-12 22:48:53,820 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 124 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.87012, test acc: 0.7024\n",
      "2019-05-12 22:48:53,821 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000950\n",
      "2019-05-12 22:49:00,077 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 125 Train Loss: 0.0180 ce: 0.01798, vat: 0.00000, test loss: 1.84494, test acc: 0.6963\n",
      "2019-05-12 22:49:00,078 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000937\n",
      "2019-05-12 22:49:06,750 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 126 Train Loss: 0.0016 ce: 0.00163, vat: 0.00000, test loss: 1.93405, test acc: 0.6985\n",
      "2019-05-12 22:49:06,751 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000925\n",
      "2019-05-12 22:49:13,772 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 127 Train Loss: 0.0284 ce: 0.02842, vat: 0.00000, test loss: 1.95329, test acc: 0.6895\n",
      "2019-05-12 22:49:13,773 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000913\n",
      "2019-05-12 22:49:22,675 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 128 Train Loss: 0.0266 ce: 0.02657, vat: 0.00000, test loss: 2.03865, test acc: 0.6731\n",
      "2019-05-12 22:49:22,688 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000900\n",
      "2019-05-12 22:49:30,541 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 129 Train Loss: 0.0014 ce: 0.00140, vat: 0.00000, test loss: 1.81269, test acc: 0.6972\n",
      "2019-05-12 22:49:30,544 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000887\n",
      "2019-05-12 22:49:38,935 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 130 Train Loss: 0.0014 ce: 0.00138, vat: 0.00000, test loss: 1.84343, test acc: 0.7032\n",
      "2019-05-12 22:49:38,936 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000875\n",
      "2019-05-12 22:49:47,198 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 131 Train Loss: 0.0008 ce: 0.00079, vat: 0.00000, test loss: 1.90623, test acc: 0.7005\n",
      "2019-05-12 22:49:47,202 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:49:55,710 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 132 Train Loss: 0.0128 ce: 0.01283, vat: 0.00000, test loss: 1.80345, test acc: 0.7056\n",
      "2019-05-12 22:49:55,717 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000850\n",
      "2019-05-12 22:50:05,294 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 133 Train Loss: 0.0040 ce: 0.00401, vat: 0.00000, test loss: 1.88455, test acc: 0.7041\n",
      "2019-05-12 22:50:05,308 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000838\n",
      "2019-05-12 22:50:15,574 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 134 Train Loss: 0.0048 ce: 0.00477, vat: 0.00000, test loss: 1.73273, test acc: 0.7137\n",
      "2019-05-12 22:50:15,587 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000825\n",
      "2019-05-12 22:50:24,819 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 135 Train Loss: 0.0010 ce: 0.00099, vat: 0.00000, test loss: 1.79238, test acc: 0.7089\n",
      "2019-05-12 22:50:24,826 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000813\n",
      "2019-05-12 22:50:34,749 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 136 Train Loss: 0.0704 ce: 0.07041, vat: 0.00000, test loss: 2.01399, test acc: 0.6895\n",
      "2019-05-12 22:50:34,750 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000800\n",
      "2019-05-12 22:50:44,253 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 137 Train Loss: 0.0029 ce: 0.00290, vat: 0.00000, test loss: 1.75690, test acc: 0.7155\n",
      "2019-05-12 22:50:44,259 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000788\n",
      "2019-05-12 22:50:53,571 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 138 Train Loss: 0.0044 ce: 0.00438, vat: 0.00000, test loss: 1.78137, test acc: 0.7033\n",
      "2019-05-12 22:50:53,575 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000775\n",
      "2019-05-12 22:51:03,320 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 139 Train Loss: 0.0001 ce: 0.00013, vat: 0.00000, test loss: 2.00773, test acc: 0.6898\n",
      "2019-05-12 22:51:03,333 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000762\n",
      "2019-05-12 22:51:13,941 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 140 Train Loss: 0.0042 ce: 0.00424, vat: 0.00000, test loss: 1.76920, test acc: 0.7134\n",
      "2019-05-12 22:51:13,943 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000750\n",
      "2019-05-12 22:51:24,356 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 141 Train Loss: 0.0009 ce: 0.00090, vat: 0.00000, test loss: 1.73293, test acc: 0.7169\n",
      "2019-05-12 22:51:24,358 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000738\n",
      "2019-05-12 22:51:34,694 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 142 Train Loss: 0.0183 ce: 0.01825, vat: 0.00000, test loss: 1.79371, test acc: 0.7136\n",
      "2019-05-12 22:51:34,701 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000725\n",
      "2019-05-12 22:51:43,191 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 143 Train Loss: 0.0021 ce: 0.00207, vat: 0.00000, test loss: 1.85950, test acc: 0.7019\n",
      "2019-05-12 22:51:43,197 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000713\n",
      "2019-05-12 22:51:50,325 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 144 Train Loss: 0.0002 ce: 0.00024, vat: 0.00000, test loss: 1.76103, test acc: 0.7116\n",
      "2019-05-12 22:51:50,327 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000700\n",
      "2019-05-12 22:51:57,046 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 145 Train Loss: 0.0006 ce: 0.00060, vat: 0.00000, test loss: 1.99393, test acc: 0.7043\n",
      "2019-05-12 22:51:57,047 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000687\n",
      "2019-05-12 22:52:04,548 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 146 Train Loss: 0.0010 ce: 0.00104, vat: 0.00000, test loss: 1.82180, test acc: 0.7149\n",
      "2019-05-12 22:52:04,549 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000675\n",
      "2019-05-12 22:52:11,676 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 147 Train Loss: 0.0001 ce: 0.00005, vat: 0.00000, test loss: 2.06575, test acc: 0.6933\n",
      "2019-05-12 22:52:11,677 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000663\n",
      "2019-05-12 22:52:17,852 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 148 Train Loss: 0.0046 ce: 0.00465, vat: 0.00000, test loss: 1.91151, test acc: 0.7018\n",
      "2019-05-12 22:52:17,853 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000650\n",
      "2019-05-12 22:52:23,982 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 149 Train Loss: 0.0002 ce: 0.00017, vat: 0.00000, test loss: 1.87788, test acc: 0.7001\n",
      "2019-05-12 22:52:23,983 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000637\n",
      "2019-05-12 22:52:30,177 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 150 Train Loss: 0.0010 ce: 0.00104, vat: 0.00000, test loss: 1.79811, test acc: 0.7209\n",
      "2019-05-12 22:52:30,178 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000625\n",
      "2019-05-12 22:52:36,480 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 151 Train Loss: 0.0001 ce: 0.00008, vat: 0.00000, test loss: 1.79086, test acc: 0.7179\n",
      "2019-05-12 22:52:36,481 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000613\n",
      "2019-05-12 22:52:42,656 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 152 Train Loss: 0.0001 ce: 0.00012, vat: 0.00000, test loss: 1.77105, test acc: 0.7170\n",
      "2019-05-12 22:52:42,658 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000600\n",
      "2019-05-12 22:52:48,877 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 153 Train Loss: 0.0001 ce: 0.00013, vat: 0.00000, test loss: 1.81749, test acc: 0.7103\n",
      "2019-05-12 22:52:48,878 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000588\n",
      "2019-05-12 22:52:55,041 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 154 Train Loss: 0.0001 ce: 0.00012, vat: 0.00000, test loss: 1.79863, test acc: 0.7137\n",
      "2019-05-12 22:52:55,042 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000575\n",
      "2019-05-12 22:53:01,196 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 155 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.84468, test acc: 0.7165\n",
      "2019-05-12 22:53:01,197 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000563\n",
      "2019-05-12 22:53:07,318 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 156 Train Loss: 0.0012 ce: 0.00124, vat: 0.00000, test loss: 1.97853, test acc: 0.7027\n",
      "2019-05-12 22:53:07,319 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000550\n",
      "2019-05-12 22:53:13,553 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 157 Train Loss: 0.0012 ce: 0.00120, vat: 0.00000, test loss: 1.89568, test acc: 0.7054\n",
      "2019-05-12 22:53:13,554 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000538\n",
      "2019-05-12 22:53:19,868 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 158 Train Loss: 0.0000 ce: 0.00002, vat: 0.00000, test loss: 1.88768, test acc: 0.7145\n",
      "2019-05-12 22:53:19,869 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000525\n",
      "2019-05-12 22:53:26,349 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 159 Train Loss: 0.0503 ce: 0.05033, vat: 0.00000, test loss: 1.82617, test acc: 0.7146\n",
      "2019-05-12 22:53:26,351 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000512\n",
      "2019-05-12 22:53:32,967 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 160 Train Loss: 0.0003 ce: 0.00026, vat: 0.00000, test loss: 1.91279, test acc: 0.7104\n",
      "2019-05-12 22:53:32,968 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000500\n",
      "2019-05-12 22:53:39,818 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 161 Train Loss: 0.0007 ce: 0.00067, vat: 0.00000, test loss: 1.86825, test acc: 0.7132\n",
      "2019-05-12 22:53:39,820 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000487\n",
      "2019-05-12 22:53:47,387 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 162 Train Loss: 0.0004 ce: 0.00035, vat: 0.00000, test loss: 1.86831, test acc: 0.7175\n",
      "2019-05-12 22:53:47,400 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000475\n",
      "2019-05-12 22:53:54,938 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 163 Train Loss: 0.0001 ce: 0.00015, vat: 0.00000, test loss: 1.84311, test acc: 0.7167\n",
      "2019-05-12 22:53:54,939 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000463\n",
      "2019-05-12 22:54:03,362 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 164 Train Loss: 0.0001 ce: 0.00008, vat: 0.00000, test loss: 1.84273, test acc: 0.7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:54:03,364 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000450\n",
      "2019-05-12 22:54:11,111 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 165 Train Loss: 0.0002 ce: 0.00018, vat: 0.00000, test loss: 1.87161, test acc: 0.7182\n",
      "2019-05-12 22:54:11,112 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000438\n",
      "2019-05-12 22:54:20,399 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 166 Train Loss: 0.0005 ce: 0.00046, vat: 0.00000, test loss: 1.82611, test acc: 0.7208\n",
      "2019-05-12 22:54:20,400 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000425\n",
      "2019-05-12 22:54:29,837 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 167 Train Loss: 0.0027 ce: 0.00274, vat: 0.00000, test loss: 1.86158, test acc: 0.7122\n",
      "2019-05-12 22:54:29,838 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000412\n",
      "2019-05-12 22:54:39,733 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 168 Train Loss: 0.0044 ce: 0.00437, vat: 0.00000, test loss: 1.84648, test acc: 0.7195\n",
      "2019-05-12 22:54:39,740 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000400\n",
      "2019-05-12 22:54:48,349 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 169 Train Loss: 0.0004 ce: 0.00037, vat: 0.00000, test loss: 1.92949, test acc: 0.7105\n",
      "2019-05-12 22:54:48,350 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000388\n",
      "2019-05-12 22:54:57,361 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 170 Train Loss: 0.0586 ce: 0.05859, vat: 0.00000, test loss: 1.85518, test acc: 0.7091\n",
      "2019-05-12 22:54:57,374 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000375\n",
      "2019-05-12 22:55:06,483 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 171 Train Loss: 0.0023 ce: 0.00233, vat: 0.00000, test loss: 1.89097, test acc: 0.7126\n",
      "2019-05-12 22:55:06,484 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000362\n",
      "2019-05-12 22:55:15,675 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 172 Train Loss: 0.0000 ce: 0.00000, vat: 0.00000, test loss: 1.88224, test acc: 0.7182\n",
      "2019-05-12 22:55:15,688 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000350\n",
      "2019-05-12 22:55:24,933 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 173 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.96270, test acc: 0.7119\n",
      "2019-05-12 22:55:24,943 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000338\n",
      "2019-05-12 22:55:33,839 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 174 Train Loss: 0.0004 ce: 0.00038, vat: 0.00000, test loss: 1.96462, test acc: 0.7040\n",
      "2019-05-12 22:55:33,844 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000325\n",
      "2019-05-12 22:55:43,458 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 175 Train Loss: 0.0002 ce: 0.00017, vat: 0.00000, test loss: 1.90799, test acc: 0.7201\n",
      "2019-05-12 22:55:43,470 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000313\n",
      "2019-05-12 22:55:52,300 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 176 Train Loss: 0.0002 ce: 0.00017, vat: 0.00000, test loss: 1.85129, test acc: 0.7216\n",
      "2019-05-12 22:55:52,302 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000300\n",
      "2019-05-12 22:56:01,392 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 177 Train Loss: 0.0000 ce: 0.00002, vat: 0.00000, test loss: 1.79923, test acc: 0.7280\n",
      "2019-05-12 22:56:01,393 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000287\n",
      "2019-05-12 22:56:10,121 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 178 Train Loss: 0.0001 ce: 0.00006, vat: 0.00000, test loss: 1.83404, test acc: 0.7299\n",
      "2019-05-12 22:56:10,138 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000275\n",
      "2019-05-12 22:56:18,389 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 179 Train Loss: 0.0001 ce: 0.00006, vat: 0.00000, test loss: 2.00648, test acc: 0.7161\n",
      "2019-05-12 22:56:18,391 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000263\n",
      "2019-05-12 22:56:26,397 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 180 Train Loss: 0.0005 ce: 0.00050, vat: 0.00000, test loss: 1.83217, test acc: 0.7225\n",
      "2019-05-12 22:56:26,400 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000250\n",
      "2019-05-12 22:56:34,258 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 181 Train Loss: 0.0006 ce: 0.00063, vat: 0.00000, test loss: 1.90042, test acc: 0.7220\n",
      "2019-05-12 22:56:34,261 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000237\n",
      "2019-05-12 22:56:42,007 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 182 Train Loss: 0.0002 ce: 0.00022, vat: 0.00000, test loss: 1.86608, test acc: 0.7234\n",
      "2019-05-12 22:56:42,009 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000225\n",
      "2019-05-12 22:56:49,776 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 183 Train Loss: 0.0008 ce: 0.00080, vat: 0.00000, test loss: 1.92532, test acc: 0.7198\n",
      "2019-05-12 22:56:49,783 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000212\n",
      "2019-05-12 22:56:57,143 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 184 Train Loss: 0.0000 ce: 0.00000, vat: 0.00000, test loss: 1.89861, test acc: 0.7196\n",
      "2019-05-12 22:56:57,145 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000200\n",
      "2019-05-12 22:57:04,426 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 185 Train Loss: 0.0004 ce: 0.00043, vat: 0.00000, test loss: 1.86479, test acc: 0.7247\n",
      "2019-05-12 22:57:04,429 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000188\n",
      "2019-05-12 22:57:11,784 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 186 Train Loss: 0.0006 ce: 0.00064, vat: 0.00000, test loss: 1.89587, test acc: 0.7233\n",
      "2019-05-12 22:57:11,786 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000175\n",
      "2019-05-12 22:57:19,077 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 187 Train Loss: 0.0000 ce: 0.00003, vat: 0.00000, test loss: 1.83897, test acc: 0.7225\n",
      "2019-05-12 22:57:19,079 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000163\n",
      "2019-05-12 22:57:26,361 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 188 Train Loss: 0.0001 ce: 0.00005, vat: 0.00000, test loss: 1.83742, test acc: 0.7249\n",
      "2019-05-12 22:57:26,362 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000150\n",
      "2019-05-12 22:57:33,692 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 189 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.86389, test acc: 0.7262\n",
      "2019-05-12 22:57:33,697 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000138\n",
      "2019-05-12 22:57:41,743 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 190 Train Loss: 0.0001 ce: 0.00012, vat: 0.00000, test loss: 1.84123, test acc: 0.7250\n",
      "2019-05-12 22:57:41,745 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000125\n",
      "2019-05-12 22:57:50,006 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 191 Train Loss: 0.0001 ce: 0.00012, vat: 0.00000, test loss: 1.85151, test acc: 0.7251\n",
      "2019-05-12 22:57:50,008 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000113\n",
      "2019-05-12 22:57:58,052 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 192 Train Loss: 0.0001 ce: 0.00005, vat: 0.00000, test loss: 1.87447, test acc: 0.7247\n",
      "2019-05-12 22:57:58,054 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000100\n",
      "2019-05-12 22:58:07,300 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 193 Train Loss: 0.0000 ce: 0.00002, vat: 0.00000, test loss: 1.86034, test acc: 0.7282\n",
      "2019-05-12 22:58:07,315 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000087\n",
      "2019-05-12 22:58:16,585 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 194 Train Loss: 0.0001 ce: 0.00008, vat: 0.00000, test loss: 1.88052, test acc: 0.7247\n",
      "2019-05-12 22:58:16,586 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000075\n",
      "2019-05-12 22:58:25,793 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 195 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.83563, test acc: 0.7263\n",
      "2019-05-12 22:58:25,799 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000063\n",
      "2019-05-12 22:58:35,269 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 196 Train Loss: 0.0001 ce: 0.00008, vat: 0.00000, test loss: 1.86083, test acc: 0.7257\n",
      "2019-05-12 22:58:35,275 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000050\n",
      "2019-05-12 22:58:43,929 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 197 Train Loss: 0.0000 ce: 0.00002, vat: 0.00000, test loss: 1.83504, test acc: 0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 22:58:43,935 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000037\n",
      "2019-05-12 22:58:53,701 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 198 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.82305, test acc: 0.7319\n",
      "2019-05-12 22:58:53,703 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000025\n",
      "2019-05-12 22:59:02,877 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 199 Train Loss: 0.0001 ce: 0.00005, vat: 0.00000, test loss: 1.85397, test acc: 0.7297\n",
      "2019-05-12 22:59:02,882 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000013\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"cifar10\"\n",
    "    args.trainer = \"mle\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"CNN9c\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 1e-6\n",
    "    args.eps = 10\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.reg_lamb = 1\n",
    "    args.affine = False\n",
    "    args.ent_min = False\n",
    "    \n",
    "    args.gpu_id = \"1\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 1\n",
    "    args.log_interval = 1\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 32\n",
    "    args.batchsize_ul = 128\n",
    "    args.batchsize_eval = 100\n",
    "    \n",
    "    args.num_epochs = 200\n",
    "    args.num_iter_per_epoch = 400\n",
    "    args.num_batch_it = 400\n",
    "    args.epoch_decay_start = 120\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 10\n",
    "    args.extra_lamb = 1\n",
    "    \n",
    "    args.drop = 0.5\n",
    "    args.top_bn = False\n",
    "    args.vis = False\n",
    "    args.debug = False\n",
    "    args.data_dir = os.path.join(\"./data/%s\" % args.dataset)\n",
    "\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "arg = parse_args()\n",
    "arg.exp = \"avg\"\n",
    "\n",
    "wlog(\"args in this experiment %s\", '\\n'.join(str(e) for e in sorted(vars(arg).items())))\n",
    "# noinspection PyBroadException\n",
    "\n",
    "acc_list = train(arg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train with dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:29:57,903 - <ipython-input-15-c475fea2cf72>[line:5]: args in this experiment ('affine', False)\n",
      "('alpha', 1)\n",
      "('arch', 'CNN9c')\n",
      "('aug_flip', False)\n",
      "('aug_trans', False)\n",
      "('batchsize', 32)\n",
      "('batchsize_eval', 100)\n",
      "('batchsize_ul', 128)\n",
      "('cuda', True)\n",
      "('data_dir', './data/cifar10')\n",
      "('dataset', 'cifar10')\n",
      "('dataset_seed', 1)\n",
      "('debug', False)\n",
      "('device', device(type='cuda'))\n",
      "('drop', 0.5)\n",
      "('dropout_rate', 0.0)\n",
      "('ent_min', False)\n",
      "('epoch_decay_start', 120)\n",
      "('eps', 10)\n",
      "('epsilon', 10)\n",
      "('eval_freq', 1)\n",
      "('exp', 'avg')\n",
      "('extra_lamb', 1)\n",
      "('gpu_id', '1')\n",
      "('iterations', 1000)\n",
      "('k', 1)\n",
      "('log_dir', 'log')\n",
      "('log_interval', 1)\n",
      "('lr', 0.001)\n",
      "('method', 'vat')\n",
      "('mom1', 0.9)\n",
      "('mom2', 0.5)\n",
      "('n_categories', 10)\n",
      "('no_cuda', False)\n",
      "('num_batch_it', 400)\n",
      "('num_epochs', 200)\n",
      "('num_iter_per_epoch', 400)\n",
      "('reg_lamb', 1)\n",
      "('seed', 1)\n",
      "('size', 100)\n",
      "('snapshot_freq', 20)\n",
      "('top_bn', False)\n",
      "('trainer', 'mle')\n",
      "('use_entmin', False)\n",
      "('validation', False)\n",
      "('vis', False)\n",
      "('xi', 1e-06)\n",
      "2019-05-12 23:30:02,018 - <ipython-input-7-bdc23bc4b139>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -2591.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:30:13,182 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 0 Train Loss: 1.7980 ce: 1.79801, vat: 0.00000, test loss: 1.74110, test acc: 0.3553\n",
      "2019-05-12 23:30:13,195 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:30:24,516 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 1 Train Loss: 1.5821 ce: 1.58208, vat: 0.00000, test loss: 1.56515, test acc: 0.4277\n",
      "2019-05-12 23:30:24,522 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:30:36,349 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 2 Train Loss: 1.0772 ce: 1.07718, vat: 0.00000, test loss: 1.22319, test acc: 0.5559\n",
      "2019-05-12 23:30:36,363 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:30:46,903 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 3 Train Loss: 1.1493 ce: 1.14932, vat: 0.00000, test loss: 1.17222, test acc: 0.5844\n",
      "2019-05-12 23:30:46,904 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:30:58,778 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 4 Train Loss: 0.7615 ce: 0.76148, vat: 0.00000, test loss: 1.37633, test acc: 0.5462\n",
      "2019-05-12 23:30:58,780 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:31:09,092 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 5 Train Loss: 0.8370 ce: 0.83701, vat: 0.00000, test loss: 1.18171, test acc: 0.6022\n",
      "2019-05-12 23:31:09,093 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:31:20,581 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 6 Train Loss: 0.4475 ce: 0.44752, vat: 0.00000, test loss: 1.32477, test acc: 0.5592\n",
      "2019-05-12 23:31:20,586 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:31:32,273 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 7 Train Loss: 0.6720 ce: 0.67203, vat: 0.00000, test loss: 1.18827, test acc: 0.6211\n",
      "2019-05-12 23:31:32,288 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:31:43,477 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 8 Train Loss: 0.4087 ce: 0.40869, vat: 0.00000, test loss: 1.58588, test acc: 0.5537\n",
      "2019-05-12 23:31:43,484 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:31:54,092 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 9 Train Loss: 0.4256 ce: 0.42559, vat: 0.00000, test loss: 1.38208, test acc: 0.6083\n",
      "2019-05-12 23:31:54,093 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:05,359 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 10 Train Loss: 0.2856 ce: 0.28559, vat: 0.00000, test loss: 1.60989, test acc: 0.5824\n",
      "2019-05-12 23:32:05,366 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:16,345 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 11 Train Loss: 0.4445 ce: 0.44447, vat: 0.00000, test loss: 1.34910, test acc: 0.6208\n",
      "2019-05-12 23:32:16,350 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:28,779 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 12 Train Loss: 0.1974 ce: 0.19737, vat: 0.00000, test loss: 1.18120, test acc: 0.6545\n",
      "2019-05-12 23:32:28,781 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:39,829 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 13 Train Loss: 0.2454 ce: 0.24544, vat: 0.00000, test loss: 1.40547, test acc: 0.6255\n",
      "2019-05-12 23:32:39,834 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:49,416 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 14 Train Loss: 0.1284 ce: 0.12838, vat: 0.00000, test loss: 1.41734, test acc: 0.6275\n",
      "2019-05-12 23:32:49,418 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:32:59,649 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 15 Train Loss: 0.0949 ce: 0.09492, vat: 0.00000, test loss: 1.33851, test acc: 0.6478\n",
      "2019-05-12 23:32:59,658 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:09,036 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 16 Train Loss: 0.1485 ce: 0.14851, vat: 0.00000, test loss: 1.33641, test acc: 0.6606\n",
      "2019-05-12 23:33:09,042 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:17,080 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 17 Train Loss: 0.1884 ce: 0.18836, vat: 0.00000, test loss: 1.47145, test acc: 0.6537\n",
      "2019-05-12 23:33:17,086 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:24,666 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 18 Train Loss: 0.0568 ce: 0.05684, vat: 0.00000, test loss: 1.67322, test acc: 0.6142\n",
      "2019-05-12 23:33:24,667 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:32,294 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 19 Train Loss: 0.0552 ce: 0.05519, vat: 0.00000, test loss: 1.54395, test acc: 0.6434\n",
      "2019-05-12 23:33:32,305 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:40,320 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 20 Train Loss: 0.0544 ce: 0.05441, vat: 0.00000, test loss: 1.44268, test acc: 0.6514\n",
      "2019-05-12 23:33:40,323 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:48,402 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 21 Train Loss: 0.2415 ce: 0.24153, vat: 0.00000, test loss: 1.51863, test acc: 0.6511\n",
      "2019-05-12 23:33:48,403 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:33:56,621 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 22 Train Loss: 0.0487 ce: 0.04868, vat: 0.00000, test loss: 1.75903, test acc: 0.6338\n",
      "2019-05-12 23:33:56,630 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:05,616 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 23 Train Loss: 0.0435 ce: 0.04352, vat: 0.00000, test loss: 1.52184, test acc: 0.6605\n",
      "2019-05-12 23:34:05,618 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:15,416 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 24 Train Loss: 0.0351 ce: 0.03510, vat: 0.00000, test loss: 1.73734, test acc: 0.6376\n",
      "2019-05-12 23:34:15,418 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:24,358 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 25 Train Loss: 0.1962 ce: 0.19623, vat: 0.00000, test loss: 1.91934, test acc: 0.6261\n",
      "2019-05-12 23:34:24,360 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:34,605 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 26 Train Loss: 0.0778 ce: 0.07784, vat: 0.00000, test loss: 1.61574, test acc: 0.6574\n",
      "2019-05-12 23:34:34,607 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:45,319 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 27 Train Loss: 0.1381 ce: 0.13815, vat: 0.00000, test loss: 1.72910, test acc: 0.6525\n",
      "2019-05-12 23:34:45,327 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:34:55,714 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 28 Train Loss: 0.1135 ce: 0.11351, vat: 0.00000, test loss: 1.81784, test acc: 0.6271\n",
      "2019-05-12 23:34:55,730 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:35:06,406 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 29 Train Loss: 0.0187 ce: 0.01873, vat: 0.00000, test loss: 1.43404, test acc: 0.6782\n",
      "2019-05-12 23:35:06,408 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:35:18,174 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 30 Train Loss: 0.1129 ce: 0.11290, vat: 0.00000, test loss: 1.55674, test acc: 0.6614\n",
      "2019-05-12 23:35:18,175 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:35:28,912 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 31 Train Loss: 0.0461 ce: 0.04614, vat: 0.00000, test loss: 1.64135, test acc: 0.6345\n",
      "2019-05-12 23:35:28,913 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:35:39,937 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 32 Train Loss: 0.0164 ce: 0.01644, vat: 0.00000, test loss: 1.51205, test acc: 0.6785\n",
      "2019-05-12 23:35:39,938 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:35:51,061 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 33 Train Loss: 0.1279 ce: 0.12789, vat: 0.00000, test loss: 1.76780, test acc: 0.6445\n",
      "2019-05-12 23:35:51,066 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:01,765 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 34 Train Loss: 0.0743 ce: 0.07429, vat: 0.00000, test loss: 1.77777, test acc: 0.6494\n",
      "2019-05-12 23:36:01,766 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:11,925 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 35 Train Loss: 0.0581 ce: 0.05814, vat: 0.00000, test loss: 1.72107, test acc: 0.6450\n",
      "2019-05-12 23:36:11,928 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:23,425 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 36 Train Loss: 0.0472 ce: 0.04722, vat: 0.00000, test loss: 1.65901, test acc: 0.6673\n",
      "2019-05-12 23:36:23,427 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:34,319 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 37 Train Loss: 0.0435 ce: 0.04347, vat: 0.00000, test loss: 1.56443, test acc: 0.6826\n",
      "2019-05-12 23:36:34,327 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:45,774 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 38 Train Loss: 0.2570 ce: 0.25702, vat: 0.00000, test loss: 1.80291, test acc: 0.6373\n",
      "2019-05-12 23:36:45,776 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:36:55,991 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 39 Train Loss: 0.0299 ce: 0.02991, vat: 0.00000, test loss: 1.88117, test acc: 0.6361\n",
      "2019-05-12 23:36:56,001 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:37:06,959 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 40 Train Loss: 0.0557 ce: 0.05569, vat: 0.00000, test loss: 2.06181, test acc: 0.6354\n",
      "2019-05-12 23:37:06,970 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:37:18,400 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 41 Train Loss: 0.0584 ce: 0.05842, vat: 0.00000, test loss: 1.58582, test acc: 0.6635\n",
      "2019-05-12 23:37:18,401 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:37:29,753 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 42 Train Loss: 0.0581 ce: 0.05811, vat: 0.00000, test loss: 2.21143, test acc: 0.6181\n",
      "2019-05-12 23:37:29,766 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:37:40,696 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 43 Train Loss: 0.0830 ce: 0.08304, vat: 0.00000, test loss: 1.70955, test acc: 0.6716\n",
      "2019-05-12 23:37:40,724 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:37:51,947 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 44 Train Loss: 0.0386 ce: 0.03857, vat: 0.00000, test loss: 1.74943, test acc: 0.6582\n",
      "2019-05-12 23:37:51,961 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:02,355 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 45 Train Loss: 0.0175 ce: 0.01746, vat: 0.00000, test loss: 1.65561, test acc: 0.6762\n",
      "2019-05-12 23:38:02,366 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:12,340 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 46 Train Loss: 0.0149 ce: 0.01486, vat: 0.00000, test loss: 1.82781, test acc: 0.6588\n",
      "2019-05-12 23:38:12,342 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:21,377 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 47 Train Loss: 0.0052 ce: 0.00522, vat: 0.00000, test loss: 1.66865, test acc: 0.6792\n",
      "2019-05-12 23:38:21,390 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:28,937 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 48 Train Loss: 0.1409 ce: 0.14087, vat: 0.00000, test loss: 1.66513, test acc: 0.6756\n",
      "2019-05-12 23:38:28,944 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:36,962 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 49 Train Loss: 0.1199 ce: 0.11989, vat: 0.00000, test loss: 1.77901, test acc: 0.6649\n",
      "2019-05-12 23:38:36,975 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:45,009 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 50 Train Loss: 0.0031 ce: 0.00306, vat: 0.00000, test loss: 1.80820, test acc: 0.6664\n",
      "2019-05-12 23:38:45,011 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:38:53,170 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 51 Train Loss: 0.0191 ce: 0.01909, vat: 0.00000, test loss: 1.78409, test acc: 0.6720\n",
      "2019-05-12 23:38:53,180 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:01,113 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 52 Train Loss: 0.0039 ce: 0.00394, vat: 0.00000, test loss: 1.71474, test acc: 0.6796\n",
      "2019-05-12 23:39:01,115 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:08,662 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 53 Train Loss: 0.0293 ce: 0.02928, vat: 0.00000, test loss: 1.69502, test acc: 0.6827\n",
      "2019-05-12 23:39:08,664 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:16,880 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 54 Train Loss: 0.0170 ce: 0.01705, vat: 0.00000, test loss: 1.86775, test acc: 0.6578\n",
      "2019-05-12 23:39:16,882 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:24,416 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 55 Train Loss: 0.0181 ce: 0.01808, vat: 0.00000, test loss: 1.70261, test acc: 0.6848\n",
      "2019-05-12 23:39:24,417 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:32,110 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 56 Train Loss: 0.0026 ce: 0.00256, vat: 0.00000, test loss: 1.74167, test acc: 0.6682\n",
      "2019-05-12 23:39:32,118 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:39,718 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 57 Train Loss: 0.0474 ce: 0.04737, vat: 0.00000, test loss: 1.90788, test acc: 0.6648\n",
      "2019-05-12 23:39:39,719 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:48,562 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 58 Train Loss: 0.0115 ce: 0.01153, vat: 0.00000, test loss: 1.70763, test acc: 0.6753\n",
      "2019-05-12 23:39:48,584 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:39:58,664 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 59 Train Loss: 0.0486 ce: 0.04856, vat: 0.00000, test loss: 1.96408, test acc: 0.6597\n",
      "2019-05-12 23:39:58,678 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:40:08,241 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 60 Train Loss: 0.0009 ce: 0.00091, vat: 0.00000, test loss: 1.89316, test acc: 0.6634\n",
      "2019-05-12 23:40:08,243 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:40:16,848 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 61 Train Loss: 0.0029 ce: 0.00290, vat: 0.00000, test loss: 1.84446, test acc: 0.6626\n",
      "2019-05-12 23:40:16,850 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:40:26,826 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 62 Train Loss: 0.0278 ce: 0.02779, vat: 0.00000, test loss: 1.79948, test acc: 0.6784\n",
      "2019-05-12 23:40:26,832 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:40:39,011 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 63 Train Loss: 0.0066 ce: 0.00664, vat: 0.00000, test loss: 1.81838, test acc: 0.6691\n",
      "2019-05-12 23:40:39,018 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:40:49,376 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 64 Train Loss: 0.0029 ce: 0.00294, vat: 0.00000, test loss: 1.71174, test acc: 0.6808\n",
      "2019-05-12 23:40:49,387 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:41:01,630 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 65 Train Loss: 0.0675 ce: 0.06751, vat: 0.00000, test loss: 1.81099, test acc: 0.6760\n",
      "2019-05-12 23:41:01,637 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:41:13,406 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 66 Train Loss: 0.0187 ce: 0.01873, vat: 0.00000, test loss: 1.74873, test acc: 0.6849\n",
      "2019-05-12 23:41:13,407 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:41:25,576 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 67 Train Loss: 0.0035 ce: 0.00348, vat: 0.00000, test loss: 2.01983, test acc: 0.6529\n",
      "2019-05-12 23:41:25,595 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:41:37,411 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 68 Train Loss: 0.0084 ce: 0.00840, vat: 0.00000, test loss: 1.81304, test acc: 0.6697\n",
      "2019-05-12 23:41:37,413 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:41:48,158 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 69 Train Loss: 0.1795 ce: 0.17948, vat: 0.00000, test loss: 2.21490, test acc: 0.6368\n",
      "2019-05-12 23:41:48,164 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:41:59,587 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 70 Train Loss: 0.0327 ce: 0.03266, vat: 0.00000, test loss: 1.89190, test acc: 0.6558\n",
      "2019-05-12 23:41:59,589 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:42:10,838 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 71 Train Loss: 0.0370 ce: 0.03702, vat: 0.00000, test loss: 1.90424, test acc: 0.6697\n",
      "2019-05-12 23:42:10,843 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:42:22,375 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 72 Train Loss: 0.0563 ce: 0.05626, vat: 0.00000, test loss: 1.79575, test acc: 0.6838\n",
      "2019-05-12 23:42:22,390 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:42:34,018 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 73 Train Loss: 0.0030 ce: 0.00305, vat: 0.00000, test loss: 1.73013, test acc: 0.6989\n",
      "2019-05-12 23:42:34,023 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:42:44,650 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 74 Train Loss: 0.0070 ce: 0.00703, vat: 0.00000, test loss: 1.85535, test acc: 0.6622\n",
      "2019-05-12 23:42:44,655 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:42:55,052 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 75 Train Loss: 0.1427 ce: 0.14274, vat: 0.00000, test loss: 1.73532, test acc: 0.6831\n",
      "2019-05-12 23:42:55,054 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:05,463 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 76 Train Loss: 0.1052 ce: 0.10520, vat: 0.00000, test loss: 1.81697, test acc: 0.6796\n",
      "2019-05-12 23:43:05,470 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:14,204 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 77 Train Loss: 0.0359 ce: 0.03593, vat: 0.00000, test loss: 1.79728, test acc: 0.6828\n",
      "2019-05-12 23:43:14,210 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:23,344 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 78 Train Loss: 0.0008 ce: 0.00077, vat: 0.00000, test loss: 2.09409, test acc: 0.6524\n",
      "2019-05-12 23:43:23,345 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:32,742 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 79 Train Loss: 0.0277 ce: 0.02774, vat: 0.00000, test loss: 1.90209, test acc: 0.6729\n",
      "2019-05-12 23:43:32,743 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:43,328 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 80 Train Loss: 0.0006 ce: 0.00063, vat: 0.00000, test loss: 1.89027, test acc: 0.6791\n",
      "2019-05-12 23:43:43,329 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:43:52,543 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 81 Train Loss: 0.0017 ce: 0.00167, vat: 0.00000, test loss: 1.75725, test acc: 0.6902\n",
      "2019-05-12 23:43:52,548 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:01,044 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 82 Train Loss: 0.0086 ce: 0.00859, vat: 0.00000, test loss: 2.06097, test acc: 0.6543\n",
      "2019-05-12 23:44:01,048 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:08,984 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 83 Train Loss: 0.1630 ce: 0.16297, vat: 0.00000, test loss: 1.97950, test acc: 0.6720\n",
      "2019-05-12 23:44:08,987 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:17,238 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 84 Train Loss: 0.0035 ce: 0.00353, vat: 0.00000, test loss: 1.92205, test acc: 0.6832\n",
      "2019-05-12 23:44:17,240 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:25,830 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 85 Train Loss: 0.0008 ce: 0.00078, vat: 0.00000, test loss: 1.87044, test acc: 0.6891\n",
      "2019-05-12 23:44:25,832 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:33,349 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 86 Train Loss: 0.0244 ce: 0.02442, vat: 0.00000, test loss: 1.87148, test acc: 0.6718\n",
      "2019-05-12 23:44:33,354 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:42,315 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 87 Train Loss: 0.0003 ce: 0.00034, vat: 0.00000, test loss: 1.80178, test acc: 0.6915\n",
      "2019-05-12 23:44:42,320 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:44:50,471 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 88 Train Loss: 0.0078 ce: 0.00782, vat: 0.00000, test loss: 1.82591, test acc: 0.6887\n",
      "2019-05-12 23:44:50,484 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:00,218 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 89 Train Loss: 0.0041 ce: 0.00413, vat: 0.00000, test loss: 1.76072, test acc: 0.6938\n",
      "2019-05-12 23:45:00,219 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:09,106 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 90 Train Loss: 0.0008 ce: 0.00080, vat: 0.00000, test loss: 1.88118, test acc: 0.6829\n",
      "2019-05-12 23:45:09,112 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:17,455 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 91 Train Loss: 0.0036 ce: 0.00364, vat: 0.00000, test loss: 2.17235, test acc: 0.6578\n",
      "2019-05-12 23:45:17,469 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:26,643 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 92 Train Loss: 0.0031 ce: 0.00312, vat: 0.00000, test loss: 1.92377, test acc: 0.6779\n",
      "2019-05-12 23:45:26,645 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:36,465 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 93 Train Loss: 0.0043 ce: 0.00434, vat: 0.00000, test loss: 1.83430, test acc: 0.6844\n",
      "2019-05-12 23:45:36,471 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:47,071 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 94 Train Loss: 0.0047 ce: 0.00472, vat: 0.00000, test loss: 1.92368, test acc: 0.6769\n",
      "2019-05-12 23:45:47,078 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:45:57,186 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 95 Train Loss: 0.0134 ce: 0.01337, vat: 0.00000, test loss: 1.85124, test acc: 0.6912\n",
      "2019-05-12 23:45:57,192 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:46:07,746 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 96 Train Loss: 0.0041 ce: 0.00410, vat: 0.00000, test loss: 1.92528, test acc: 0.6839\n",
      "2019-05-12 23:46:07,747 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:46:18,409 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 97 Train Loss: 0.0712 ce: 0.07124, vat: 0.00000, test loss: 1.87057, test acc: 0.6828\n",
      "2019-05-12 23:46:18,410 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:46:29,825 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 98 Train Loss: 0.0103 ce: 0.01030, vat: 0.00000, test loss: 1.86226, test acc: 0.6785\n",
      "2019-05-12 23:46:29,839 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:46:40,026 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 99 Train Loss: 0.0108 ce: 0.01079, vat: 0.00000, test loss: 1.87065, test acc: 0.6879\n",
      "2019-05-12 23:46:40,027 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:46:50,852 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 100 Train Loss: 0.0607 ce: 0.06072, vat: 0.00000, test loss: 2.13085, test acc: 0.6690\n",
      "2019-05-12 23:46:50,860 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:03,350 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 101 Train Loss: 0.0024 ce: 0.00241, vat: 0.00000, test loss: 1.90697, test acc: 0.6779\n",
      "2019-05-12 23:47:03,359 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:13,753 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 102 Train Loss: 0.0015 ce: 0.00153, vat: 0.00000, test loss: 1.80750, test acc: 0.7013\n",
      "2019-05-12 23:47:13,763 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:25,222 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 103 Train Loss: 0.0010 ce: 0.00097, vat: 0.00000, test loss: 1.86035, test acc: 0.6904\n",
      "2019-05-12 23:47:25,233 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:37,008 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 104 Train Loss: 0.0135 ce: 0.01351, vat: 0.00000, test loss: 1.81590, test acc: 0.6952\n",
      "2019-05-12 23:47:37,018 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:47,800 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 105 Train Loss: 0.0036 ce: 0.00365, vat: 0.00000, test loss: 1.76676, test acc: 0.6940\n",
      "2019-05-12 23:47:47,807 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:47:57,396 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 106 Train Loss: 0.0240 ce: 0.02405, vat: 0.00000, test loss: 1.95034, test acc: 0.6806\n",
      "2019-05-12 23:47:57,403 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:07,503 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 107 Train Loss: 0.0037 ce: 0.00370, vat: 0.00000, test loss: 1.84600, test acc: 0.6918\n",
      "2019-05-12 23:48:07,509 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:18,289 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 108 Train Loss: 0.0153 ce: 0.01529, vat: 0.00000, test loss: 1.91044, test acc: 0.6837\n",
      "2019-05-12 23:48:18,296 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:28,274 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 109 Train Loss: 0.0036 ce: 0.00361, vat: 0.00000, test loss: 1.90754, test acc: 0.6807\n",
      "2019-05-12 23:48:28,275 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:39,178 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 110 Train Loss: 0.0052 ce: 0.00525, vat: 0.00000, test loss: 1.84273, test acc: 0.6914\n",
      "2019-05-12 23:48:39,193 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:48,428 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 111 Train Loss: 0.0016 ce: 0.00161, vat: 0.00000, test loss: 1.85907, test acc: 0.6885\n",
      "2019-05-12 23:48:48,434 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:48:57,038 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 112 Train Loss: 0.0106 ce: 0.01055, vat: 0.00000, test loss: 2.03669, test acc: 0.6747\n",
      "2019-05-12 23:48:57,039 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:05,537 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 113 Train Loss: 0.0002 ce: 0.00017, vat: 0.00000, test loss: 1.80814, test acc: 0.6980\n",
      "2019-05-12 23:49:05,542 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:14,389 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 114 Train Loss: 0.0370 ce: 0.03700, vat: 0.00000, test loss: 2.02083, test acc: 0.6852\n",
      "2019-05-12 23:49:14,390 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:23,785 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 115 Train Loss: 0.0015 ce: 0.00151, vat: 0.00000, test loss: 1.89339, test acc: 0.6846\n",
      "2019-05-12 23:49:23,800 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:31,387 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 116 Train Loss: 0.0005 ce: 0.00052, vat: 0.00000, test loss: 1.95229, test acc: 0.6888\n",
      "2019-05-12 23:49:31,391 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:39,459 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 117 Train Loss: 0.0003 ce: 0.00032, vat: 0.00000, test loss: 1.87265, test acc: 0.6914\n",
      "2019-05-12 23:49:39,482 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:48,276 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 118 Train Loss: 0.0018 ce: 0.00184, vat: 0.00000, test loss: 1.97370, test acc: 0.6820\n",
      "2019-05-12 23:49:48,278 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:49:56,978 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 119 Train Loss: 0.0097 ce: 0.00966, vat: 0.00000, test loss: 1.80580, test acc: 0.7006\n",
      "2019-05-12 23:49:56,991 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:50:04,839 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 120 Train Loss: 0.0075 ce: 0.00753, vat: 0.00000, test loss: 1.82050, test acc: 0.6981\n",
      "2019-05-12 23:50:04,850 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.001000\n",
      "2019-05-12 23:50:13,937 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 121 Train Loss: 0.0009 ce: 0.00091, vat: 0.00000, test loss: 1.85617, test acc: 0.6915\n",
      "2019-05-12 23:50:13,939 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000988\n",
      "2019-05-12 23:50:22,190 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 122 Train Loss: 0.0010 ce: 0.00100, vat: 0.00000, test loss: 1.87971, test acc: 0.6900\n",
      "2019-05-12 23:50:22,191 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000975\n",
      "2019-05-12 23:50:31,776 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 123 Train Loss: 0.0099 ce: 0.00986, vat: 0.00000, test loss: 1.90178, test acc: 0.6910\n",
      "2019-05-12 23:50:31,781 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000963\n",
      "2019-05-12 23:50:40,439 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 124 Train Loss: 0.0004 ce: 0.00044, vat: 0.00000, test loss: 1.81711, test acc: 0.6972\n",
      "2019-05-12 23:50:40,446 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000950\n",
      "2019-05-12 23:50:49,562 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 125 Train Loss: 0.0013 ce: 0.00128, vat: 0.00000, test loss: 1.96378, test acc: 0.6748\n",
      "2019-05-12 23:50:49,564 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000937\n",
      "2019-05-12 23:50:57,446 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 126 Train Loss: 0.0004 ce: 0.00043, vat: 0.00000, test loss: 1.90209, test acc: 0.6997\n",
      "2019-05-12 23:50:57,449 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000925\n",
      "2019-05-12 23:51:06,272 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 127 Train Loss: 0.0038 ce: 0.00383, vat: 0.00000, test loss: 1.91211, test acc: 0.6949\n",
      "2019-05-12 23:51:06,274 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000913\n",
      "2019-05-12 23:51:15,821 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 128 Train Loss: 0.0800 ce: 0.08002, vat: 0.00000, test loss: 1.98784, test acc: 0.6865\n",
      "2019-05-12 23:51:15,822 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000900\n",
      "2019-05-12 23:51:26,166 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 129 Train Loss: 0.0200 ce: 0.02003, vat: 0.00000, test loss: 1.89591, test acc: 0.6926\n",
      "2019-05-12 23:51:26,171 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000887\n",
      "2019-05-12 23:51:35,717 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 130 Train Loss: 0.0017 ce: 0.00166, vat: 0.00000, test loss: 2.01471, test acc: 0.6831\n",
      "2019-05-12 23:51:35,723 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000875\n",
      "2019-05-12 23:51:46,658 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 131 Train Loss: 0.0008 ce: 0.00080, vat: 0.00000, test loss: 2.02666, test acc: 0.6878\n",
      "2019-05-12 23:51:46,669 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:51:57,719 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 132 Train Loss: 0.0011 ce: 0.00107, vat: 0.00000, test loss: 2.04189, test acc: 0.6854\n",
      "2019-05-12 23:51:57,721 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000850\n",
      "2019-05-12 23:52:08,776 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 133 Train Loss: 0.0121 ce: 0.01212, vat: 0.00000, test loss: 2.11726, test acc: 0.6805\n",
      "2019-05-12 23:52:08,790 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000838\n",
      "2019-05-12 23:52:19,787 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 134 Train Loss: 0.0120 ce: 0.01203, vat: 0.00000, test loss: 1.94733, test acc: 0.6910\n",
      "2019-05-12 23:52:19,792 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000825\n",
      "2019-05-12 23:52:31,552 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 135 Train Loss: 0.0057 ce: 0.00573, vat: 0.00000, test loss: 1.92403, test acc: 0.6952\n",
      "2019-05-12 23:52:31,557 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000813\n",
      "2019-05-12 23:52:43,267 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 136 Train Loss: 0.0005 ce: 0.00047, vat: 0.00000, test loss: 1.98903, test acc: 0.6933\n",
      "2019-05-12 23:52:43,269 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000800\n",
      "2019-05-12 23:52:53,899 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 137 Train Loss: 0.0035 ce: 0.00349, vat: 0.00000, test loss: 1.92505, test acc: 0.6966\n",
      "2019-05-12 23:52:53,900 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000788\n",
      "2019-05-12 23:53:05,207 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 138 Train Loss: 0.0134 ce: 0.01337, vat: 0.00000, test loss: 2.03759, test acc: 0.6905\n",
      "2019-05-12 23:53:05,217 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000775\n",
      "2019-05-12 23:53:16,212 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 139 Train Loss: 0.0015 ce: 0.00151, vat: 0.00000, test loss: 2.01710, test acc: 0.6928\n",
      "2019-05-12 23:53:16,219 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000762\n",
      "2019-05-12 23:53:26,103 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 140 Train Loss: 0.0030 ce: 0.00303, vat: 0.00000, test loss: 1.91138, test acc: 0.6995\n",
      "2019-05-12 23:53:26,109 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000750\n",
      "2019-05-12 23:53:35,752 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 141 Train Loss: 0.0001 ce: 0.00007, vat: 0.00000, test loss: 1.78000, test acc: 0.7098\n",
      "2019-05-12 23:53:35,763 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000738\n",
      "2019-05-12 23:53:45,136 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 142 Train Loss: 0.0007 ce: 0.00074, vat: 0.00000, test loss: 1.92444, test acc: 0.6917\n",
      "2019-05-12 23:53:45,143 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000725\n",
      "2019-05-12 23:53:55,476 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 143 Train Loss: 0.1205 ce: 0.12055, vat: 0.00000, test loss: 1.92773, test acc: 0.6956\n",
      "2019-05-12 23:53:55,489 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000713\n",
      "2019-05-12 23:54:03,917 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 144 Train Loss: 0.0004 ce: 0.00040, vat: 0.00000, test loss: 1.88750, test acc: 0.6996\n",
      "2019-05-12 23:54:03,925 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000700\n",
      "2019-05-12 23:54:13,483 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 145 Train Loss: 0.0041 ce: 0.00415, vat: 0.00000, test loss: 1.86589, test acc: 0.7067\n",
      "2019-05-12 23:54:13,489 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000687\n",
      "2019-05-12 23:54:22,686 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 146 Train Loss: 0.0006 ce: 0.00062, vat: 0.00000, test loss: 1.94782, test acc: 0.7014\n",
      "2019-05-12 23:54:22,687 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000675\n",
      "2019-05-12 23:54:32,970 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 147 Train Loss: 0.0001 ce: 0.00015, vat: 0.00000, test loss: 1.94657, test acc: 0.7009\n",
      "2019-05-12 23:54:32,973 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000663\n",
      "2019-05-12 23:54:41,477 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 148 Train Loss: 0.0356 ce: 0.03558, vat: 0.00000, test loss: 1.90419, test acc: 0.7001\n",
      "2019-05-12 23:54:41,485 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000650\n",
      "2019-05-12 23:54:49,623 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 149 Train Loss: 0.0001 ce: 0.00006, vat: 0.00000, test loss: 2.05887, test acc: 0.6915\n",
      "2019-05-12 23:54:49,630 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000637\n",
      "2019-05-12 23:54:57,647 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 150 Train Loss: 0.0013 ce: 0.00132, vat: 0.00000, test loss: 1.87744, test acc: 0.7024\n",
      "2019-05-12 23:54:57,649 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000625\n",
      "2019-05-12 23:55:05,451 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 151 Train Loss: 0.0091 ce: 0.00914, vat: 0.00000, test loss: 2.02533, test acc: 0.6932\n",
      "2019-05-12 23:55:05,453 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000613\n",
      "2019-05-12 23:55:13,681 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 152 Train Loss: 0.0023 ce: 0.00227, vat: 0.00000, test loss: 1.83729, test acc: 0.7117\n",
      "2019-05-12 23:55:13,691 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000600\n",
      "2019-05-12 23:55:21,538 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 153 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.92149, test acc: 0.6993\n",
      "2019-05-12 23:55:21,540 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000588\n",
      "2019-05-12 23:55:29,577 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 154 Train Loss: 0.0003 ce: 0.00035, vat: 0.00000, test loss: 1.92669, test acc: 0.7083\n",
      "2019-05-12 23:55:29,582 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000575\n",
      "2019-05-12 23:55:37,697 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 155 Train Loss: 0.0002 ce: 0.00016, vat: 0.00000, test loss: 1.88187, test acc: 0.7106\n",
      "2019-05-12 23:55:37,699 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000563\n",
      "2019-05-12 23:55:46,818 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 156 Train Loss: 0.0002 ce: 0.00018, vat: 0.00000, test loss: 1.87192, test acc: 0.7083\n",
      "2019-05-12 23:55:46,831 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000550\n",
      "2019-05-12 23:55:55,559 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 157 Train Loss: 0.0006 ce: 0.00056, vat: 0.00000, test loss: 1.93623, test acc: 0.7067\n",
      "2019-05-12 23:55:55,566 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000538\n",
      "2019-05-12 23:56:04,904 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 158 Train Loss: 0.0005 ce: 0.00052, vat: 0.00000, test loss: 2.02339, test acc: 0.6959\n",
      "2019-05-12 23:56:04,910 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000525\n",
      "2019-05-12 23:56:15,148 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 159 Train Loss: 0.0025 ce: 0.00249, vat: 0.00000, test loss: 1.94427, test acc: 0.7048\n",
      "2019-05-12 23:56:15,156 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000512\n",
      "2019-05-12 23:56:25,731 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 160 Train Loss: 0.0002 ce: 0.00015, vat: 0.00000, test loss: 1.92788, test acc: 0.7086\n",
      "2019-05-12 23:56:25,740 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000500\n",
      "2019-05-12 23:56:36,339 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 161 Train Loss: 0.0020 ce: 0.00200, vat: 0.00000, test loss: 1.94403, test acc: 0.7081\n",
      "2019-05-12 23:56:36,352 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000487\n",
      "2019-05-12 23:56:48,405 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 162 Train Loss: 0.0028 ce: 0.00276, vat: 0.00000, test loss: 1.93202, test acc: 0.7097\n",
      "2019-05-12 23:56:48,407 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000475\n",
      "2019-05-12 23:56:59,208 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 163 Train Loss: 0.0000 ce: 0.00005, vat: 0.00000, test loss: 1.97736, test acc: 0.7002\n",
      "2019-05-12 23:56:59,214 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000463\n",
      "2019-05-12 23:57:09,323 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 164 Train Loss: 0.0002 ce: 0.00024, vat: 0.00000, test loss: 1.91255, test acc: 0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 23:57:09,331 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000450\n",
      "2019-05-12 23:57:21,123 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 165 Train Loss: 0.0005 ce: 0.00046, vat: 0.00000, test loss: 2.03857, test acc: 0.6967\n",
      "2019-05-12 23:57:21,137 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000438\n",
      "2019-05-12 23:57:33,243 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 166 Train Loss: 0.0007 ce: 0.00071, vat: 0.00000, test loss: 1.92666, test acc: 0.7114\n",
      "2019-05-12 23:57:33,253 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000425\n",
      "2019-05-12 23:57:44,153 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 167 Train Loss: 0.0056 ce: 0.00561, vat: 0.00000, test loss: 1.92602, test acc: 0.6958\n",
      "2019-05-12 23:57:44,167 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000412\n",
      "2019-05-12 23:57:56,113 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 168 Train Loss: 0.0001 ce: 0.00011, vat: 0.00000, test loss: 1.92666, test acc: 0.7060\n",
      "2019-05-12 23:57:56,128 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000400\n",
      "2019-05-12 23:58:07,433 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 169 Train Loss: 0.0060 ce: 0.00600, vat: 0.00000, test loss: 2.00207, test acc: 0.7033\n",
      "2019-05-12 23:58:07,440 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000388\n",
      "2019-05-12 23:58:19,231 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 170 Train Loss: 0.0023 ce: 0.00230, vat: 0.00000, test loss: 1.87922, test acc: 0.7139\n",
      "2019-05-12 23:58:19,234 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000375\n",
      "2019-05-12 23:58:29,732 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 171 Train Loss: 0.0038 ce: 0.00377, vat: 0.00000, test loss: 1.95481, test acc: 0.7104\n",
      "2019-05-12 23:58:29,733 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000362\n",
      "2019-05-12 23:58:40,781 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 172 Train Loss: 0.0001 ce: 0.00011, vat: 0.00000, test loss: 1.87852, test acc: 0.7147\n",
      "2019-05-12 23:58:40,782 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000350\n",
      "2019-05-12 23:58:50,070 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 173 Train Loss: 0.0001 ce: 0.00007, vat: 0.00000, test loss: 1.84909, test acc: 0.7187\n",
      "2019-05-12 23:58:50,076 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000338\n",
      "2019-05-12 23:58:58,627 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 174 Train Loss: 0.0007 ce: 0.00068, vat: 0.00000, test loss: 1.88443, test acc: 0.7112\n",
      "2019-05-12 23:58:58,632 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000325\n",
      "2019-05-12 23:59:08,439 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 175 Train Loss: 0.0001 ce: 0.00010, vat: 0.00000, test loss: 2.01252, test acc: 0.7024\n",
      "2019-05-12 23:59:08,444 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000313\n",
      "2019-05-12 23:59:18,496 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 176 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.93162, test acc: 0.7154\n",
      "2019-05-12 23:59:18,507 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000300\n",
      "2019-05-12 23:59:28,594 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 177 Train Loss: 0.0000 ce: 0.00002, vat: 0.00000, test loss: 1.90260, test acc: 0.7135\n",
      "2019-05-12 23:59:28,605 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000287\n",
      "2019-05-12 23:59:38,075 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 178 Train Loss: 0.0000 ce: 0.00005, vat: 0.00000, test loss: 1.89641, test acc: 0.7145\n",
      "2019-05-12 23:59:38,082 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000275\n",
      "2019-05-12 23:59:47,575 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 179 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.93238, test acc: 0.7180\n",
      "2019-05-12 23:59:47,586 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000263\n",
      "2019-05-12 23:59:56,217 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 180 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.95490, test acc: 0.7063\n",
      "2019-05-12 23:59:56,233 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000250\n",
      "2019-05-13 00:00:05,696 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 181 Train Loss: 0.0002 ce: 0.00016, vat: 0.00000, test loss: 1.97998, test acc: 0.7100\n",
      "2019-05-13 00:00:05,698 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000237\n",
      "2019-05-13 00:00:13,972 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 182 Train Loss: 0.0022 ce: 0.00216, vat: 0.00000, test loss: 1.91844, test acc: 0.7104\n",
      "2019-05-13 00:00:13,973 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000225\n",
      "2019-05-13 00:00:22,563 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 183 Train Loss: 0.0002 ce: 0.00021, vat: 0.00000, test loss: 1.97622, test acc: 0.7072\n",
      "2019-05-13 00:00:22,566 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000212\n",
      "2019-05-13 00:00:31,269 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 184 Train Loss: 0.0001 ce: 0.00006, vat: 0.00000, test loss: 1.87776, test acc: 0.7209\n",
      "2019-05-13 00:00:31,277 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000200\n",
      "2019-05-13 00:00:40,248 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 185 Train Loss: 0.0012 ce: 0.00123, vat: 0.00000, test loss: 1.96822, test acc: 0.7096\n",
      "2019-05-13 00:00:40,249 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000188\n",
      "2019-05-13 00:00:48,558 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 186 Train Loss: 0.0001 ce: 0.00013, vat: 0.00000, test loss: 1.92856, test acc: 0.7134\n",
      "2019-05-13 00:00:48,559 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000175\n",
      "2019-05-13 00:00:57,324 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 187 Train Loss: 0.0002 ce: 0.00025, vat: 0.00000, test loss: 1.95324, test acc: 0.7102\n",
      "2019-05-13 00:00:57,326 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000163\n",
      "2019-05-13 00:01:06,826 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 188 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.93190, test acc: 0.7157\n",
      "2019-05-13 00:01:06,828 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000150\n",
      "2019-05-13 00:01:16,405 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 189 Train Loss: 0.0000 ce: 0.00000, vat: 0.00000, test loss: 1.89493, test acc: 0.7185\n",
      "2019-05-13 00:01:16,407 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000138\n",
      "2019-05-13 00:01:25,776 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 190 Train Loss: 0.0000 ce: 0.00003, vat: 0.00000, test loss: 1.91791, test acc: 0.7146\n",
      "2019-05-13 00:01:25,782 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000125\n",
      "2019-05-13 00:01:35,704 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 191 Train Loss: 0.0002 ce: 0.00018, vat: 0.00000, test loss: 1.87955, test acc: 0.7151\n",
      "2019-05-13 00:01:35,712 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000113\n",
      "2019-05-13 00:01:44,105 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 192 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.92993, test acc: 0.7131\n",
      "2019-05-13 00:01:44,107 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000100\n",
      "2019-05-13 00:01:54,171 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 193 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.92797, test acc: 0.7202\n",
      "2019-05-13 00:01:54,172 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000087\n",
      "2019-05-13 00:02:04,384 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 194 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.90792, test acc: 0.7227\n",
      "2019-05-13 00:02:04,385 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000075\n",
      "2019-05-13 00:02:14,852 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 195 Train Loss: 0.0000 ce: 0.00004, vat: 0.00000, test loss: 1.88716, test acc: 0.7178\n",
      "2019-05-13 00:02:14,856 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000063\n",
      "2019-05-13 00:02:25,351 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 196 Train Loss: 0.0001 ce: 0.00010, vat: 0.00000, test loss: 1.89131, test acc: 0.7198\n",
      "2019-05-13 00:02:25,356 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000050\n",
      "2019-05-13 00:02:36,466 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 197 Train Loss: 0.0001 ce: 0.00008, vat: 0.00000, test loss: 1.92308, test acc: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 00:02:36,492 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000037\n",
      "2019-05-13 00:02:47,150 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 198 Train Loss: 0.0003 ce: 0.00031, vat: 0.00000, test loss: 1.92160, test acc: 0.7180\n",
      "2019-05-13 00:02:47,152 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000025\n",
      "2019-05-13 00:02:59,866 - <ipython-input-7-bdc23bc4b139>[line:64]: Epoch: 199 Train Loss: 0.0000 ce: 0.00001, vat: 0.00000, test loss: 1.88841, test acc: 0.7177\n",
      "2019-05-13 00:02:59,875 - <ipython-input-7-bdc23bc4b139>[line:72]: learning rate 0.000013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arg.exp = \"avg\"\n",
    "arg.dropout_rate = 0.0\n",
    "\n",
    "wlog(\"args in this experiment %s\", '\\n'.join(str(e) for e in sorted(vars(arg).items())))\n",
    "# noinspection PyBroadException\n",
    "\n",
    "acc_z_list = train(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb18eb9be80>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(acc_list)), acc_z_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
