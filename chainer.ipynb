{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-9.0/lib64/'\n",
    "sys.path.append('/usr/local/cuda-9.0/lib64/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "import math\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "import sys, os, time, argparse\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import Variable, optimizers, cuda, serializers\n",
    "\n",
    "from chainer_func.source.chainer_functions import loss\n",
    "from chainer_func.source.data import Data\n",
    "from chainer_func.source.utils import mkdir_p, load_npz_as_dict\n",
    "from chainer_func.models import CNN, MLP\n",
    "\n",
    "from ExpUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"mnist\"\n",
    "    args.trainer = \"VATReg\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"mlp\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 10\n",
    "    args.eps = 1\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    \n",
    "    args.gpu = -1\n",
    "    args.data_dir = \"./dataset/cifar10/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 100\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 100\n",
    "    args.num_iter_per_epoch = 1\n",
    "    args.epoch_decay_start = 80\n",
    "    args.lr = 0.001\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 3.5\n",
    "    args.extra_lamb = 1\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = True\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "    args.data_dir = os.path.join(os.environ['HOME'], \"project/data/dataset/%s\" % args.dataset)\n",
    "\n",
    "    chainer.global_config.cudnn_deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    if int(args.gpu) > -1:\n",
    "        chainer.cuda.get_device(args.gpu).use()\n",
    "    np.random.seed(args.seed)\n",
    "    cp.random.seed(args.seed)\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_labeled:100, N_train_unlabeled:60000\n",
      "10491.246 0.13381691 0.99609375\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "train_l, train_ul, test = load_dataset(args.data_dir, valid=args.validation, dataset_seed=args.dataset_seed, size=100)\n",
    "print(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "print(train_l.data.sum(), train_l.data.mean(), train_l.data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 3, 9, 6, 1, 2, 8, 5, 3, 3, 6, 7, 9, 4, 2, 6, 2, 7, 1, 1, 9,\n",
       "       9, 0, 5, 4, 0, 1, 5, 4, 8, 7, 4, 4, 3, 0, 0, 6, 1, 0, 4, 8, 2, 2,\n",
       "       0, 7, 4, 7, 3, 9, 9, 2, 6, 0, 2, 3, 1, 0, 3, 8, 2, 5, 0, 7, 7, 7,\n",
       "       4, 6, 7, 6, 5, 5, 8, 8, 9, 1, 8, 5, 2, 6, 9, 5, 3, 2, 4, 6, 3, 8,\n",
       "       1, 9, 3, 6, 5, 5, 7, 4, 8, 1, 0, 8], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 4 5 5 2 0 3 6 7 5 0 1 6 2 4 7 0 2 3 3 8 1 0 1 0 7 1 4 1 6 8 2 4 1 4 3\n",
      " 8 0 3 3 0 5 2 2 8 7 6 0 0 2 3 5 6 3 9 9 8 9 9 8 4 5 2 7 9 5 2 1 3 0 9 5 7\n",
      " 8 8 9 7 6 4 4 5 6 7 1 9 4 2 5 8 9 6 7 6 1 1 3 8 7 6]\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, test=False, update_batch_stats=True):\n",
    "    if test:\n",
    "        return F.fixed_batch_normalization(x, bn.gamma, bn.beta, bn.avg_mean, bn.avg_var)\n",
    "    elif not update_batch_stats:\n",
    "        return F.batch_normalization(x, bn.gamma, bn.beta)\n",
    "    else:\n",
    "        return bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_ch=1, n_res=28, n_outputs=10, dropout_rate=0.5, top_bn=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        super(MLP, self).__init__(\n",
    "            l_c1=L.Linear(n_ch * n_res * n_res, 1200, initialW=initializer),\n",
    "            l_c2=L.Linear(1200, 1200, initialW=initializer),\n",
    "            l_c3=L.Linear(1200, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(1200),\n",
    "            bn2=L.BatchNormalization(1200),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.l_c1(h)\n",
    "        h = F.relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        h = self.l_c2(h)\n",
    "        h = F.relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        logit = self.l_c3(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable gamma([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match MLP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10491.246\n",
      "[9 4 4 5 5 2 0 3 6 7 5 0 1 6 2 4 7 0 2 3 3 8 1 0 1 0 7 1 4 1 6 8 2 4 1 4 3\n",
      " 8 0 3 3 0 5 2 2 8 7 6 0 0 2 3 5 6 3 9 9 8 9 9 8 4 5 2 7 9 5 2 1 3 0 9 5 7\n",
      " 8 8 9 7 6 4 4 5 6 7 1 9 4 2 5 8 9 6 7 6 1 1 3 8 7 6]\n",
      "-139.04709\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "print(x.sum())\n",
    "print(t)\n",
    "set_framework_seed(1)\n",
    "te = enc(Variable(x))\n",
    "print(te.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_labeled(enc, Variable(x), t)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, enc, test, args):\n",
    "    with chainer.using_config(\"train\", False):\n",
    "        acc_test_sum = 0\n",
    "        test_x, test_t = test.get()\n",
    "        N_test = test_x.shape[0]\n",
    "        for i in range(0, N_test, args.batchsize_eval):\n",
    "            x = test_x[i:i + args.batchsize_eval]\n",
    "            t = test_t[i:i + args.batchsize_eval]\n",
    "            if args.gpu > -1:\n",
    "                x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "            _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "            acc_test_sum += acc * x.shape[0]\n",
    "        accs_test = acc_test_sum / N_test\n",
    "        if epoch < 5 or epoch % 10 == 0:\n",
    "            wlog(\"Epoch:{}, nll loss:{}\".format(epoch, cl_losses[epoch]))\n",
    "            wlog(\"test acc:{}\".format(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 16:23:03,625 - <ipython-input-62-239c4b580332>[line:2]: N_train_labeled:100, N_train_unlabeled:60000\n",
      "2019-03-06 16:23:04,938 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:0, nll loss:2.476536750793457\n",
      "2019-03-06 16:23:04,939 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.46589999765157697\n",
      "2019-03-06 16:23:06,299 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:1, nll loss:0.2439270168542862\n",
      "2019-03-06 16:23:06,301 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.5460999998450279\n",
      "2019-03-06 16:23:07,580 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:2, nll loss:0.0213171374052763\n",
      "2019-03-06 16:23:07,581 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.5950999960303307\n",
      "2019-03-06 16:23:08,835 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:3, nll loss:0.004524578806012869\n",
      "2019-03-06 16:23:08,837 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.6248999983072281\n",
      "2019-03-06 16:23:10,134 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:4, nll loss:0.0017549371114000678\n",
      "2019-03-06 16:23:10,135 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.6518000012636185\n",
      "2019-03-06 16:23:17,099 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:10, nll loss:0.00028099058545194566\n",
      "2019-03-06 16:23:17,101 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.709400002360344\n",
      "2019-03-06 16:23:29,445 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:20, nll loss:0.00016156196943484247\n",
      "2019-03-06 16:23:29,447 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7314000028371811\n",
      "2019-03-06 16:23:41,800 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:30, nll loss:0.0001131248427554965\n",
      "2019-03-06 16:23:41,803 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.737700001001358\n",
      "2019-03-06 16:23:53,769 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:40, nll loss:8.460044773528352e-05\n",
      "2019-03-06 16:23:53,770 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7403000003099441\n",
      "2019-03-06 16:24:05,615 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:50, nll loss:6.760597170796245e-05\n",
      "2019-03-06 16:24:05,617 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7411999982595444\n",
      "2019-03-06 16:24:17,296 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:60, nll loss:5.678176603396423e-05\n",
      "2019-03-06 16:24:17,297 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7432000011205673\n",
      "2019-03-06 16:24:29,001 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:70, nll loss:4.938125493936241e-05\n",
      "2019-03-06 16:24:29,003 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.743200004696846\n",
      "2019-03-06 16:24:40,917 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:80, nll loss:4.3830870708916336e-05\n",
      "2019-03-06 16:24:40,920 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7442000007629395\n",
      "2019-03-06 16:24:52,970 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:90, nll loss:3.9625167119083926e-05\n",
      "2019-03-06 16:24:52,973 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7446999990940094\n"
     ]
    }
   ],
   "source": [
    "train_l, train_ul, test = load_dataset(args.data_dir, valid=args.validation, dataset_seed=args.dataset_seed, size=args.size)\n",
    "wlog(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "    \n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "\n",
    "cl_losses = np.zeros(args.num_epochs)\n",
    "for epoch in range(args.num_epochs):\n",
    "    sum_loss_l = 0\n",
    "    sum_loss_ul = 0\n",
    "    for it in range(args.num_iter_per_epoch):\n",
    "        with chainer.using_config(\"train\", True):\n",
    "            x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            loss_total = loss_labeled(enc, Variable(x), t)\n",
    "            enc.cleargrads()\n",
    "            loss_total.backward()\n",
    "            optimizer.update()\n",
    "            sum_loss_l += loss_total.data\n",
    "        cl_losses[epoch] = sum_loss_l / args.num_iter_per_epoch\n",
    "    evaluate(epoch, enc, test, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        super(CNN, self).__init__(\n",
    "            c1=L.Convolution2D(3, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c2=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c3=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c4=L.Convolution2D(128, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c5=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c6=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c7=L.Convolution2D(256, 512, ksize=3, stride=1, pad=0, initialW=initializer),\n",
    "            c8=L.Convolution2D(512, 256, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            c9=L.Convolution2D(256, 128, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            l_cl=L.Linear(128, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(128),\n",
    "            bn2=L.BatchNormalization(128),\n",
    "            bn3=L.BatchNormalization(128),\n",
    "            bn4=L.BatchNormalization(256),\n",
    "            bn5=L.BatchNormalization(256),\n",
    "            bn6=L.BatchNormalization(256),\n",
    "            bn7=L.BatchNormalization(512),\n",
    "            bn8=L.BatchNormalization(256),\n",
    "            bn9=L.BatchNormalization(128),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.average_pooling_2d(h, ksize=h.data.shape[2])\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 40.848587\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "out = enc( Variable(x), update_batch_stats=True)\n",
    "print(x.sum(), out.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_labeled(enc, Variable(x), t)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN without dropout\n",
    "\n",
    "the results are very close, the difference is caused by error/precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.260621)\n",
      "-239.57237 variable(2.737069)\n",
      "2.0764809 variable(2.6666842)\n",
      "126.50333 variable(2.240642)\n",
      "31.462202 variable(2.0930262)\n",
      "-653.71985 variable(2.2371545)\n",
      "-206.56343 variable(2.0542524)\n",
      "-199.04095 variable(2.359859)\n",
      "173.35344 variable(2.3512278)\n",
      "-129.28891 variable(2.3139074)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10249999959021806\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.270143)\n",
      "-239.57237 variable(2.5744624)\n",
      "2.0764809 variable(2.8344507)\n",
      "126.50333 variable(2.2541957)\n",
      "31.462202 variable(2.2864416)\n",
      "-653.71985 variable(2.2899199)\n",
      "-206.56343 variable(2.2821083)\n",
      "-199.04095 variable(2.5041566)\n",
      "173.35344 variable(2.545939)\n",
      "-129.28891 variable(2.350461)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    set_framework_seed(it % 10000)\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10439999988302588\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064\n",
      "[0 2 1 5 2 6 7 2 3 2 0 8 2 4 3 5 1 6 3 3 3 2 8 1 7 0 0 6 6 3 3 1]\n",
      "-5.4525633\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "print(x.sum())\n",
    "print(t)\n",
    "set_framework_seed(1)\n",
    "te = enc(Variable(x))\n",
    "print(te.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.4525633\n"
     ]
    }
   ],
   "source": [
    "x_variable = Variable(x)\n",
    "te = enc(x_variable)\n",
    "print(te.data.sum())\n",
    "logits = te\n",
    "\n",
    "new_data = F.repeat(logits.reshape(32, 10, 1), 10, axis=2)\n",
    "new_data_t = F.transpose(new_data, axes=(0, 2, 1))\n",
    "dif = F.absolute(new_data - new_data_t)\n",
    "dif.data[:, np.arange(10), np.arange(10)] = 10000\n",
    "minimum_dif = F.min(dif, axis=(1, 2))\n",
    "loss = F.sum(minimum_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "d = x_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 32, 32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([43.8882  , 32.518997, 42.099136, 42.29032 , 35.56052 , 32.028145,\n",
       "          44.87703 , 35.137177, 42.562088, 49.06534 , 37.460518, 56.70897 ,\n",
       "          48.79129 , 58.22379 , 58.045277, 53.70053 , 44.11779 , 36.081795,\n",
       "          57.63233 , 31.73954 , 48.41804 , 42.846928, 40.61837 , 42.383553,\n",
       "          47.325497, 34.945408, 45.08779 , 55.141594, 45.598877, 52.274117,\n",
       "          45.350273, 36.295547])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.batch_l2_norm_squared(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.8882  , 32.518997, 42.099136, 42.29032 , 35.56052 , 32.028145,\n",
       "       44.87703 , 35.137177, 42.562088, 49.06534 , 37.460518, 56.70897 ,\n",
       "       48.79129 , 58.22379 , 58.045277, 53.70053 , 44.11779 , 36.081795,\n",
       "       57.63233 , 31.73954 , 48.41804 , 42.846928, 40.61837 , 42.383553,\n",
       "       47.325497, 34.945408, 45.08779 , 55.141594, 45.598877, 52.274117,\n",
       "       45.350273, 36.295547], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.sum(d ** 2, axis =tuple(range(1, len(d.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.get_array_module(x_variable.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d / xp.sqrt(xp.sum(d ** 2, axis =tuple(range(1, len(d.shape))), keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 32, 32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.03196675, 0.01531726, 0.03652483, 0.01392016, 0.01174247,\n",
       "          0.0463711 , 0.04788411, 0.01914483, 0.02834719, 0.05728129,\n",
       "          0.00792599, 0.01005088, 0.00395525, 0.06017971, 0.01658297,\n",
       "          0.05855221, 0.03900647, 0.00564754, 0.00618389, 0.0116004 ,\n",
       "          0.03603446, 0.03971651, 0.01833946, 0.01108512, 0.00718176,\n",
       "          0.01502152, 0.17572033, 0.00577849, 0.00456291, 0.0354563 ,\n",
       "          0.01088476, 0.10022384])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.0048253 , 0.00268604, 0.00562926, 0.00214054, 0.00196914,\n",
       "          0.00819373, 0.00714791, 0.00322974, 0.00434509, 0.00817759,\n",
       "          0.00129499, 0.00133468, 0.00056624, 0.00788678, 0.0021766 ,\n",
       "          0.00799013, 0.00587259, 0.00094019, 0.00081457, 0.00205908,\n",
       "          0.00517863, 0.00606752, 0.00287757, 0.00170271, 0.00104396,\n",
       "          0.00254108, 0.02616932, 0.00077817, 0.00067572, 0.004904  ,\n",
       "          0.00161633, 0.01663583])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif / xp.sqrt(xp.sum(d ** 2, axis =tuple(range(1, len(d.shape)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob = F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[1.54169023e-01, 5.03710285e-02, 8.62801373e-02, 5.62610142e-02,\n",
       "           7.01920614e-02, 4.87862900e-02, 1.48522973e-01, 1.27890825e-01,\n",
       "           1.61214247e-01, 9.63124484e-02],\n",
       "          [1.34237275e-01, 5.56553602e-02, 1.00720234e-01, 1.13789409e-01,\n",
       "           1.36309236e-01, 9.80082080e-02, 1.54791638e-01, 6.56634644e-02,\n",
       "           1.04473017e-01, 3.63521986e-02],\n",
       "          [1.61101922e-01, 2.13813096e-01, 6.77430443e-03, 3.02409520e-03,\n",
       "           7.02630868e-03, 9.21606726e-04, 9.70342234e-02, 5.04525900e-01,\n",
       "           4.32566099e-04, 5.34597319e-03]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prob[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class_prob = class_prob[np.arange(class_prob.shape[0]), t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.15416902, 0.10072023, 0.2138131 , 0.08669429, 0.32581642,\n",
       "          0.0384522 , 0.23383127, 0.10946563, 0.01286336, 0.10806721,\n",
       "          0.03154936, 0.0925095 , 0.07900738, 0.02592808, 0.04358358,\n",
       "          0.09468471, 0.2182813 , 0.15935653, 0.01923206, 0.09014764,\n",
       "          0.08973202, 0.15906514, 0.25026926, 0.0619849 , 0.08292997,\n",
       "          0.09619284, 0.05878942, 0.04833115, 0.19643797, 0.08501525,\n",
       "          0.03976822, 0.28050393])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_margin(_sentinel=None, logits=None, one_hot_labels=None, layers_list=None, gamma=10000, alpha_factor=2, top_k=1, dist_norm=2,\n",
    "                 epsilon=1e-8, use_approximation=True, loss_type=\"all_top_k\", loss_collection=tf.GraphKeys.LOSSES):\n",
    "    # Pick the correct class probability.\n",
    "    correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "    # Class probabilities except the correct.\n",
    "    other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "    if top_k > 1:\n",
    "        # Pick the top k class probabilities other than the correct.\n",
    "        top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "    else:\n",
    "        top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "    # Difference between correct class probailities and top_k probabilities.\n",
    "    difference_prob = correct_class_prob - top_k_class_prob\n",
    "    losses_list = []\n",
    "\n",
    "    difference_prob_grad = [\n",
    "        tf.layers.flatten(tf.gradients(difference_prob[:, i], layer)[0])\n",
    "        for i in range(top_k)\n",
    "    ]\n",
    "\n",
    "    difference_prob_gradnorm = tf.concat([\n",
    "        tf.map_fn(norm_fn, difference_prob_grad[i])[:, tf.newaxis]\n",
    "        for i in range(top_k)\n",
    "    ], axis=1)\n",
    "\n",
    "    if use_approximation:\n",
    "        difference_prob_gradnorm = tf.stop_gradient(difference_prob_gradnorm)\n",
    "\n",
    "    distance_to_boundary = difference_prob / (\n",
    "                    difference_prob_gradnorm + epsilon)\n",
    "\n",
    "    if loss_type == \"worst_top_k\":\n",
    "        # Only consider worst distance to boundary.\n",
    "        distance_to_boundary = tf.reduce_min(distance_to_boundary, axis=1)\n",
    "\n",
    "    elif loss_type == \"average_top_k\":\n",
    "        # Only consider average distance to boundary.\n",
    "        distance_to_boundary = tf.reduce_mean(distance_to_boundary, axis=1)\n",
    "\n",
    "    # Distances to consider between distance_upper and distance_lower bounds\n",
    "    distance_upper = gamma\n",
    "    distance_lower = gamma * (1 - alpha_factor)\n",
    "\n",
    "    # Enforce lower bound.\n",
    "    loss_layer = maximum_with_relu(distance_to_boundary, distance_lower)\n",
    "\n",
    "    # Enforce upper bound.\n",
    "    loss_layer = maximum_with_relu(\n",
    "        0, distance_upper - loss_layer) - distance_upper\n",
    "\n",
    "    losses_list.append(tf.reduce_mean(loss_layer))\n",
    "\n",
    "    loss = tf.reduce_mean(losses_list)\n",
    "    # Add loss to loss_collection.\n",
    "    tf.losses.add_loss(loss, loss_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
