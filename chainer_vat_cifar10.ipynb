{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-9.0/lib64/'\n",
    "sys.path.append('/usr/local/cuda-9.0/lib64/')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "gpu = \"\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "tf.enable_eager_execution(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tf_func import data_loader\n",
    "from tf_func import mnist_model\n",
    "\n",
    "\n",
    "class ConfigDict(object):\n",
    "    \"\"\"MNIST configration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # List of tuples specify (kernel_size, number of filters) for each layer.\n",
    "        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\n",
    "        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\n",
    "        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\n",
    "        self.num_units_fc_layers = [512]\n",
    "        self.dropout_rate = 0\n",
    "        self.batch_norm = True\n",
    "        self.activation = None\n",
    "        self.regularizer = None\n",
    "        \n",
    "        \n",
    "config = ConfigDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"train\",\n",
    "    batch_size=128,\n",
    "    is_training=False)\n",
    "\n",
    "test_dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"test\",\n",
    "    batch_size=128,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12790.145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels, num_examples, num_classes = (dataset.images, dataset.labels, dataset.num_examples, dataset.num_classes)\n",
    "images, labels = dataset.get_next()\n",
    "images.numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "import sys, os, time, argparse\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Variable, optimizers, cuda, serializers\n",
    "\n",
    "# from source.chainer_functions import loss\n",
    "# from source.data import Data\n",
    "# from source.utils import mkdir_p, load_npz_as_dict\n",
    "# from models.cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"mnist\"\n",
    "    args.trainer = \"VATReg\"\n",
    "    args.lr = 0.002\n",
    "    args.arch = \"mlp\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 10\n",
    "    args.eps = 1\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    \n",
    "    args.gpu = -1\n",
    "    args.data_dir = \"./dataset/svhn/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 100\n",
    "    args.batchsize_ul = 250\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 120\n",
    "    args.num_iter_per_epoch = 500\n",
    "    args.epoch_decay_start = 0\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 0.3\n",
    "    args.extra_lamb = 1\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = True\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "    args.data_dir = os.path.join(\"./dataset/%s\" % args.dataset)\n",
    "\n",
    "    chainer.global_config.cudnn_deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    if int(args.gpu) > -1:\n",
    "        chainer.cuda.get_device(args.gpu).use()\n",
    "    np.random.seed(args.seed)\n",
    "    cp.random.seed(args.seed)\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc\n",
    "\n",
    "def set_framework_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, test_iter):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with chainer.using_config(\"train\", False):\n",
    "        for images, labels in test_iter:\n",
    "            size += images.numpy().shape[0]\n",
    "            images = images.numpy()\n",
    "            labels = labels.numpy()\n",
    "            loss, acc = loss_test(model, images, labels)\n",
    "            total_loss += loss * images.shape[0]\n",
    "            \n",
    "            total_acc += acc * images.shape[0]\n",
    "    \n",
    "    return total_acc / size, total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, test=False, update_batch_stats=True):\n",
    "    if test:\n",
    "        return F.fixed_batch_normalization(x, bn.gamma, bn.beta, bn.avg_mean, bn.avg_var)\n",
    "    elif not update_batch_stats:\n",
    "        return F.batch_normalization(x, bn.gamma, bn.beta)\n",
    "    else:\n",
    "        return bn(x)\n",
    "    \n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1)\n",
    "        super(MLP, self).__init__(\n",
    "            c1=L.Linear(784, 1200, initialW=initializer),\n",
    "            c2=L.Linear(1200, 1200, initialW=initializer),\n",
    "            l_cl=L.Linear(1200, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(1200),\n",
    "            bn2=L.BatchNormalization(1200),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        h = self.c2(h)\n",
    "        h = F.relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits 1279.9625\n",
      "True\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.3949525)\n",
      "logits 1279.9597\n",
      "True\n",
      "it 1 ce loss variable(2.447002) vat loss variable(0.59222496)\n",
      "logits 1279.9594\n",
      "True\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(0.60971195)\n",
      "logits 1279.9573\n",
      "True\n",
      "it 3 ce loss variable(2.6826663) vat loss variable(0.62293935)\n",
      "logits 1279.9609\n",
      "True\n",
      "it 4 ce loss variable(2.6122522) vat loss variable(0.6236712)\n",
      "test loss 2.315154103132395 test acc 0.11057692307692307\n"
     ]
    }
   ],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    print(\"logits\", (y.data ** 2).sum())\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "\n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits 6.67572e-06\n",
      "it 0 ce loss variable(2.6372051) vat loss 0\n",
      "logits -6.389618e-05\n",
      "it 1 ce loss variable(1.0207927) vat loss 0\n",
      "logits 0.0028562546\n",
      "it 2 ce loss variable(0.9196422) vat loss 0\n",
      "logits 0.026527405\n",
      "it 3 ce loss variable(0.9217887) vat loss 0\n",
      "logits 0.05603504\n",
      "it 4 ce loss variable(0.94327796) vat loss 0\n",
      "test loss 0.9446233698190787 test acc 0.7655248397435898\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul = 0\n",
    "        #loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 3101.001574004157 127.99999951050698\n",
      "input 12790.146\n",
      "output 1.7166138e-05\n",
      "loss variable(-1.1133411e-08)\n",
      "d -0.9175085\n",
      "post cost variable(0.18286765)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18286765)\n",
      "d 3104.2961100598077 127.99999951136697\n",
      "input 12953.986\n",
      "output 1.9073486e-06\n",
      "loss variable(0.77622306)\n",
      "d -0.025210619\n",
      "post cost variable(1.2601826)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(1.2601826)\n",
      "d 3103.1908931663115 127.99999951057141\n",
      "input 12442.994\n",
      "output -1.4781952e-05\n",
      "loss variable(0.8542405)\n",
      "d 9.233387\n",
      "post cost variable(1.3496399)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(1.3496399)\n",
      "test loss 2.316648061458881 test acc 0.1141826923076923\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "debug = True\n",
    "def vat_loss(forward, distance, x, y=None, train=True, epsilon=8.0,\n",
    "             xi=1e-6, num_iter=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = np.random.random(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    if debug:\n",
    "        print(\"d\", d.sum(), (d ** 2).sum())\n",
    "        \n",
    "    for ip in range(num_iter):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        if debug:\n",
    "            print(\"input\", x_d.data.sum())\n",
    "        p_d_logit = forward(x_d, train=train, update_batch_stats=True)\n",
    "        if debug:\n",
    "            print(\"output\", p_d_logit.data.sum())\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        if debug:\n",
    "            print(\"loss\", kl_loss)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        if debug:\n",
    "            print(\"d\", d.sum())\n",
    "    x_adv = x + epsilon * d\n",
    "\n",
    "    p_adv_logit = forward(x_adv, train=train, update_batch_stats=False)\n",
    "    pos_cost = distance(p_logit, p_adv_logit)\n",
    "    if debug:\n",
    "        print(\"post cost\", pos_cost)\n",
    "    return pos_cost, p_d_logit\n",
    "\n",
    "for it in range(3):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, test=False, update_batch_stats=True):\n",
    "    if test:\n",
    "        return F.fixed_batch_normalization(x, bn.gamma, bn.beta, bn.avg_mean, bn.avg_var)\n",
    "    elif not update_batch_stats:\n",
    "        return F.batch_normalization(x, bn.gamma, bn.beta)\n",
    "    else:\n",
    "        return bn(x)\n",
    "    \n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1)\n",
    "        super(MLP, self).__init__(\n",
    "            c1=L.Linear(784, 1200, initialW=initializer),\n",
    "            c2=L.Linear(1200, 1200, initialW=initializer),\n",
    "            l_cl=L.Linear(1200, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(1200),\n",
    "            bn2=L.BatchNormalization(1200),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        h = self.c2(h)\n",
    "        h = F.relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12442.993\n",
      "2.4318695e-05\n",
      "2.4318695e-05\n",
      "68.56414\n",
      "68.56414\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "\n",
    "model = enc\n",
    "print(images.numpy().sum())\n",
    "x = images.numpy()\n",
    "p = model(x, train=True, update_batch_stats=True)\n",
    "print(p.data.sum())\n",
    "p = model(x, train=True, update_batch_stats=False)\n",
    "print(p.data.sum())\n",
    "p = model(x, train=False, update_batch_stats=True)\n",
    "print(p.data.sum())\n",
    "\n",
    "p = model(x, train=False, update_batch_stats=False)\n",
    "print(p.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 3101.001574004157 127.99999951050698\n",
      "True\n",
      "input 12790.146\n",
      "output 60.197327\n",
      "loss variable(0.27454972)\n",
      "grad d 0.16488276\n",
      "d 51.43709\n",
      "post cost variable(0.3949525)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.3949525)\n",
      "d 3104.2961100598077 127.99999951136697\n",
      "True\n",
      "input 12953.986\n",
      "output 68.14215\n",
      "loss variable(0.43572783)\n",
      "grad d 0.07118307\n",
      "d 14.237335\n",
      "post cost variable(0.59222496)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(0.59222496)\n",
      "d 3103.1908931663115 127.99999951057141\n",
      "True\n",
      "input 12442.994\n",
      "output 47.62139\n",
      "loss variable(0.43799925)\n",
      "grad d 0.0547363\n",
      "d 14.975924\n",
      "post cost variable(0.60971195)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(0.60971195)\n",
      "test loss 2.3108466466267905 test acc 0.11207932692307693\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "debug = True\n",
    "\n",
    "def vat_loss(forward, distance, x, y=None, train=True, epsilon=8.0,\n",
    "             xi=1e-6, num_iter=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = np.random.random(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    if debug:\n",
    "        print(\"d\", d.sum(), (d ** 2).sum())\n",
    "    print(train)\n",
    "    for ip in range(num_iter):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        if debug:\n",
    "            print(\"input\", x_d.data.sum())\n",
    "        p_d_logit = forward(x_d, train=False, update_batch_stats=False)\n",
    "        if debug:\n",
    "            print(\"output\", p_d_logit.data.sum())\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        if debug:\n",
    "            print(\"loss\", kl_loss)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        if debug:\n",
    "            print(\"grad d\", d.sum())\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        if debug:\n",
    "            print(\"d\", d.sum())\n",
    "    x_adv = x + epsilon * d\n",
    "\n",
    "    p_adv_logit = forward(x_adv, train=False, update_batch_stats=False)\n",
    "    pos_cost = distance(p_logit, p_adv_logit)\n",
    "    if debug:\n",
    "        print(\"post cost\", pos_cost)\n",
    "    return pos_cost, p_d_logit\n",
    "\n",
    "for it in range(3):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_categorical(p_logit, q_logit):\n",
    "    if isinstance(p_logit, chainer.Variable):\n",
    "        xp = cuda.get_array_module(p_logit.data)\n",
    "    else:\n",
    "        xp = cuda.get_array_module(p_logit)\n",
    "    p = F.softmax(p_logit)\n",
    "    # print(p_logit.sum())\n",
    "    # print(q_logit.data.sum())\n",
    "    _kl = F.sum(p * (F.log_softmax(p_logit) - F.log_softmax(q_logit)), 1)\n",
    "    # print(_kl)\n",
    "    return F.sum(_kl) / xp.prod(xp.array(_kl.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18520069)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(1.2601826)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(1.3496399)\n",
      "it 3 ce loss variable(2.6826663) vat loss variable(1.3082993)\n",
      "it 4 ce loss variable(2.6122522) vat loss variable(1.2677813)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "    if (it+1) % 10 == 0:\n",
    "        \n",
    "        acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "        print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 9 ce loss variable(0.34254467) vat loss variable(1.3673117)\n",
      "test loss 1.2030614102498078 test acc 0.7214543269230769\n",
      "it 19 ce loss variable(0.2949698) vat loss variable(1.4227189)\n",
      "test loss 1.136353354423474 test acc 0.7433894230769231\n",
      "it 29 ce loss variable(0.27102554) vat loss variable(1.4771286)\n",
      "test loss 1.1068920233310797 test acc 0.7440905448717948\n",
      "it 39 ce loss variable(0.25327155) vat loss variable(1.5224514)\n",
      "test loss 1.1016949224166381 test acc 0.7450921474358975\n",
      "it 49 ce loss variable(0.23831564) vat loss variable(1.565478)\n",
      "test loss 1.1002999788675554 test acc 0.7425881410256411\n",
      "it 59 ce loss variable(0.22491816) vat loss variable(1.6105677)\n",
      "test loss 1.0964220601778765 test acc 0.7416866987179487\n",
      "it 69 ce loss variable(0.21271741) vat loss variable(1.6542144)\n",
      "test loss 1.0899064196990087 test acc 0.7415865384615384\n",
      "it 79 ce loss variable(0.20149495) vat loss variable(1.6965376)\n",
      "test loss 1.0825705895057092 test acc 0.7407852564102564\n",
      "it 89 ce loss variable(0.19111884) vat loss variable(1.7388303)\n",
      "test loss 1.0754560790000818 test acc 0.7403846153846154\n",
      "it 99 ce loss variable(0.1815022) vat loss variable(1.7804842)\n",
      "test loss 1.0681669826690967 test acc 0.7398838141025641\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "for it in range(100):\n",
    "    \n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        \n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "    if (it+1) % 10 == 0:\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "        print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 18.809333635120456\n",
      "loss variable(-1.0035267e-08)\n",
      "d 2.8182023\n",
      "variable(0.18520069)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18520069)\n",
      "d 13.625716652791654\n",
      "loss variable(1.0147533)\n",
      "d -10.247404\n",
      "variable(1.2203554)\n",
      "it 1 ce loss variable(1.0207927) vat loss variable(1.2203554)\n",
      "d -6.073275889670836\n",
      "loss variable(1.0948114)\n",
      "d 2.128046\n",
      "variable(1.2472886)\n",
      "it 2 ce loss variable(0.9196422) vat loss variable(1.2472886)\n",
      "d -5.657291669940053\n",
      "loss variable(1.1145282)\n",
      "d -7.6926003\n",
      "variable(1.2436831)\n",
      "it 3 ce loss variable(0.9217887) vat loss variable(1.2436831)\n",
      "d -0.31942830946959505\n",
      "loss variable(1.0268389)\n",
      "d 17.752928\n",
      "variable(1.159715)\n",
      "it 4 ce loss variable(0.94327796) vat loss variable(1.159715)\n",
      "d 15.941850424374195\n",
      "loss variable(1.0886321)\n",
      "d -1.6491866\n",
      "variable(1.1983008)\n",
      "it 5 ce loss variable(0.84185266) vat loss variable(1.1983008)\n",
      "d -1.8059021547236176\n",
      "loss variable(1.1284835)\n",
      "d 12.763174\n",
      "variable(1.232116)\n",
      "it 6 ce loss variable(0.8465243) vat loss variable(1.232116)\n",
      "d 12.150667692702767\n",
      "loss variable(1.0905163)\n",
      "d 11.111301\n",
      "variable(1.1997862)\n",
      "it 7 ce loss variable(0.8774015) vat loss variable(1.1997862)\n",
      "d -22.19119122219938\n",
      "loss variable(1.0985621)\n",
      "d 1.4018455\n",
      "variable(1.2154684)\n",
      "it 8 ce loss variable(0.95440537) vat loss variable(1.2154684)\n",
      "d -1.1151227363142457\n",
      "loss variable(1.0837042)\n",
      "d -3.3026528\n",
      "variable(1.1895165)\n",
      "it 9 ce loss variable(0.79499936) vat loss variable(1.1895165)\n",
      "d 19.537457862010598\n",
      "loss variable(1.1167276)\n",
      "d 6.0228558\n",
      "variable(1.2159164)\n",
      "it 10 ce loss variable(0.7709754) vat loss variable(1.2159164)\n",
      "d -28.764608738084966\n",
      "loss variable(1.0599291)\n",
      "d -4.6187477\n",
      "variable(1.170081)\n",
      "it 11 ce loss variable(0.7052435) vat loss variable(1.170081)\n",
      "d 8.82450542634338\n",
      "loss variable(1.1599612)\n",
      "d 4.174144\n",
      "variable(1.2607242)\n",
      "it 12 ce loss variable(0.7232995) vat loss variable(1.2607242)\n",
      "d 17.07257253366464\n",
      "loss variable(1.1669801)\n",
      "d -10.064225\n",
      "variable(1.2728804)\n",
      "it 13 ce loss variable(0.64415395) vat loss variable(1.2728804)\n",
      "d -18.55460323395467\n",
      "loss variable(1.1778262)\n",
      "d -12.064077\n",
      "variable(1.2870444)\n",
      "it 14 ce loss variable(0.6375556) vat loss variable(1.2870444)\n",
      "d -12.81132006292529\n",
      "loss variable(1.1236259)\n",
      "d -9.917589\n",
      "variable(1.2267137)\n",
      "it 15 ce loss variable(0.68250287) vat loss variable(1.2267137)\n",
      "d -12.5480754497891\n",
      "loss variable(1.2604878)\n",
      "d -1.2017266\n",
      "variable(1.3578336)\n",
      "it 16 ce loss variable(0.6229531) vat loss variable(1.3578336)\n",
      "d 11.990174416589168\n",
      "loss variable(1.1660719)\n",
      "d -9.589942\n",
      "variable(1.2682521)\n",
      "it 17 ce loss variable(0.66972756) vat loss variable(1.2682521)\n",
      "d -10.407844440895506\n",
      "loss variable(1.1450057)\n",
      "d 12.380405\n",
      "variable(1.2499914)\n",
      "it 18 ce loss variable(0.669995) vat loss variable(1.2499914)\n",
      "d -3.8609322673812807\n",
      "loss variable(1.2059662)\n",
      "d -1.9027166\n",
      "variable(1.3109951)\n",
      "it 19 ce loss variable(0.5574555) vat loss variable(1.3109951)\n",
      "d -3.3537872066015533\n",
      "loss variable(1.262367)\n",
      "d 4.4025707\n",
      "variable(1.3641775)\n",
      "it 20 ce loss variable(0.5962399) vat loss variable(1.3641775)\n",
      "d -5.536906575310836\n",
      "loss variable(1.1905057)\n",
      "d 0.22041392\n",
      "variable(1.3089786)\n",
      "it 21 ce loss variable(0.6798394) vat loss variable(1.3089786)\n",
      "d 9.461606045093934\n",
      "loss variable(1.199542)\n",
      "d -0.82267714\n",
      "variable(1.3103716)\n",
      "it 22 ce loss variable(0.65722597) vat loss variable(1.3103716)\n",
      "d 1.659321208207567\n",
      "loss variable(1.2956293)\n",
      "d -14.656862\n",
      "variable(1.4014994)\n",
      "it 23 ce loss variable(0.62663084) vat loss variable(1.4014994)\n",
      "d 2.1031809384112496\n",
      "loss variable(1.2720475)\n",
      "d 1.4196811\n",
      "variable(1.3819842)\n",
      "it 24 ce loss variable(0.52777994) vat loss variable(1.3819842)\n",
      "d 1.9628915927641861\n",
      "loss variable(1.235307)\n",
      "d -9.867787\n",
      "variable(1.3473048)\n",
      "it 25 ce loss variable(0.5819466) vat loss variable(1.3473048)\n",
      "d 0.7717617320859329\n",
      "loss variable(1.2398214)\n",
      "d -26.134771\n",
      "variable(1.3555927)\n",
      "it 26 ce loss variable(0.5324574) vat loss variable(1.3555927)\n",
      "d -9.484701186795872\n",
      "loss variable(1.2564429)\n",
      "d -12.015231\n",
      "variable(1.3745458)\n",
      "it 27 ce loss variable(0.5595169) vat loss variable(1.3745458)\n",
      "d 3.6070528373359263\n",
      "loss variable(1.24495)\n",
      "d -23.010424\n",
      "variable(1.3467615)\n",
      "it 28 ce loss variable(0.5578137) vat loss variable(1.3467615)\n",
      "d -0.4210237207167622\n",
      "loss variable(1.2669747)\n",
      "d -9.834547\n",
      "variable(1.3829197)\n",
      "it 29 ce loss variable(0.6134574) vat loss variable(1.3829197)\n",
      "d 0.6141587798513892\n",
      "loss variable(1.2770869)\n",
      "d -16.520443\n",
      "variable(1.387226)\n",
      "it 30 ce loss variable(0.52135503) vat loss variable(1.387226)\n",
      "d 11.642277595008917\n",
      "loss variable(1.2927568)\n",
      "d -0.32007957\n",
      "variable(1.3948996)\n",
      "it 31 ce loss variable(0.61205536) vat loss variable(1.3948996)\n",
      "d 11.742239405125495\n",
      "loss variable(1.3124548)\n",
      "d -19.97419\n",
      "variable(1.4259231)\n",
      "it 32 ce loss variable(0.54090494) vat loss variable(1.4259231)\n",
      "d -5.521902693695445\n",
      "loss variable(1.2701559)\n",
      "d -17.54828\n",
      "variable(1.3852599)\n",
      "it 33 ce loss variable(0.6153326) vat loss variable(1.3852599)\n",
      "d 11.443393270560357\n",
      "loss variable(1.2957718)\n",
      "d 10.306329\n",
      "variable(1.4033513)\n",
      "it 34 ce loss variable(0.55727744) vat loss variable(1.4033513)\n",
      "d 0.1089651630876809\n",
      "loss variable(1.2646873)\n",
      "d -32.211845\n",
      "variable(1.3706245)\n",
      "it 35 ce loss variable(0.46945387) vat loss variable(1.3706245)\n",
      "d -18.958932883439566\n",
      "loss variable(1.3405955)\n",
      "d -21.759275\n",
      "variable(1.4391092)\n",
      "it 36 ce loss variable(0.5755157) vat loss variable(1.4391092)\n",
      "d 17.834838563985212\n",
      "loss variable(1.3036685)\n",
      "d -11.244947\n",
      "variable(1.4127519)\n",
      "it 37 ce loss variable(0.5539928) vat loss variable(1.4127519)\n",
      "d -1.9762978829981006\n",
      "loss variable(1.2770425)\n",
      "d -15.831317\n",
      "variable(1.3949046)\n",
      "it 38 ce loss variable(0.52981627) vat loss variable(1.3949046)\n",
      "d 5.711908568132681\n",
      "loss variable(1.2900443)\n",
      "d 7.8729\n",
      "variable(1.4052365)\n",
      "it 39 ce loss variable(0.48810005) vat loss variable(1.4052365)\n",
      "d -1.4692007664838331\n",
      "loss variable(1.341851)\n",
      "d -17.915264\n",
      "variable(1.4504914)\n",
      "it 40 ce loss variable(0.55558777) vat loss variable(1.4504914)\n",
      "d -5.126474138727224\n",
      "loss variable(1.3140737)\n",
      "d 10.415051\n",
      "variable(1.4239393)\n",
      "it 41 ce loss variable(0.55384386) vat loss variable(1.4239393)\n",
      "d 9.56829916398193\n",
      "loss variable(1.335239)\n",
      "d -5.7372437\n",
      "variable(1.4276927)\n",
      "it 42 ce loss variable(0.46935156) vat loss variable(1.4276927)\n",
      "d -20.339647875375555\n",
      "loss variable(1.3320531)\n",
      "d -11.940778\n",
      "variable(1.443888)\n",
      "it 43 ce loss variable(0.5271072) vat loss variable(1.443888)\n",
      "d 6.838921258408536\n",
      "loss variable(1.3308517)\n",
      "d -11.9447975\n",
      "variable(1.4417572)\n",
      "it 44 ce loss variable(0.5176019) vat loss variable(1.4417572)\n",
      "d -20.882921617519422\n",
      "loss variable(1.3548068)\n",
      "d -8.516261\n",
      "variable(1.4687951)\n",
      "it 45 ce loss variable(0.46282414) vat loss variable(1.4687951)\n",
      "d -7.4476275107837155\n",
      "loss variable(1.3583913)\n",
      "d -14.767474\n",
      "variable(1.4662354)\n",
      "it 46 ce loss variable(0.44224188) vat loss variable(1.4662354)\n",
      "d -4.8765588240176045\n",
      "loss variable(1.4045091)\n",
      "d -31.569254\n",
      "variable(1.5023803)\n",
      "it 47 ce loss variable(0.4666009) vat loss variable(1.5023803)\n",
      "d 6.198011766313341\n",
      "loss variable(1.3791254)\n",
      "d -14.198257\n",
      "variable(1.4844203)\n",
      "it 48 ce loss variable(0.4652521) vat loss variable(1.4844203)\n",
      "d -18.417532164672053\n",
      "loss variable(1.3293316)\n",
      "d -19.668766\n",
      "variable(1.441889)\n",
      "it 49 ce loss variable(0.4740532) vat loss variable(1.441889)\n",
      "d 11.499870444884642\n",
      "loss variable(1.3680856)\n",
      "d -18.667166\n",
      "variable(1.4715075)\n",
      "it 50 ce loss variable(0.5043552) vat loss variable(1.4715075)\n",
      "d -3.916152708342042\n",
      "loss variable(1.4374003)\n",
      "d -0.6954527\n",
      "variable(1.5389137)\n",
      "it 51 ce loss variable(0.427392) vat loss variable(1.5389137)\n",
      "d 10.01712975610236\n",
      "loss variable(1.4190614)\n",
      "d -20.789139\n",
      "variable(1.5397143)\n",
      "it 52 ce loss variable(0.4457674) vat loss variable(1.5397143)\n",
      "d 7.301845633936739\n",
      "loss variable(1.3485345)\n",
      "d -18.79001\n",
      "variable(1.4802843)\n",
      "it 53 ce loss variable(0.585494) vat loss variable(1.4802843)\n",
      "d 15.86757225070745\n",
      "loss variable(1.343765)\n",
      "d -23.600498\n",
      "variable(1.4597104)\n",
      "it 54 ce loss variable(0.59236884) vat loss variable(1.4597104)\n",
      "d 6.545725992832868\n",
      "loss variable(1.4197702)\n",
      "d -2.1002972\n",
      "variable(1.530483)\n",
      "it 55 ce loss variable(0.4669355) vat loss variable(1.530483)\n",
      "d -0.9768616916579125\n",
      "loss variable(1.358917)\n",
      "d -6.005143\n",
      "variable(1.4934279)\n",
      "it 56 ce loss variable(0.5691479) vat loss variable(1.4934279)\n",
      "d -11.935087234972634\n",
      "loss variable(1.3851652)\n",
      "d -10.979122\n",
      "variable(1.511116)\n",
      "it 57 ce loss variable(0.49034953) vat loss variable(1.511116)\n",
      "d -20.651292250502454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss variable(1.3605834)\n",
      "d -5.204693\n",
      "variable(1.4866124)\n",
      "it 58 ce loss variable(0.4668119) vat loss variable(1.4866124)\n",
      "d 5.930736741602792\n",
      "loss variable(1.4217685)\n",
      "d -2.96503\n",
      "variable(1.5363104)\n",
      "it 59 ce loss variable(0.48291457) vat loss variable(1.5363104)\n",
      "d -2.0687157650696317\n",
      "loss variable(1.3750725)\n",
      "d -20.187246\n",
      "variable(1.5088575)\n",
      "it 60 ce loss variable(0.4801722) vat loss variable(1.5088575)\n",
      "d 1.8124146704867552\n",
      "loss variable(1.4250214)\n",
      "d -31.683117\n",
      "variable(1.5315392)\n",
      "it 61 ce loss variable(0.50071263) vat loss variable(1.5315392)\n",
      "d -2.850726191424384\n",
      "loss variable(1.4569461)\n",
      "d -34.39248\n",
      "variable(1.5637801)\n",
      "it 62 ce loss variable(0.42511448) vat loss variable(1.5637801)\n",
      "d -11.751381348113794\n",
      "loss variable(1.3885074)\n",
      "d -24.86853\n",
      "variable(1.5128889)\n",
      "it 63 ce loss variable(0.49216902) vat loss variable(1.5128889)\n",
      "d 6.378625198477101\n",
      "loss variable(1.4517913)\n",
      "d -32.447678\n",
      "variable(1.5701138)\n",
      "it 64 ce loss variable(0.5427566) vat loss variable(1.5701138)\n",
      "d 2.7153277303448657\n",
      "loss variable(1.465215)\n",
      "d -22.916714\n",
      "variable(1.5846007)\n",
      "it 65 ce loss variable(0.40772316) vat loss variable(1.5846007)\n",
      "d -22.308087885109458\n",
      "loss variable(1.4969636)\n",
      "d -36.372955\n",
      "variable(1.6092582)\n",
      "it 66 ce loss variable(0.50262094) vat loss variable(1.6092582)\n",
      "d -2.4192262486470897\n",
      "loss variable(1.4375935)\n",
      "d -14.680861\n",
      "variable(1.5536546)\n",
      "it 67 ce loss variable(0.5135555) vat loss variable(1.5536546)\n",
      "d 6.019773782497798\n",
      "loss variable(1.3961962)\n",
      "d -63.45828\n",
      "variable(1.527636)\n",
      "it 68 ce loss variable(0.55315363) vat loss variable(1.527636)\n",
      "d -13.050808521321562\n",
      "loss variable(1.4348187)\n",
      "d -35.248444\n",
      "variable(1.5626605)\n",
      "it 69 ce loss variable(0.5986991) vat loss variable(1.5626605)\n",
      "d 12.242728546553089\n",
      "loss variable(1.4967582)\n",
      "d -0.91281223\n",
      "variable(1.6048044)\n",
      "it 70 ce loss variable(0.39243954) vat loss variable(1.6048044)\n",
      "d -1.0065886980648036\n",
      "loss variable(1.4938748)\n",
      "d -26.504574\n",
      "variable(1.6141081)\n",
      "it 71 ce loss variable(0.425413) vat loss variable(1.6141081)\n",
      "d -5.898650847588845\n",
      "loss variable(1.4512051)\n",
      "d -26.741228\n",
      "variable(1.5761062)\n",
      "it 72 ce loss variable(0.46639082) vat loss variable(1.5761062)\n",
      "d 0.20407054445150008\n",
      "loss variable(1.5079597)\n",
      "d -9.618024\n",
      "variable(1.6198823)\n",
      "it 73 ce loss variable(0.5143717) vat loss variable(1.6198823)\n",
      "d -1.8879851720624448\n",
      "loss variable(1.5007646)\n",
      "d -18.218437\n",
      "variable(1.6331649)\n",
      "it 74 ce loss variable(0.48229313) vat loss variable(1.6331649)\n",
      "d -17.765654704610878\n",
      "loss variable(1.4432151)\n",
      "d -23.259878\n",
      "variable(1.5719182)\n",
      "it 75 ce loss variable(0.40724617) vat loss variable(1.5719182)\n",
      "d 8.99825119708387\n",
      "loss variable(1.4926869)\n",
      "d 6.604527\n",
      "variable(1.5989352)\n",
      "it 76 ce loss variable(0.43710768) vat loss variable(1.5989352)\n",
      "d -12.127983976464943\n",
      "loss variable(1.5324087)\n",
      "d -6.8169055\n",
      "variable(1.6448643)\n",
      "it 77 ce loss variable(0.36148173) vat loss variable(1.6448643)\n",
      "d 22.194063711211534\n",
      "loss variable(1.4740313)\n",
      "d -32.723717\n",
      "variable(1.5895956)\n",
      "it 78 ce loss variable(0.40229177) vat loss variable(1.5895956)\n",
      "d 1.0636439482504092\n",
      "loss variable(1.501802)\n",
      "d -26.298784\n",
      "variable(1.6283778)\n",
      "it 79 ce loss variable(0.4478165) vat loss variable(1.6283778)\n",
      "d -2.360875721615897\n",
      "loss variable(1.4691436)\n",
      "d -10.845854\n",
      "variable(1.5909395)\n",
      "it 80 ce loss variable(0.4349839) vat loss variable(1.5909395)\n",
      "d -16.35444721518794\n",
      "loss variable(1.5467818)\n",
      "d -22.867163\n",
      "variable(1.6699708)\n",
      "it 81 ce loss variable(0.38690618) vat loss variable(1.6699708)\n",
      "d -5.459936451380143\n",
      "loss variable(1.5207355)\n",
      "d 0.65002143\n",
      "variable(1.6383059)\n",
      "it 82 ce loss variable(0.31777406) vat loss variable(1.6383059)\n",
      "d 4.842541580840611\n",
      "loss variable(1.5121353)\n",
      "d -16.626282\n",
      "variable(1.6369443)\n",
      "it 83 ce loss variable(0.42345172) vat loss variable(1.6369443)\n",
      "d 5.4152817471985975\n",
      "loss variable(1.5143564)\n",
      "d -5.9943886\n",
      "variable(1.6413108)\n",
      "it 84 ce loss variable(0.4236464) vat loss variable(1.6413108)\n",
      "d -5.754446120699502\n",
      "loss variable(1.5209382)\n",
      "d -24.78445\n",
      "variable(1.6328276)\n",
      "it 85 ce loss variable(0.38971806) vat loss variable(1.6328276)\n",
      "d 23.01949145393792\n",
      "loss variable(1.5336045)\n",
      "d -2.1896753\n",
      "variable(1.6568484)\n",
      "it 86 ce loss variable(0.41371632) vat loss variable(1.6568484)\n",
      "d 1.026650813650571\n",
      "loss variable(1.5694077)\n",
      "d 0.43878627\n",
      "variable(1.6916659)\n",
      "it 87 ce loss variable(0.43581235) vat loss variable(1.6916659)\n",
      "d -3.7272299351401204\n",
      "loss variable(1.5322535)\n",
      "d -12.44911\n",
      "variable(1.6510043)\n",
      "it 88 ce loss variable(0.39081147) vat loss variable(1.6510043)\n",
      "d 2.4539012455792886\n",
      "loss variable(1.49538)\n",
      "d -42.64389\n",
      "variable(1.6265726)\n",
      "it 89 ce loss variable(0.36579055) vat loss variable(1.6265726)\n",
      "d -9.077089979175865\n",
      "loss variable(1.515259)\n",
      "d -32.392242\n",
      "variable(1.6441574)\n",
      "it 90 ce loss variable(0.47298357) vat loss variable(1.6441574)\n",
      "d 1.5298456952706845\n",
      "loss variable(1.5651612)\n",
      "d -38.877937\n",
      "variable(1.6898088)\n",
      "it 91 ce loss variable(0.42935228) vat loss variable(1.6898088)\n",
      "d 7.189722053016781\n",
      "loss variable(1.5272)\n",
      "d -53.860912\n",
      "variable(1.6615038)\n",
      "it 92 ce loss variable(0.42192256) vat loss variable(1.6615038)\n",
      "d 9.480157833083993\n",
      "loss variable(1.5183927)\n",
      "d -49.79727\n",
      "variable(1.652884)\n",
      "it 93 ce loss variable(0.4143789) vat loss variable(1.652884)\n",
      "d 3.1108946017806796\n",
      "loss variable(1.5771916)\n",
      "d -59.646393\n",
      "variable(1.6935525)\n",
      "it 94 ce loss variable(0.37112278) vat loss variable(1.6935525)\n",
      "d 2.823493722115363\n",
      "loss variable(1.5676838)\n",
      "d -20.721746\n",
      "variable(1.6882787)\n",
      "it 95 ce loss variable(0.39960742) vat loss variable(1.6882787)\n",
      "d -12.189618523958252\n",
      "loss variable(1.5682818)\n",
      "d -16.407381\n",
      "variable(1.697522)\n",
      "it 96 ce loss variable(0.42520842) vat loss variable(1.697522)\n",
      "d 1.818478622741708\n",
      "loss variable(1.575295)\n",
      "d 11.22592\n",
      "variable(1.7041317)\n",
      "it 97 ce loss variable(0.40007377) vat loss variable(1.7041317)\n",
      "d 0.1806178413765278\n",
      "loss variable(1.5444249)\n",
      "d -46.634815\n",
      "variable(1.670377)\n",
      "it 98 ce loss variable(0.5384443) vat loss variable(1.670377)\n",
      "d 1.1534798487339804\n",
      "loss variable(1.5304189)\n",
      "d -47.21165\n",
      "variable(1.6656125)\n",
      "it 99 ce loss variable(0.46958965) vat loss variable(1.6656125)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(100):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=args.top_bn)\n",
    "if args.gpu:\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "    \n",
    "with chainer.using_config(\"train\", False):\n",
    "    logits = enc(Variable(x), train=False, update_batch_stats=False)\n",
    "    print(logits.shape)\n",
    "    print(logits.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_loss(forward, distance, x, train=True, epsilon=8.0, xi=1e-6, Ip=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = xp.random.normal(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    for ip in range(Ip):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        p_d_logit = forward(x_d, train=train, update_batch_stats=False)\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "    x_adv = x + epsilon * d \n",
    "    p_adv_logit = forward(x_adv, train=train, update_batch_stats=False)\n",
    "    return distance(p_logit, p_adv_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_framework_seed(args.seed)\n",
    "device = args.device\n",
    "\n",
    "train_all, test, shape, num_classes = data_set_name(args.dataset)\n",
    "if args.size == 0:\n",
    "    args.size = len(train_all)\n",
    "test_loader = DataLoader(test, 1000, num_workers=3)\n",
    "\n",
    "train_l = SubsetDataset(train_all, list(range(args.size)))\n",
    "train_ul = SubsetDataset(train_all, list(range(len(train_all) - 1000)))\n",
    "\n",
    "print(len(train_l), len(train_ul))\n",
    "\n",
    "batch_size_l = 32\n",
    "batch_size_ul = 128\n",
    "\n",
    "Arch = getattr(models, args.arch)\n",
    "api_criterion = None\n",
    "\n",
    "if args.trainer != \"none\":\n",
    "    api_criterion = getattr(SemiMode, args.trainer)(args)\n",
    "\n",
    "set_framework_seed(args.seed)\n",
    "l_train_iter = iter(DataLoader(train_l, batch_size_l, num_workers=0, sampler=InfiniteSampler(len(train_l))))\n",
    "ul_train_iter = iter(DataLoader(train_ul, batch_size_ul, num_workers=0, sampler=InfiniteSampler(len(train_ul))))\n",
    "\n",
    "l_x, l_y = next(l_train_iter)\n",
    "print(\"l_y\", l_y[:5])\n",
    "ul_x, ul_y = next(ul_train_iter)\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "print(\"ul_y\", ul_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1)\n",
    "        super(CNN, self).__init__(\n",
    "            c1=L.Convolution2D(3, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c2=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c3=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c4=L.Convolution2D(128, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c5=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c6=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c7=L.Convolution2D(256, 512, ksize=3, stride=1, pad=0, initialW=initializer),\n",
    "            c8=L.Convolution2D(512, 256, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            c9=L.Convolution2D(256, 128, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            l_cl=L.Linear(128, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(128),\n",
    "            bn2=L.BatchNormalization(128),\n",
    "            bn3=L.BatchNormalization(128),\n",
    "            bn4=L.BatchNormalization(256),\n",
    "            bn5=L.BatchNormalization(256),\n",
    "            bn6=L.BatchNormalization(256),\n",
    "            bn7=L.BatchNormalization(512),\n",
    "            bn8=L.BatchNormalization(256),\n",
    "            bn9=L.BatchNormalization(128),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.average_pooling_2d(h, ksize=h.data.shape[2])\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 40.848587\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "out = enc(Variable(x), update_batch_stats=True)\n",
    "print(x.sum(), out.data.sum())\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
