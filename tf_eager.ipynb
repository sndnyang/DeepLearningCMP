{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_func.layers as L\n",
    "import tf_func.margin_loss as lmargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/xyang2/project/data/dataset/mnist/seed1/labeled_train.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input_fn(filenames, batch_size=1000, shuffle=False):\n",
    "    \n",
    "    def _parser(record):\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        parsed_record = tf.parse_single_example(record, features)\n",
    "        image = tf.decode_raw(parsed_record['image'], tf.float32)\n",
    "\n",
    "        label = tf.cast(parsed_record['label'], tf.int32)\n",
    "\n",
    "        return image, tf.one_hot(label, depth=10)\n",
    "    \n",
    "    def _iter():\n",
    "        dataset = (tf.data.TFRecordDataset(filenames)\n",
    "            .map(_parser))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10_000)\n",
    "\n",
    "        dataset = dataset.repeat(None) # Infinite iterations: let experiment determine num_epochs\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator\n",
    "    \n",
    "    def _input_fn():        \n",
    "        iterator = _iter()\n",
    "        features, labels = iterator.get_next()\n",
    "        \n",
    "        return features, labels\n",
    "    return _iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10697.64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = data_input_fn(\"./mnist/train-1.tfrecords\", batch_size=100)()\n",
    "val_iterator = data_input_fn(\"./mnist/validation.tfrecords\", batch_size=100)()\n",
    "images, labels = iterator.get_next()\n",
    "images.numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, is_training=True, update_batch_stats=True, stochastic=True, seed=1234):\n",
    "    h = x\n",
    "    endpoints = {}\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # h = tf.layers.flatten(h)\n",
    "    h = L.fc(h, 784, 1200, seed=rng.randint(123456), name='fc1')\n",
    "    h = L.relu(L.bn(h, 1200, is_training=is_training, update_batch_stats=update_batch_stats, name='b1'))\n",
    "    endpoints[\"fc1\"] = h\n",
    "    h = L.fc(h, 1200, 1200, seed=rng.randint(123456), name='fc2')\n",
    "    h = L.relu(L.bn(h, 1200, is_training=is_training, update_batch_stats=update_batch_stats, name='b2'))\n",
    "    endpoints[\"fc2\"] = h\n",
    "    h = L.fc(h, 1200, 10, seed=rng.randint(123456), name='fc')\n",
    "\n",
    "    if True:\n",
    "        h = L.bn(h, 10, is_training=is_training, update_batch_stats=update_batch_stats, name='bfc')\n",
    "\n",
    "    return h, endpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_norm_fn(norm_type):\n",
    "    norm_fn = lambda x: tf.norm(x, ord=norm_type)\n",
    "    return norm_fn\n",
    "\n",
    "\n",
    "def maximum_with_relu(a, b):\n",
    "    return a + tf.nn.relu(b - a)\n",
    "\n",
    "\n",
    "def large_margin(model, x=None, one_hot_labels=None, layers_list=None, gamma=10000, alpha_factor=2, top_k=1, dist_norm=2,\n",
    "                 epsilon=1e-8, use_approximation=True, loss_type=\"all_top_k\", loss_collection=tf.GraphKeys.LOSSES):\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        logits, endpoints = model(x)\n",
    "        layers_list = [x, endpoints['fc1']]\n",
    "        layers_list = [x]\n",
    "        logits = tf.convert_to_tensor(logits)\n",
    "        one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n",
    "        logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n",
    "        assert top_k > 0\n",
    "        assert top_k <= logits.get_shape()[1]\n",
    "\n",
    "        dual_norm = {1: np.inf, 2: 2, np.inf: 1}\n",
    "        norm_fn = get_norm_fn(dual_norm[dist_norm])\n",
    "        with tf.name_scope(\"large_margin_loss\"):\n",
    "            class_prob = tf.nn.softmax(logits)\n",
    "            # Pick the correct class probability.\n",
    "            correct_class_prob = tf.reduce_sum(\n",
    "                class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "            # Class probabilities except the correct.\n",
    "            other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "            if top_k > 1:\n",
    "                # Pick the top k class probabilities other than the correct.\n",
    "                top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "            else:\n",
    "                top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "            # Difference between correct class probailities and top_k probabilities.\n",
    "            difference_prob = correct_class_prob - top_k_class_prob\n",
    "            losses_list = []\n",
    "            for layer in layers_list:\n",
    "                # m = t.gradient(difference_prob[:, 0], layer)\n",
    "                # print(m.shape)\n",
    "                difference_prob_grad = [\n",
    "                    tf.layers.flatten(t.gradient(difference_prob[:, i], layer))\n",
    "                    for i in range(top_k)\n",
    "                ]\n",
    "\n",
    "                difference_prob_gradnorm = tf.concat([\n",
    "                    tf.map_fn(norm_fn, difference_prob_grad[i])[:, tf.newaxis]\n",
    "                    for i in range(top_k)\n",
    "                ], axis=1)\n",
    "\n",
    "                if use_approximation:\n",
    "                    difference_prob_gradnorm = tf.stop_gradient(difference_prob_gradnorm)\n",
    "\n",
    "                distance_to_boundary = difference_prob / (\n",
    "                        difference_prob_gradnorm + epsilon)\n",
    "\n",
    "                if loss_type == \"worst_top_k\":\n",
    "                    # Only consider worst distance to boundary.\n",
    "                    distance_to_boundary = tf.reduce_min(distance_to_boundary, axis=1)\n",
    "\n",
    "                elif loss_type == \"average_top_k\":\n",
    "                    # Only consider average distance to boundary.\n",
    "                    distance_to_boundary = tf.reduce_mean(distance_to_boundary, axis=1)\n",
    "\n",
    "                # Distances to consider between distance_upper and distance_lower bounds\n",
    "                distance_upper = gamma\n",
    "                distance_lower = gamma * (1 - alpha_factor)\n",
    "\n",
    "                # Enforce lower bound.\n",
    "                loss_layer = maximum_with_relu(distance_to_boundary, distance_lower)\n",
    "\n",
    "                # Enforce upper bound.\n",
    "                loss_layer = maximum_with_relu(\n",
    "                    0, distance_upper - loss_layer) - distance_upper\n",
    "\n",
    "                losses_list.append(tf.reduce_mean(loss_layer))\n",
    "\n",
    "            loss = tf.reduce_mean(losses_list)\n",
    "            # Add loss to loss_collection.\n",
    "            tf.losses.add_loss(loss, loss_collection)\n",
    "    return loss\n",
    "\n",
    "x = images\n",
    "y = labels\n",
    "   \n",
    "nll_loss = large_margin(mlp, x, one_hot_labels=y, layers_list=layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7501, shape=(), dtype=float32, numpy=0.14169922>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    loss, grads_and_vars = cal_gradient(x, y)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    print(\"step: {}  loss: {}\".format(step, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-341083e152b7>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "def softmax_model(model, image_batch):\n",
    "    model_output = tf.nn.softmax(model(image_batch)[0])\n",
    "    return model_output\n",
    "\n",
    "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels))\\\n",
    "    .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "    .shuffle(buffer_size=1000)\\\n",
    "    .batch(100)\\\n",
    "\n",
    "model = mlp\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "it = tfe.Iterator(train_ds)\n",
    "image_batch, label_batch = it.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  loss: 2.737337350845337\n",
      "test accuracy = 0.1062999963760376\n",
      "step: 1  loss: 2.5117690563201904\n",
      "step: 2  loss: 2.625941514968872\n",
      "step: 3  loss: 2.5772452354431152\n",
      "step: 4  loss: 3.005286455154419\n",
      "step: 5  loss: 2.7645673751831055\n",
      "step: 6  loss: 2.7314765453338623\n",
      "step: 7  loss: 2.449230432510376\n",
      "step: 8  loss: 2.8209028244018555\n",
      "step: 9  loss: 2.541626214981079\n",
      "step: 10  loss: 2.708069086074829\n",
      "test accuracy = 0.0925000011920929\n",
      "step: 11  loss: 2.415274143218994\n",
      "step: 12  loss: 2.7800986766815186\n",
      "step: 13  loss: 2.892584800720215\n",
      "step: 14  loss: 2.6332077980041504\n",
      "step: 15  loss: 2.8512630462646484\n",
      "step: 16  loss: 2.949721336364746\n",
      "step: 17  loss: 2.599675178527832\n",
      "step: 18  loss: 2.8251454830169678\n",
      "step: 19  loss: 2.8685660362243652\n",
      "step: 20  loss: 2.6079752445220947\n",
      "test accuracy = 0.09889999777078629\n",
      "step: 21  loss: 2.708016872406006\n",
      "step: 22  loss: 2.6244702339172363\n",
      "step: 23  loss: 2.7577314376831055\n",
      "step: 24  loss: 2.8216304779052734\n",
      "step: 25  loss: 2.8594720363616943\n",
      "step: 26  loss: 2.5315749645233154\n",
      "step: 27  loss: 2.9569361209869385\n",
      "step: 28  loss: 2.675708055496216\n",
      "step: 29  loss: 2.8561525344848633\n",
      "step: 30  loss: 2.956704616546631\n",
      "test accuracy = 0.10189999639987946\n",
      "step: 31  loss: 2.900059700012207\n",
      "step: 32  loss: 2.9351227283477783\n",
      "step: 33  loss: 2.5208022594451904\n",
      "step: 34  loss: 2.9226415157318115\n",
      "step: 35  loss: 2.6470487117767334\n",
      "step: 36  loss: 2.7522876262664795\n",
      "step: 37  loss: 2.744605779647827\n",
      "step: 38  loss: 2.962878942489624\n",
      "step: 39  loss: 2.9832229614257812\n",
      "step: 40  loss: 2.388190984725952\n",
      "test accuracy = 0.07720000296831131\n",
      "step: 41  loss: 2.9932563304901123\n",
      "step: 42  loss: 3.043084144592285\n",
      "step: 43  loss: 2.901310682296753\n",
      "step: 44  loss: 2.8240976333618164\n",
      "step: 45  loss: 2.948392391204834\n",
      "step: 46  loss: 2.7501418590545654\n",
      "step: 47  loss: 2.5570569038391113\n",
      "step: 48  loss: 2.812335252761841\n",
      "step: 49  loss: 2.573533058166504\n",
      "step: 50  loss: 2.9109199047088623\n",
      "test accuracy = 0.07360000163316727\n",
      "step: 51  loss: 2.462693929672241\n",
      "step: 52  loss: 2.8876070976257324\n",
      "step: 53  loss: 2.830448865890503\n",
      "step: 54  loss: 2.7153775691986084\n",
      "step: 55  loss: 2.9034061431884766\n",
      "step: 56  loss: 2.6522936820983887\n",
      "step: 57  loss: 2.5196852684020996\n",
      "step: 58  loss: 2.661834955215454\n",
      "step: 59  loss: 2.756715774536133\n",
      "step: 60  loss: 2.697019338607788\n",
      "test accuracy = 0.08250000327825546\n",
      "step: 61  loss: 2.75862193107605\n",
      "step: 62  loss: 2.5932254791259766\n",
      "step: 63  loss: 2.6304080486297607\n",
      "step: 64  loss: 2.7892510890960693\n",
      "step: 65  loss: 2.6816599369049072\n",
      "step: 66  loss: 2.601005792617798\n",
      "step: 67  loss: 2.7636590003967285\n",
      "step: 68  loss: 2.7065956592559814\n",
      "step: 69  loss: 2.9823412895202637\n",
      "step: 70  loss: 2.7066562175750732\n",
      "test accuracy = 0.0625\n",
      "step: 71  loss: 2.9130125045776367\n",
      "step: 72  loss: 2.375154733657837\n",
      "step: 73  loss: 2.45863676071167\n",
      "step: 74  loss: 2.627134084701538\n",
      "step: 75  loss: 2.8437912464141846\n",
      "step: 76  loss: 2.5910892486572266\n",
      "step: 77  loss: 2.573404312133789\n",
      "step: 78  loss: 2.9531805515289307\n",
      "step: 79  loss: 2.4965078830718994\n",
      "step: 80  loss: 2.675997257232666\n",
      "test accuracy = 0.08569999784231186\n",
      "step: 81  loss: 2.593198776245117\n",
      "step: 82  loss: 2.898207664489746\n",
      "step: 83  loss: 2.6586620807647705\n",
      "step: 84  loss: 2.5374693870544434\n",
      "step: 85  loss: 2.6598024368286133\n",
      "step: 86  loss: 2.8259410858154297\n",
      "step: 87  loss: 2.659135341644287\n",
      "step: 88  loss: 2.6660752296447754\n",
      "step: 89  loss: 2.704774856567383\n",
      "step: 90  loss: 2.635735511779785\n",
      "test accuracy = 0.09149999916553497\n",
      "step: 91  loss: 2.56563663482666\n",
      "step: 92  loss: 2.797112226486206\n",
      "step: 93  loss: 2.6907858848571777\n",
      "step: 94  loss: 2.5909194946289062\n",
      "step: 95  loss: 2.9875118732452393\n",
      "step: 96  loss: 2.931793212890625\n",
      "step: 97  loss: 2.5849368572235107\n",
      "step: 98  loss: 2.7803146839141846\n",
      "step: 99  loss: 2.7194466590881348\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    loss, grads_and_vars = cal_gradient(model, image_batch, label_batch)\n",
    "\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    print(\"step: {}  loss: {}\".format(step, loss.numpy()))\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        model_test_output = softmax_model(model, data.test.images)\n",
    "        model_test_label = data.test.labels\n",
    "        correct_prediction = tf.equal(tf.argmax(model_test_output, 1), tf.argmax(model_test_label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print(\"test accuracy = {}\".format(accuracy.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(model_output, label_batch):\n",
    "    loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(label_batch * tf.log(model_output),\n",
    "        reduction_indices=[1]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tfe.implicit_value_and_gradients\n",
    "def cal_gradient(model, image_batch, label_batch):\n",
    "    return cross_entropy(softmax_model(model, image_batch), label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(None)\n",
    "logits, endpoints = model(image_batch, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.18038413 0.22659564 0.         ... 0.         0.         0.3759577 ]\n",
      " [0.36179423 0.36371958 0.         ... 0.         0.         0.        ]\n",
      " [0.20473713 0.3723094  0.         ... 0.         0.1492198  0.        ]\n",
      " ...\n",
      " [0.25645143 0.6003772  0.         ... 0.         0.13306507 0.        ]\n",
      " [0.01401306 0.30898386 0.         ... 0.         0.         0.        ]\n",
      " [0.05506128 0.23868443 0.00085331 ... 0.         0.         0.29849178]], shape=(100, 1200), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.06070615 0.04257288 0.05172792 ... 0.29537678 0.         0.1104888 ]\n",
      " [0.         0.         0.25382587 ... 0.         0.14778559 0.        ]\n",
      " [0.         0.04479885 0.07401633 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.16355065 0.         0.         ... 0.26173037 0.         0.        ]\n",
      " [0.02450862 0.         0.         ... 0.05797724 0.         0.04237015]\n",
      " [0.         0.10344929 0.         ... 0.4595394  0.00108536 0.09780887]], shape=(100, 1200), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.06070615 0.04257288 0.05172792 ... 0.29537678 0.         0.1104888 ]\n",
      " [0.         0.         0.25382587 ... 0.         0.14778559 0.        ]\n",
      " [0.         0.04479885 0.07401633 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.16355065 0.         0.         ... 0.26173037 0.         0.        ]\n",
      " [0.02450862 0.         0.         ... 0.05797724 0.         0.04237015]\n",
      " [0.         0.10344929 0.         ... 0.4595394  0.00108536 0.09780887]], shape=(100, 1200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for layer in layers_list:\n",
    "    print(layer)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer flatten_9 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [1200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-902972ac91e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         difference_prob_grad = [\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m<ipython-input-18-902972ac91e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         difference_prob_grad = [\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m   \"\"\"\n\u001b[1;32m    315\u001b[0m   \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                            \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m                            \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                            str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m   1441\u001b[0m       \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer flatten_9 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [1200]"
     ]
    }
   ],
   "source": [
    "model = MLP(None)\n",
    "\n",
    "one_hot_labels = label_batch\n",
    "top_k = 1\n",
    "with tf.GradientTape() as t:\n",
    "    logits, endpoints = model(image_batch, is_training=True)\n",
    "    layers_list = [endpoints[e] for e in endpoints]\n",
    "    class_prob = tf.nn.softmax(logits)\n",
    "    # Pick the correct class probability.\n",
    "    correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "    # Class probabilities except the correct.\n",
    "    other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "    if top_k > 1:\n",
    "        # Pick the top k class probabilities other than the correct.\n",
    "        top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "    else:\n",
    "        top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "    # Difference between correct class probailities and top_k probabilities.\n",
    "    difference_prob = correct_class_prob - top_k_class_prob\n",
    "    losses_list = []\n",
    "    for layer in layers_list:\n",
    "        difference_prob_grad = [\n",
    "            tf.layers.flatten(t.gradient(difference_prob[:, i], layer)[0])\n",
    "            for i in range(top_k)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-803edc17e53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlarge_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e9ce10671bfc>\u001b[0m in \u001b[0;36mlarge_margin\u001b[0;34m(_sentinel, logits, one_hot_labels, layers_list, gamma, alpha_factor, top_k, dist_norm, epsilon, use_approximation, loss_type, loss_collection)\u001b[0m\n\u001b[1;32m     90\u001b[0m             difference_prob_grad = [\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             ]\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e9ce10671bfc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m             difference_prob_grad = [\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             ]\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 596\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    611\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    612\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
     ]
    }
   ],
   "source": [
    "large_margin(logits=logits, one_hot_labels=label_batch, layers_list=endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_norm_fn(norm_type):\n",
    "    norm_fn = lambda x: tf.norm(x, ord=norm_type)\n",
    "    return norm_fn\n",
    "\n",
    "\n",
    "def maximum_with_relu(a, b):\n",
    "    return a + tf.nn.relu(b - a)\n",
    "\n",
    "\n",
    "def _ensure_large_margin_args(name, sentinel, one_hot_labels, logits, layers_list, dist_norm, loss_type):\n",
    "    \"\"\"Ensures arguments are correct.\"\"\"\n",
    "    # Make sure that all arguments were passed as named arguments.\n",
    "    if sentinel is not None:\n",
    "        raise ValueError(\n",
    "            \"Only call `%s` with \"\n",
    "            \"named arguments (one_hot_labels=..., logits=..., ...)\" % name)\n",
    "    if one_hot_labels is None or logits is None or not layers_list:\n",
    "        raise ValueError(\"logits, one_hot_labels and layers_list must be provided.\")\n",
    "\n",
    "    if dist_norm not in {1, 2, np.inf}:\n",
    "        raise ValueError(\"dist_norm must be 1, 2, or np.inf.\")\n",
    "\n",
    "    if loss_type not in {\"all_top_k\", \"average_top_k\", \"worst_top_k\"}:\n",
    "        raise ValueError(\n",
    "            \"loss_type must be 'all_top_k', 'average_top_k', or 'worst_top_k'.\")\n",
    "\n",
    "\n",
    "# pylint: disable=invalid-name\n",
    "def large_margin(_sentinel=None, logits=None, one_hot_labels=None, layers_list=None, gamma=10000, alpha_factor=2, top_k=1, dist_norm=2,\n",
    "                 epsilon=1e-8, use_approximation=True, loss_type=\"all_top_k\", loss_collection=tf.GraphKeys.LOSSES):\n",
    "    \"\"\"Creates a large margin loss.\n",
    "\n",
    "    Args:\n",
    "        _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
    "        logits: Float `[batch_size, num_classes]` logits outputs of the network.\n",
    "        one_hot_labels: `[batch_size, num_classes]` Target integer labels in `{0,\n",
    "            1}`.\n",
    "        layers_list: List of network Tensors at different layers. The large margin\n",
    "            is enforced at the layers specified.\n",
    "        gamma: Desired margin, and distance to boundary above the margin will be\n",
    "            clipped.\n",
    "        alpha_factor: Factor to determine the lower bound of margin. Both gamma and\n",
    "            alpha_factor determine points to include in training the margin these\n",
    "            points lie with distance to boundary of [gamma * (1 - alpha), gamma]\n",
    "        top_k: Number of top classes to include in the margin loss.\n",
    "        dist_norm: Distance to boundary defined on norm (options: be 1, 2, np.inf).\n",
    "        epsilon: Small number to avoid division by 0.\n",
    "        use_approximation: If true, use approximation of the margin gradient for\n",
    "            less computationally expensive training.\n",
    "        loss_type: 'worst_top_k', 'average_top_k', or 'all_top_k'. If 'worst_top_k'\n",
    "            only consider the minimum distance to boundary of the top_k classes. If\n",
    "            'average_top_k' consider average distance to boundary. If 'all_top_k'\n",
    "            consider all top_k. When top_k = 1, these choices are equivalent.\n",
    "        loss_collection: Collection to which the loss will be added.\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar `Tensor` of the same type as `logits`.\n",
    "    Raises:\n",
    "        ValueError: If the shape of `logits` doesn't match that of\n",
    "            `one_hot_labels`.    Also if `one_hot_labels` or `logits` is None.\n",
    "    \"\"\"\n",
    "\n",
    "    _ensure_large_margin_args(\"large_margin\", _sentinel, one_hot_labels, logits, layers_list, dist_norm, loss_type)\n",
    "    logits = tf.convert_to_tensor(logits)\n",
    "    one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n",
    "    logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n",
    "    assert top_k > 0\n",
    "    assert top_k <= logits.get_shape()[1]\n",
    "\n",
    "    dual_norm = {1: np.inf, 2: 2, np.inf: 1}\n",
    "    norm_fn = get_norm_fn(dual_norm[dist_norm])\n",
    "    with tf.name_scope(\"large_margin_loss\"):\n",
    "        class_prob = tf.nn.softmax(logits)\n",
    "        # Pick the correct class probability.\n",
    "        correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "        # Class probabilities except the correct.\n",
    "        other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "        if top_k > 1:\n",
    "            # Pick the top k class probabilities other than the correct.\n",
    "            top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "        else:\n",
    "            top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "        # Difference between correct class probailities and top_k probabilities.\n",
    "        difference_prob = correct_class_prob - top_k_class_prob\n",
    "        losses_list = []\n",
    "        for layer in layers_list:\n",
    "            difference_prob_grad = [\n",
    "                tf.layers.flatten(tf.gradients(difference_prob[:, i], layer)[0])\n",
    "                for i in range(top_k)\n",
    "            ]\n",
    "\n",
    "            difference_prob_gradnorm = tf.concat([\n",
    "                tf.map_fn(norm_fn, difference_prob_grad[i])[:, tf.newaxis]\n",
    "                for i in range(top_k)\n",
    "            ], axis=1)\n",
    "\n",
    "            if use_approximation:\n",
    "                difference_prob_gradnorm = tf.stop_gradient(difference_prob_gradnorm)\n",
    "\n",
    "            distance_to_boundary = difference_prob / (difference_prob_gradnorm + epsilon)\n",
    "\n",
    "            if loss_type == \"worst_top_k\":\n",
    "                # Only consider worst distance to boundary.\n",
    "                distance_to_boundary = tf.reduce_min(distance_to_boundary, axis=1)\n",
    "\n",
    "            elif loss_type == \"average_top_k\":\n",
    "                # Only consider average distance to boundary.\n",
    "                distance_to_boundary = tf.reduce_mean(distance_to_boundary, axis=1)\n",
    "\n",
    "            # Distances to consider between distance_upper and distance_lower bounds\n",
    "            distance_upper = gamma\n",
    "            distance_lower = gamma * (1 - alpha_factor)\n",
    "\n",
    "            # Enforce lower bound.\n",
    "            loss_layer = maximum_with_relu(distance_to_boundary, distance_lower)\n",
    "\n",
    "            # Enforce upper bound.\n",
    "            loss_layer = maximum_with_relu(\n",
    "                0, distance_upper - loss_layer) - distance_upper\n",
    "\n",
    "            losses_list.append(tf.reduce_mean(loss_layer))\n",
    "\n",
    "        loss = tf.reduce_mean(losses_list)\n",
    "        # Add loss to loss_collection.\n",
    "        tf.losses.add_loss(loss, loss_collection)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/xyang2/project/data/dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "(100, 784)\n",
      "(100, 784)\n",
      "(10000, 784)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e480e7e6bc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_test_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel_test_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_test_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_test_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a4f77f724512>\u001b[0m in \u001b[0;36msoftmax_model\u001b[0;34m(image_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b69d9b734326>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, is_training)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_units_fc_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m   \"\"\"\n\u001b[1;32m    315\u001b[0m   \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m           spec.max_ndim is not None):\n\u001b[0;32m-> 1410\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m   1412\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = input_data.read_data_sets(\"/home/xyang2/project/data/dataset/mnist/\", one_hot=True)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels))\\\n",
    "    .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "    .shuffle(buffer_size=1000)\\\n",
    "    .batch(100)\\\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_ds)):\n",
    "    print(image_batch.shape)\n",
    "    loss, grads_and_vars = cal_gradient(image_batch, label_batch)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    if step % 100:\n",
    "        print(\"step: {}  loss: {}\".format(step, loss.numpy()))\n",
    "    break\n",
    "\n",
    "model_test_output = softmax_model(data.test.images)\n",
    "model_test_label = data.test.labels\n",
    "correct_prediction = tf.equal(tf.argmax(model_test_output, 1), tf.argmax(model_test_label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"test accuracy = {}\".format(accuracy.numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
