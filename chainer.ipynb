{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-9.0/lib64/'\n",
    "sys.path.append('/usr/local/cuda-9.0/lib64/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch.nn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-659d8826418a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named torch.nn"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "import sys, os, time, argparse\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import Variable, optimizers, cuda, serializers\n",
    "\n",
    "from chainer_func.source.chainer_functions import loss\n",
    "from chainer_func.source.data import Data\n",
    "from chainer_func.source.utils import mkdir_p, load_npz_as_dict\n",
    "from chainer_func.models import CNN, MLP\n",
    "\n",
    "from ExpUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"cifar10\"\n",
    "    args.trainer = \"VATReg\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"mlp\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 10\n",
    "    args.eps = 1\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    \n",
    "    args.gpu = -1\n",
    "    args.data_dir = \"./dataset/cifar10/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 32\n",
    "    args.batchsize_ul = 128\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 120\n",
    "    args.num_iter_per_epoch = 400\n",
    "    args.epoch_decay_start = 80\n",
    "    args.lr = 0.001\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 3.5\n",
    "    args.extra_lamb = 1\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = True\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "    args.data_dir = os.path.join(os.environ['HOME'], \"project/data/dataset/%s\" % args.dataset)\n",
    "\n",
    "    chainer.global_config.cudnn_deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    if int(args.gpu) > -1:\n",
    "        chainer.cuda.get_device(args.gpu).use()\n",
    "    np.random.seed(args.seed)\n",
    "    cp.random.seed(args.seed)\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dirpath, valid=False, dataset_seed=1):\n",
    "    if valid:\n",
    "        train_l = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'labeled_train_valid.npz'))\n",
    "        train_ul = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'unlabeled_train_valid.npz'))\n",
    "        test = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'test_valid.npz'))\n",
    "    else:\n",
    "        train_l = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'labeled_train.npz'))\n",
    "        train_ul = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'unlabeled_train.npz'))\n",
    "        test = load_npz_as_dict(os.path.join(dirpath, 'seed' + str(dataset_seed), 'test.npz'))\n",
    "    if 'mnist' in dirpath:\n",
    "        train_set, test_set = load_mnist_dataset()\n",
    "    print(train_ul['images'].shape)\n",
    "    print(train_l['labels'][:20])\n",
    "    train_l['images'] = train_l['images'].reshape(train_l['images'].shape[0], 3, 32, 32).astype(np.float32)\n",
    "    train_ul['images'] = train_ul['images'].reshape(train_ul['images'].shape[0], 3, 32, 32).astype(np.float32)\n",
    "    test['images'] = test['images'].reshape(test['images'].shape[0], 3, 32, 32).astype(np.float32)\n",
    "    return Data(train_l['images'], train_l['labels'].astype(np.int32)), \\\n",
    "           Data(train_ul['images'], train_ul['labels'].astype(np.int32)), \\\n",
    "           Data(test['images'], test['labels'].astype(np.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "N_train_labeled:4000, N_train_unlabeled:50000\n",
      "-2591.2783 -0.00021087877 24.7316\n"
     ]
    }
   ],
   "source": [
    "train_l, train_ul, test = load_dataset(args.data_dir, valid=args.validation, dataset_seed=args.dataset_seed)\n",
    "print(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "print(train_l.data.sum(), train_l.data.mean(), train_l.data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 5 2 6 7 2 3 2 0 8 2 4 3 5 1 6 3 3 3 2 8 1 7 0 0 6 6 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "x_u, _ = train_ul.get(args.batchsize_ul, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        super(MLP, self).__init__(\n",
    "            l_c1=L.Linear(3072, 1200, initialW=initializer),\n",
    "            l_c2=L.Linear(1200, 1200, initialW=initializer),\n",
    "            l_c3=L.Linear(1200, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(1200),\n",
    "            bn2=L.BatchNormalization(1200),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.l_c1(h)\n",
    "        h = F.relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        # h = F.relu(h)\n",
    "        h = self.l_c2(h)\n",
    "        h = F.relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        # h = F.relu(h)\n",
    "        logit = self.l_c3(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable gamma([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc.bn1.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, test=False, update_batch_stats=True):\n",
    "    if test:\n",
    "        return F.fixed_batch_normalization(x, bn.gamma, bn.beta, bn.avg_mean, bn.avg_var)\n",
    "    elif not update_batch_stats:\n",
    "        return F.batch_normalization(x, bn.gamma, bn.beta)\n",
    "    else:\n",
    "        return bn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match MLP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064\n",
      "[0 2 1 5 2 6 7 2 3 2 0 8 2 4 3 5 1 6 3 3 3 2 8 1 7 0 0 6 6 3 3 1]\n",
      "-5.4525614\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "print(x.sum())\n",
    "print(t)\n",
    "set_framework_seed(1)\n",
    "te = enc(Variable(x))\n",
    "print(te.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        super(CNN, self).__init__(\n",
    "            c1=L.Convolution2D(3, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c2=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c3=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c4=L.Convolution2D(128, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c5=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c6=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c7=L.Convolution2D(256, 512, ksize=3, stride=1, pad=0, initialW=initializer),\n",
    "            c8=L.Convolution2D(512, 256, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            c9=L.Convolution2D(256, 128, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            l_cl=L.Linear(128, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(128),\n",
    "            bn2=L.BatchNormalization(128),\n",
    "            bn3=L.BatchNormalization(128),\n",
    "            bn4=L.BatchNormalization(256),\n",
    "            bn5=L.BatchNormalization(256),\n",
    "            bn6=L.BatchNormalization(256),\n",
    "            bn7=L.BatchNormalization(512),\n",
    "            bn8=L.BatchNormalization(256),\n",
    "            bn9=L.BatchNormalization(128),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.average_pooling_2d(h, ksize=h.data.shape[2])\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 40.848587\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "out = enc( Variable(x), update_batch_stats=True)\n",
    "print(x.sum(), out.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN without dropout\n",
    "\n",
    "the results are very close, the difference is caused by error/precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.260621)\n",
      "-239.57237 variable(2.737069)\n",
      "2.0764809 variable(2.6666842)\n",
      "126.50333 variable(2.240642)\n",
      "31.462202 variable(2.0930262)\n",
      "-653.71985 variable(2.2371545)\n",
      "-206.56343 variable(2.0542524)\n",
      "-199.04095 variable(2.359859)\n",
      "173.35344 variable(2.3512278)\n",
      "-129.28891 variable(2.3139074)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10249999959021806\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.270143)\n",
      "-239.57237 variable(2.5744624)\n",
      "2.0764809 variable(2.8344507)\n",
      "126.50333 variable(2.2541957)\n",
      "31.462202 variable(2.2864416)\n",
      "-653.71985 variable(2.2899199)\n",
      "-206.56343 variable(2.2821083)\n",
      "-199.04095 variable(2.5041566)\n",
      "173.35344 variable(2.545939)\n",
      "-129.28891 variable(2.350461)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    set_framework_seed(it % 10000)\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10439999988302588\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
