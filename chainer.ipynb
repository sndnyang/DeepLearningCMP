{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data by tensorflow\n",
    "\n",
    "import torch and tensorflow\n",
    "\n",
    "set memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "import cupy as cp\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import Variable, optimizers, cuda, serializers\n",
    "\n",
    "gpu = \"\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "tf.enable_eager_execution(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tf_func import data_loader\n",
    "from tf_func import mnist_model\n",
    "\n",
    "\n",
    "class ConfigDict(object):\n",
    "    \"\"\"MNIST configration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # List of tuples specify (kernel_size, number of filters) for each layer.\n",
    "        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\n",
    "        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\n",
    "        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\n",
    "        self.num_units_fc_layers = [512]\n",
    "        self.dropout_rate = 0\n",
    "        self.batch_norm = True\n",
    "        self.activation = None\n",
    "        self.regularizer = None\n",
    "        \n",
    "        \n",
    "config = ConfigDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"train\",\n",
    "    batch_size=128,\n",
    "    is_training=False)\n",
    "\n",
    "test_dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"test\",\n",
    "    batch_size=128,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12790.145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels, num_examples, num_classes = (dataset.images, dataset.labels, dataset.num_examples, dataset.num_classes)\n",
    "images, labels = dataset.get_next()\n",
    "images.numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerTensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Variable(images.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d12faadc7840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "p.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight init\n",
    "# empty\n",
    "import random\n",
    "def set_framework_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    except cp.cuda.runtime.CUDARuntimeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x)\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "\n",
    "def loss_test(model, x, y):\n",
    "    logit = model(x)\n",
    "    loss = F.softmax_cross_entropy(logit, y).data\n",
    "    acc = F.accuracy(logit, y).data\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, test_iter):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with chainer.using_config(\"train\", False):\n",
    "        for images, labels in test_iter:\n",
    "            size += images.numpy().shape[0]\n",
    "            images = images.numpy()\n",
    "            labels = labels.numpy()\n",
    "            loss, acc = loss_test(model, images, labels)\n",
    "            total_loss += loss * images.shape[0]\n",
    "            \n",
    "            total_acc += acc * images.shape[0]\n",
    "    \n",
    "    return total_acc / size, total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_ch=1, n_res=28, n_outputs=10, dropout_rate=0.5, top_bn=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        \n",
    "        \n",
    "EPS = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "class CNN3(chainer.Chain):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        self.batch_norm = config.batch_norm\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        super(CNN3, self).__init__(\n",
    "            conv1 = L.Convolution2D(1, 32, ksize=5, stride=1,\n",
    "                                     pad=2, nobias=self.batch_norm,\n",
    "                                     initialW=initializer),\n",
    "            conv2 = L.Convolution2D(32, 64, ksize=5, stride=1,\n",
    "                                     pad=2, nobias=self.batch_norm,\n",
    "                                     initialW=initializer),\n",
    "            fc1 = L.Linear(64 * 7 * 7, 512),\n",
    "            fc2 = L.Linear(512, 10)\n",
    "        )\n",
    "            \n",
    "        if self.batch_norm:\n",
    "            self.add_link('bn1', L.BatchNormalization(32, eps=EPS))\n",
    "            self.add_link('bn2', L.BatchNormalization(64, eps=EPS))\n",
    "\n",
    "    def forward(self, images):\n",
    "        endpoints = {}\n",
    "        x = images\n",
    "\n",
    "        # Conv Layer 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pooling_2d(x, 2, stride=2)\n",
    "        if debug:\n",
    "            print(\"after pool\", \"%.4f\" % x.sum().item())\n",
    "        if self.dropout_rate > 0:\n",
    "            x = F.dropout(x, ratio=self.dropout_rate)\n",
    "        if self.batch_norm:\n",
    "            if debug:\n",
    "                print(\"before batech norm %.4f\" % (x ** 2).sum().item())\n",
    "            x = self.bn1(x)\n",
    "            if debug:\n",
    "                print(\"after batech norm %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"conv_layer0\"] = x\n",
    "\n",
    "        # Conv Layer 2\n",
    "        x = F.max_pool2d(nfunc.relu(self.conv2(x)), 2, stride=2)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = F.dropout(x, ratio=self.dropout_rate)\n",
    "        if self.batch_norm:\n",
    "            if debug:\n",
    "                print(\"before batech norm %.4f\" % (x ** 2).sum().item())\n",
    "            x = self.bn2(x)\n",
    "            if debug:\n",
    "                print(\"after batech norm %.4f\" % (x ** 2).sum().item())\n",
    "        if debug:\n",
    "            print(\"After two conv %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"conv_layer1\"] = x\n",
    "        x = F.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        # fully connect layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.drop_fc1(x)\n",
    "        if debug:\n",
    "            print(\"logits %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"fc_layer0\"] = x\n",
    "\n",
    "        # fully connect layer logit\n",
    "        x = self.fc2(x)\n",
    "        if debug:\n",
    "            print(\"logits %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"logits\"] = x\n",
    "        return x, endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "It works fine for Chainer and PyTorch\n",
    "\n",
    "without BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n"
     ]
    },
    {
     "ename": "InvalidType",
     "evalue": "\nInvalid operation is performed in: Convolution2DFunction (Forward)\n\nExpect: in_types[0].shape[1] == in_types[1].shape[1] * 1\nActual: 28 != 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidType\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6cea627d2a26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_labeled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss_ul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvat_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_logit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-f32633d3dd36>\u001b[0m in \u001b[0;36mloss_labeled\u001b[0;34m(forward, x, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_labeled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/link.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Call forward_postprocess hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-5f5f1c438570>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# Conv Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pooling_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/link.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Call forward_postprocess hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/links/connection/convolution_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m         return convolution_2d.convolution_2d(\n\u001b[1;32m    172\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             groups=self.groups)\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/functions/connection/convolution_2d.py\u001b[0m in \u001b[0;36mconvolution_2d\u001b[0;34m(x, W, b, stride, pad, cover_all, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mhooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36m_check_data_type_forward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0min_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in_types'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function_check_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/functions/connection/convolution_2d.py\u001b[0m in \u001b[0;36mcheck_type_forward\u001b[0;34m(self, in_types)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mx_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mw_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mx_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         )\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/utils/type_check.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(*bool_exprs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexpr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbool_exprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/chainer/utils/type_check.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m             raise InvalidType(\n\u001b[1;32m    482\u001b[0m                 \u001b[0;34m'{0} {1} {2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 '{0} {1} {2}'.format(left, self.inv, right))\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidType\u001b[0m: \nInvalid operation is performed in: Convolution2DFunction (Forward)\n\nExpect: in_types[0].shape[1] == in_types[1].shape[1] * 1\nActual: 28 != 1"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = False\n",
    "debug = True\n",
    "set_framework_seed(1)\n",
    "model = CNN3(config)\n",
    "\n",
    "max_iters = 30\n",
    "debug = False\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = optimizers.Adam(alpha=lr)\n",
    "optimizer.setup(model)\n",
    "optimizer.use_cleargrads()\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(model, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(model, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"mnist\"\n",
    "    args.trainer = \"VATReg\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"mlp\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 10\n",
    "    args.eps = 1\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    \n",
    "    args.gpu = -1\n",
    "    args.data_dir = \"./dataset/cifar10/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 100\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 100\n",
    "    args.num_iter_per_epoch = 1\n",
    "    args.epoch_decay_start = 80\n",
    "    args.lr = 0.001\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 3.5\n",
    "    args.extra_lamb = 1\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = True\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "    args.data_dir = os.path.join(os.environ['HOME'], \"project/data/dataset/%s\" % args.dataset)\n",
    "\n",
    "    chainer.global_config.cudnn_deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    if int(args.gpu) > -1:\n",
    "        chainer.cuda.get_device(args.gpu).use()\n",
    "    np.random.seed(args.seed)\n",
    "    cp.random.seed(args.seed)\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_labeled:100, N_train_unlabeled:60000\n",
      "10491.246 0.13381691 0.99609375\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "train_l, train_ul, test = load_dataset(args.data_dir, valid=args.validation, dataset_seed=args.dataset_seed, size=100)\n",
    "print(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "print(train_l.data.sum(), train_l.data.mean(), train_l.data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 3, 9, 6, 1, 2, 8, 5, 3, 3, 6, 7, 9, 4, 2, 6, 2, 7, 1, 1, 9,\n",
       "       9, 0, 5, 4, 0, 1, 5, 4, 8, 7, 4, 4, 3, 0, 0, 6, 1, 0, 4, 8, 2, 2,\n",
       "       0, 7, 4, 7, 3, 9, 9, 2, 6, 0, 2, 3, 1, 0, 3, 8, 2, 5, 0, 7, 7, 7,\n",
       "       4, 6, 7, 6, 5, 5, 8, 8, 9, 1, 8, 5, 2, 6, 9, 5, 3, 2, 4, 6, 3, 8,\n",
       "       1, 9, 3, 6, 5, 5, 7, 4, 8, 1, 0, 8], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 4 5 5 2 0 3 6 7 5 0 1 6 2 4 7 0 2 3 3 8 1 0 1 0 7 1 4 1 6 8 2 4 1 4 3\n",
      " 8 0 3 3 0 5 2 2 8 7 6 0 0 2 3 5 6 3 9 9 8 9 9 8 4 5 2 7 9 5 2 1 3 0 9 5 7\n",
      " 8 8 9 7 6 4 4 5 6 7 1 9 4 2 5 8 9 6 7 6 1 1 3 8 7 6]\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable gamma([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match MLP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10491.246\n",
      "[9 4 4 5 5 2 0 3 6 7 5 0 1 6 2 4 7 0 2 3 3 8 1 0 1 0 7 1 4 1 6 8 2 4 1 4 3\n",
      " 8 0 3 3 0 5 2 2 8 7 6 0 0 2 3 5 6 3 9 9 8 9 9 8 4 5 2 7 9 5 2 1 3 0 9 5 7\n",
      " 8 8 9 7 6 4 4 5 6 7 1 9 4 2 5 8 9 6 7 6 1 1 3 8 7 6]\n",
      "-139.04709\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "print(x.sum())\n",
    "print(t)\n",
    "set_framework_seed(1)\n",
    "te = enc(Variable(x))\n",
    "print(te.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_labeled(enc, Variable(x), t)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, enc, test, args):\n",
    "    with chainer.using_config(\"train\", False):\n",
    "        acc_test_sum = 0\n",
    "        test_x, test_t = test.get()\n",
    "        N_test = test_x.shape[0]\n",
    "        for i in range(0, N_test, args.batchsize_eval):\n",
    "            x = test_x[i:i + args.batchsize_eval]\n",
    "            t = test_t[i:i + args.batchsize_eval]\n",
    "            if args.gpu > -1:\n",
    "                x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "            _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "            acc_test_sum += acc * x.shape[0]\n",
    "        accs_test = acc_test_sum / N_test\n",
    "        if epoch < 5 or epoch % 10 == 0:\n",
    "            wlog(\"Epoch:{}, nll loss:{}\".format(epoch, cl_losses[epoch]))\n",
    "            wlog(\"test acc:{}\".format(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 16:23:03,625 - <ipython-input-62-239c4b580332>[line:2]: N_train_labeled:100, N_train_unlabeled:60000\n",
      "2019-03-06 16:23:04,938 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:0, nll loss:2.476536750793457\n",
      "2019-03-06 16:23:04,939 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.46589999765157697\n",
      "2019-03-06 16:23:06,299 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:1, nll loss:0.2439270168542862\n",
      "2019-03-06 16:23:06,301 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.5460999998450279\n",
      "2019-03-06 16:23:07,580 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:2, nll loss:0.0213171374052763\n",
      "2019-03-06 16:23:07,581 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.5950999960303307\n",
      "2019-03-06 16:23:08,835 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:3, nll loss:0.004524578806012869\n",
      "2019-03-06 16:23:08,837 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.6248999983072281\n",
      "2019-03-06 16:23:10,134 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:4, nll loss:0.0017549371114000678\n",
      "2019-03-06 16:23:10,135 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.6518000012636185\n",
      "2019-03-06 16:23:17,099 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:10, nll loss:0.00028099058545194566\n",
      "2019-03-06 16:23:17,101 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.709400002360344\n",
      "2019-03-06 16:23:29,445 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:20, nll loss:0.00016156196943484247\n",
      "2019-03-06 16:23:29,447 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7314000028371811\n",
      "2019-03-06 16:23:41,800 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:30, nll loss:0.0001131248427554965\n",
      "2019-03-06 16:23:41,803 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.737700001001358\n",
      "2019-03-06 16:23:53,769 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:40, nll loss:8.460044773528352e-05\n",
      "2019-03-06 16:23:53,770 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7403000003099441\n",
      "2019-03-06 16:24:05,615 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:50, nll loss:6.760597170796245e-05\n",
      "2019-03-06 16:24:05,617 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7411999982595444\n",
      "2019-03-06 16:24:17,296 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:60, nll loss:5.678176603396423e-05\n",
      "2019-03-06 16:24:17,297 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7432000011205673\n",
      "2019-03-06 16:24:29,001 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:70, nll loss:4.938125493936241e-05\n",
      "2019-03-06 16:24:29,003 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.743200004696846\n",
      "2019-03-06 16:24:40,917 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:80, nll loss:4.3830870708916336e-05\n",
      "2019-03-06 16:24:40,920 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7442000007629395\n",
      "2019-03-06 16:24:52,970 - <ipython-input-61-497a3700c69f>[line:15]: Epoch:90, nll loss:3.9625167119083926e-05\n",
      "2019-03-06 16:24:52,973 - <ipython-input-61-497a3700c69f>[line:16]: test acc:0.7446999990940094\n"
     ]
    }
   ],
   "source": [
    "train_l, train_ul, test = load_dataset(args.data_dir, valid=args.validation, dataset_seed=args.dataset_seed, size=args.size)\n",
    "wlog(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "    \n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "\n",
    "cl_losses = np.zeros(args.num_epochs)\n",
    "for epoch in range(args.num_epochs):\n",
    "    sum_loss_l = 0\n",
    "    sum_loss_ul = 0\n",
    "    for it in range(args.num_iter_per_epoch):\n",
    "        with chainer.using_config(\"train\", True):\n",
    "            x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            loss_total = loss_labeled(enc, Variable(x), t)\n",
    "            enc.cleargrads()\n",
    "            loss_total.backward()\n",
    "            optimizer.update()\n",
    "            sum_loss_l += loss_total.data\n",
    "        cl_losses[epoch] = sum_loss_l / args.num_iter_per_epoch\n",
    "    evaluate(epoch, enc, test, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1.0)\n",
    "        super(CNN, self).__init__(\n",
    "            c1=L.Convolution2D(3, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c2=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c3=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c4=L.Convolution2D(128, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c5=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c6=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c7=L.Convolution2D(256, 512, ksize=3, stride=1, pad=0, initialW=initializer),\n",
    "            c8=L.Convolution2D(512, 256, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            c9=L.Convolution2D(256, 128, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            l_cl=L.Linear(128, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(128),\n",
    "            bn2=L.BatchNormalization(128),\n",
    "            bn3=L.BatchNormalization(128),\n",
    "            bn4=L.BatchNormalization(256),\n",
    "            bn5=L.BatchNormalization(256),\n",
    "            bn6=L.BatchNormalization(256),\n",
    "            bn7=L.BatchNormalization(512),\n",
    "            bn8=L.BatchNormalization(256),\n",
    "            bn9=L.BatchNormalization(128),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.average_pooling_2d(h, ksize=h.data.shape[2])\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 40.848587\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "out = enc( Variable(x), update_batch_stats=True)\n",
    "print(x.sum(), out.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_labeled(enc, Variable(x), t)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_test(forward, x, t):\n",
    "    logit = forward(x, train=False)\n",
    "    L, acc = F.softmax_cross_entropy(logit, t).data, F.accuracy(logit, t).data\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN without dropout\n",
    "\n",
    "the results are very close, the difference is caused by error/precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.260621)\n",
      "-239.57237 variable(2.737069)\n",
      "2.0764809 variable(2.6666842)\n",
      "126.50333 variable(2.240642)\n",
      "31.462202 variable(2.0930262)\n",
      "-653.71985 variable(2.2371545)\n",
      "-206.56343 variable(2.0542524)\n",
      "-199.04095 variable(2.359859)\n",
      "173.35344 variable(2.3512278)\n",
      "-129.28891 variable(2.3139074)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10249999959021806\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 variable(2.270143)\n",
      "-239.57237 variable(2.5744624)\n",
      "2.0764809 variable(2.8344507)\n",
      "126.50333 variable(2.2541957)\n",
      "31.462202 variable(2.2864416)\n",
      "-653.71985 variable(2.2899199)\n",
      "-206.56343 variable(2.2821083)\n",
      "-199.04095 variable(2.5041566)\n",
      "173.35344 variable(2.545939)\n",
      "-129.28891 variable(2.350461)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "set_framework_seed(1)\n",
    "train_l.reseed()\n",
    "for it in range(10):\n",
    "    set_framework_seed(it % 10000)\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x, t = train_l.get(args.batchsize, gpu=args.gpu, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "        \n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        print(x.sum(), loss_l)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10439999988302588\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config(\"train\", False):\n",
    "    acc_test_sum = 0\n",
    "    test_x, test_t = test.get()\n",
    "    N_test = test_x.shape[0]\n",
    "    for i in range(0, N_test, args.batchsize_eval):\n",
    "        x = test_x[i:i + args.batchsize_eval]\n",
    "        t = test_t[i:i + args.batchsize_eval]\n",
    "        if args.gpu > -1:\n",
    "            x, t = cuda.to_gpu(x, device=args.gpu), cuda.to_gpu(t, device=args.gpu)\n",
    "        _, acc = loss_test(enc, Variable(x), Variable(t))\n",
    "        acc_test_sum += acc * x.shape[0]\n",
    "    print(acc_test_sum / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064\n",
      "[0 2 1 5 2 6 7 2 3 2 0 8 2 4 3 5 1 6 3 3 3 2 8 1 7 0 0 6 6 3 3 1]\n",
      "-5.4525633\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "# enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "print(x.sum())\n",
    "print(t)\n",
    "set_framework_seed(1)\n",
    "te = enc(Variable(x))\n",
    "print(te.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.4525633\n"
     ]
    }
   ],
   "source": [
    "x_variable = Variable(x)\n",
    "te = enc(x_variable)\n",
    "print(te.data.sum())\n",
    "logits = te\n",
    "\n",
    "new_data = F.repeat(logits.reshape(32, 10, 1), 10, axis=2)\n",
    "new_data_t = F.transpose(new_data, axes=(0, 2, 1))\n",
    "dif = F.absolute(new_data - new_data_t)\n",
    "dif.data[:, np.arange(10), np.arange(10)] = 10000\n",
    "minimum_dif = F.min(dif, axis=(1, 2))\n",
    "loss = F.sum(minimum_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "d = x_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 32, 32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([43.8882  , 32.518997, 42.099136, 42.29032 , 35.56052 , 32.028145,\n",
       "          44.87703 , 35.137177, 42.562088, 49.06534 , 37.460518, 56.70897 ,\n",
       "          48.79129 , 58.22379 , 58.045277, 53.70053 , 44.11779 , 36.081795,\n",
       "          57.63233 , 31.73954 , 48.41804 , 42.846928, 40.61837 , 42.383553,\n",
       "          47.325497, 34.945408, 45.08779 , 55.141594, 45.598877, 52.274117,\n",
       "          45.350273, 36.295547])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.batch_l2_norm_squared(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.8882  , 32.518997, 42.099136, 42.29032 , 35.56052 , 32.028145,\n",
       "       44.87703 , 35.137177, 42.562088, 49.06534 , 37.460518, 56.70897 ,\n",
       "       48.79129 , 58.22379 , 58.045277, 53.70053 , 44.11779 , 36.081795,\n",
       "       57.63233 , 31.73954 , 48.41804 , 42.846928, 40.61837 , 42.383553,\n",
       "       47.325497, 34.945408, 45.08779 , 55.141594, 45.598877, 52.274117,\n",
       "       45.350273, 36.295547], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.sum(d ** 2, axis =tuple(range(1, len(d.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.get_array_module(x_variable.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d / xp.sqrt(xp.sum(d ** 2, axis =tuple(range(1, len(d.shape))), keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 32, 32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.03196675, 0.01531726, 0.03652483, 0.01392016, 0.01174247,\n",
       "          0.0463711 , 0.04788411, 0.01914483, 0.02834719, 0.05728129,\n",
       "          0.00792599, 0.01005088, 0.00395525, 0.06017971, 0.01658297,\n",
       "          0.05855221, 0.03900647, 0.00564754, 0.00618389, 0.0116004 ,\n",
       "          0.03603446, 0.03971651, 0.01833946, 0.01108512, 0.00718176,\n",
       "          0.01502152, 0.17572033, 0.00577849, 0.00456291, 0.0354563 ,\n",
       "          0.01088476, 0.10022384])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.0048253 , 0.00268604, 0.00562926, 0.00214054, 0.00196914,\n",
       "          0.00819373, 0.00714791, 0.00322974, 0.00434509, 0.00817759,\n",
       "          0.00129499, 0.00133468, 0.00056624, 0.00788678, 0.0021766 ,\n",
       "          0.00799013, 0.00587259, 0.00094019, 0.00081457, 0.00205908,\n",
       "          0.00517863, 0.00606752, 0.00287757, 0.00170271, 0.00104396,\n",
       "          0.00254108, 0.02616932, 0.00077817, 0.00067572, 0.004904  ,\n",
       "          0.00161633, 0.01663583])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_dif / xp.sqrt(xp.sum(d ** 2, axis =tuple(range(1, len(d.shape)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob = F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[1.54169023e-01, 5.03710285e-02, 8.62801373e-02, 5.62610142e-02,\n",
       "           7.01920614e-02, 4.87862900e-02, 1.48522973e-01, 1.27890825e-01,\n",
       "           1.61214247e-01, 9.63124484e-02],\n",
       "          [1.34237275e-01, 5.56553602e-02, 1.00720234e-01, 1.13789409e-01,\n",
       "           1.36309236e-01, 9.80082080e-02, 1.54791638e-01, 6.56634644e-02,\n",
       "           1.04473017e-01, 3.63521986e-02],\n",
       "          [1.61101922e-01, 2.13813096e-01, 6.77430443e-03, 3.02409520e-03,\n",
       "           7.02630868e-03, 9.21606726e-04, 9.70342234e-02, 5.04525900e-01,\n",
       "           4.32566099e-04, 5.34597319e-03]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prob[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class_prob = class_prob[np.arange(class_prob.shape[0]), t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([0.15416902, 0.10072023, 0.2138131 , 0.08669429, 0.32581642,\n",
       "          0.0384522 , 0.23383127, 0.10946563, 0.01286336, 0.10806721,\n",
       "          0.03154936, 0.0925095 , 0.07900738, 0.02592808, 0.04358358,\n",
       "          0.09468471, 0.2182813 , 0.15935653, 0.01923206, 0.09014764,\n",
       "          0.08973202, 0.15906514, 0.25026926, 0.0619849 , 0.08292997,\n",
       "          0.09619284, 0.05878942, 0.04833115, 0.19643797, 0.08501525,\n",
       "          0.03976822, 0.28050393])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_margin(_sentinel=None, logits=None, one_hot_labels=None, layers_list=None, gamma=10000, alpha_factor=2, top_k=1, dist_norm=2,\n",
    "                 epsilon=1e-8, use_approximation=True, loss_type=\"all_top_k\", loss_collection=tf.GraphKeys.LOSSES):\n",
    "    # Pick the correct class probability.\n",
    "    correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "    # Class probabilities except the correct.\n",
    "    other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "    if top_k > 1:\n",
    "        # Pick the top k class probabilities other than the correct.\n",
    "        top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "    else:\n",
    "        top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "    # Difference between correct class probailities and top_k probabilities.\n",
    "    difference_prob = correct_class_prob - top_k_class_prob\n",
    "    losses_list = []\n",
    "\n",
    "    difference_prob_grad = [\n",
    "        tf.layers.flatten(tf.gradients(difference_prob[:, i], layer)[0])\n",
    "        for i in range(top_k)\n",
    "    ]\n",
    "\n",
    "    difference_prob_gradnorm = tf.concat([\n",
    "        tf.map_fn(norm_fn, difference_prob_grad[i])[:, tf.newaxis]\n",
    "        for i in range(top_k)\n",
    "    ], axis=1)\n",
    "\n",
    "    if use_approximation:\n",
    "        difference_prob_gradnorm = tf.stop_gradient(difference_prob_gradnorm)\n",
    "\n",
    "    distance_to_boundary = difference_prob / (\n",
    "                    difference_prob_gradnorm + epsilon)\n",
    "\n",
    "    if loss_type == \"worst_top_k\":\n",
    "        # Only consider worst distance to boundary.\n",
    "        distance_to_boundary = tf.reduce_min(distance_to_boundary, axis=1)\n",
    "\n",
    "    elif loss_type == \"average_top_k\":\n",
    "        # Only consider average distance to boundary.\n",
    "        distance_to_boundary = tf.reduce_mean(distance_to_boundary, axis=1)\n",
    "\n",
    "    # Distances to consider between distance_upper and distance_lower bounds\n",
    "    distance_upper = gamma\n",
    "    distance_lower = gamma * (1 - alpha_factor)\n",
    "\n",
    "    # Enforce lower bound.\n",
    "    loss_layer = maximum_with_relu(distance_to_boundary, distance_lower)\n",
    "\n",
    "    # Enforce upper bound.\n",
    "    loss_layer = maximum_with_relu(\n",
    "        0, distance_upper - loss_layer) - distance_upper\n",
    "\n",
    "    losses_list.append(tf.reduce_mean(loss_layer))\n",
    "\n",
    "    loss = tf.reduce_mean(losses_list)\n",
    "    # Add loss to loss_collection.\n",
    "    tf.losses.add_loss(loss, loss_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
