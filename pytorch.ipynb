{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-9.0/lib64/'\n",
    "# sometimes it's required by some frameworks\n",
    "sys.path.append('/usr/local/cuda-9.0/lib64/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch_func.models as models\n",
    "\n",
    "from torch_func.Subset import SubsetDataset\n",
    "from torch_func.sampler import InfiniteSampler\n",
    "from torch_func.TrainerUtils import weights_init\n",
    "from torch_func.evaluate import evaluate_classifier\n",
    "from torch_func.data_loader import load_dataset_semi, load_data_set\n",
    "from ExpUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"cifar10\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"mlp\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 4000\n",
    "    args.no_cuda = True\n",
    "    \n",
    "    args.xi = 1e-6\n",
    "    args.eps = 1\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "\n",
    "    args.gpu_id = 1\n",
    "    args.data_dir = \"./dataset/svhn/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 32\n",
    "    args.batchsize_ul = 128\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 120\n",
    "    args.num_iter_per_epoch = 400\n",
    "    args.epoch_decay_start = 80\n",
    "    args.lr = 0.001\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 3.5\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = True\n",
    "    \n",
    "    args.data_dir = os.path.join(\"./dataset/%s\" % args.dataset)\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    # very important to have the deterministic results -> slower\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    args.device = device\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch code\n",
    "\n",
    "## Load data\n",
    "\n",
    "The datasets can be downloaded by the script in dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_labeled:4000\n",
      "tensor(-2591.2605)\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "set_framework_seed(args.seed)\n",
    "device = args.device\n",
    "\n",
    "# train_set_o, test_set_o, shape, num_classes = load_data_set(args.dataset)\n",
    "train_set, _, test, shape, num_classes = load_dataset_semi(args.dataset)\n",
    "test_loader = DataLoader(test, 256, num_workers=3)\n",
    "train_set = SubsetDataset(train_set, list(range(args.size)))\n",
    "\n",
    "print(\"N_train_labeled:{}\".format(len(train_set)))\n",
    "print(train_set.dataset.tensors[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "set_framework_seed(1)\n",
    "train_iter = iter(DataLoader(train_set, batch_size, num_workers=0, sampler=InfiniteSampler(len(train_set))))\n",
    "x, y = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 5, 2, 6, 7, 2, 3, 2, 0, 8, 2, 4, 3, 5, 1, 6, 3, 3, 3, 2, 8, 1,\n",
       "        7, 0, 0, 6, 6, 3, 3, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init the model\n",
    "\n",
    "PyTorch use its own initializing library which is different from numpy.\n",
    "\n",
    "So I prefer the method to initialize the weight by a numpy random function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, update_batch_stats=True):\n",
    "    if bn.training is False:\n",
    "        return bn(x)\n",
    "    elif not update_batch_stats:\n",
    "        return nfunc.batch_norm(x, None, None, bn.weight, bn.bias, True, bn.momentum, bn.eps)\n",
    "    else:\n",
    "        return bn(x)\n",
    "\n",
    "\n",
    "class FullyNet(nn.Module):\n",
    "    def __init__(self, n_class, n_ch, res):\n",
    "        super(FullyNet, self).__init__()\n",
    "        self.input_len = n_ch * res * res\n",
    "        self.fc1 = nn.Linear(self.input_len, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, n_class)\n",
    "\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "\n",
    "    def forward(self, x, update_batch_stats=True):\n",
    "        h = nfunc.relu(call_bn(self.bn_fc1, self.fc1(x.view(-1, self.input_len)), update_batch_stats))\n",
    "        h = nfunc.relu(call_bn(self.bn_fc2, self.fc2(h), update_batch_stats))\n",
    "        # h = nfunc.relu(self.bn_fc1(self.fc1(x.view(-1, self.input_len))))\n",
    "        # h = nfunc.relu(self.bn_fc2(self.fc2(h)))\n",
    "        # h = nfunc.relu(self.fc1(x.view(-1, self.input_len)))\n",
    "        # h = nfunc.relu(self.fc2(h))\n",
    "        return self.fc3(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FullyNet(\n",
       "  (fc1): Linear(in_features=3072, out_features=1200, bias=True)\n",
       "  (fc2): Linear(in_features=1200, out_features=1200, bias=True)\n",
       "  (fc3): Linear(in_features=1200, out_features=10, bias=True)\n",
       "  (bn_fc1): BatchNorm1d(1200, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_fc2): BatchNorm1d(1200, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "print(args.arch)\n",
    "model = getattr(models, args.arch)\n",
    "set_framework_seed(args.seed)\n",
    "cls = FullyNet(num_classes, shape[0], shape[1]).to(device)\n",
    "cls.apply(weights_init)\n",
    "cls = cls.to(device)\n",
    "cls.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-562.9708)\n",
      "tensor([0, 2, 1, 5, 2, 6, 7, 2, 3, 2, 0, 8, 2, 4, 3, 5, 1, 6, 3, 3, 3, 2, 8, 1,\n",
      "        7, 0, 0, 6, 6, 3, 3, 1])\n",
      "tensor(-5.4526, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "print(x.sum())\n",
    "print(y)\n",
    "output = cls(x.to(args.device))\n",
    "print(output.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model wi/wo dropout\n",
    "\n",
    "Verify the outputs of CNN models with/without dropout layer\n",
    "\n",
    "1. Without dropout: There's a small gap between PyTorch and Chainer made by the precision(My guess)\n",
    "2. With dropout: An obivious difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape=(3, 32, 32), num_conv=128, top_bn=False, dropout=False):\n",
    "        super(CNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.c1 = nn.Conv2d(input_shape[0], num_conv, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(num_conv, num_conv, 3, 1, 1)\n",
    "        self.c3 = nn.Conv2d(num_conv, num_conv, 3, 1, 1)\n",
    "        self.c4 = nn.Conv2d(num_conv, num_conv * 2, 3, 1, 1)\n",
    "        self.c5 = nn.Conv2d(num_conv * 2, num_conv * 2, 3, 1, 1)\n",
    "        self.c6 = nn.Conv2d(num_conv * 2, num_conv * 2, 3, 1, 1)\n",
    "        self.c7 = nn.Conv2d(num_conv * 2, num_conv * 4, 3, 1, 0)\n",
    "        self.c8 = nn.Conv2d(num_conv * 4, num_conv * 2, 1, 1, 0)\n",
    "        self.c9 = nn.Conv2d(num_conv * 2, 128, 1, 1, 0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_conv, eps=2e-5)\n",
    "        self.bn2 = nn.BatchNorm2d(num_conv, eps=2e-5)\n",
    "        self.bn3 = nn.BatchNorm2d(num_conv, eps=2e-5)\n",
    "        self.bn4 = nn.BatchNorm2d(num_conv * 2, eps=2e-5)\n",
    "        self.bn5 = nn.BatchNorm2d(num_conv * 2, eps=2e-5)\n",
    "        self.bn6 = nn.BatchNorm2d(num_conv * 2, eps=2e-5)\n",
    "        self.bn7 = nn.BatchNorm2d(num_conv * 4, eps=2e-5)\n",
    "        self.bn8 = nn.BatchNorm2d(num_conv * 2, eps=2e-5)\n",
    "        self.bn9 = nn.BatchNorm2d(num_conv, eps=2e-5)\n",
    "        self.bnf = nn.BatchNorm2d(128, eps=2e-5)\n",
    "        self.mp1 = nn.MaxPool2d(2, 2)\n",
    "        self.mp2 = nn.MaxPool2d(2, 2)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.aap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(128, 10)\n",
    "\n",
    "        self.top_bn = top_bn\n",
    "        if top_bn:\n",
    "            self.bn = nn.BatchNorm1d(10, eps=2e-5)\n",
    "\n",
    "    def forward(self, x, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn1, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn2, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn3, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.mp1(h)\n",
    "        if self.dropout:\n",
    "            h = self.drop1(h)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn4, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn5, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn6, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.mp2(h)\n",
    "        if self.dropout:\n",
    "            h = self.drop2(h)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn7, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn8, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = nfunc.leaky_relu(call_bn(self.bn9, h, update_batch_stats=update_batch_stats), negative_slope=0.1)\n",
    "        h = self.aap(h)\n",
    "        output = self.linear(h.view(-1, 128))\n",
    "        if self.top_bn:\n",
    "            output = call_bn(self.bnf, output, update_batch_stats=update_batch_stats)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-562.9708) 2.2606215476989746\n",
      "tensor(-239.5726) 2.7371633052825928\n",
      "tensor(2.0767) 2.6680123805999756\n",
      "tensor(126.5036) 2.248326063156128\n",
      "tensor(31.4626) 2.090791940689087\n",
      "tensor(-653.7198) 2.224613666534424\n",
      "tensor(-206.5633) 2.0617425441741943\n",
      "tensor(-199.0407) 2.3420562744140625\n",
      "tensor(173.3533) 2.3304195404052734\n",
      "tensor(-129.2892) 2.2965545654296875\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "train_iter = iter(DataLoader(train_set, batch_size, num_workers=0, sampler=InfiniteSampler(len(train_set))))\n",
    "\n",
    "cls = CNN(dropout=False)\n",
    "set_framework_seed(1)\n",
    "cls.apply(weights_init)\n",
    "cls = cls.to(args.device)\n",
    "optimizer = Adam(list(cls.parameters()), lr=args.lr)\n",
    "\n",
    "cls.train()\n",
    "set_framework_seed(1)\n",
    "for it in range(10):\n",
    "    # batch norm need a fix seed\n",
    "    x, y = next(train_iter)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logit = cls(x.to(args.device))\n",
    "    loss = criterion(logit, y.to(device))\n",
    "    print(x.sum(), loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8990, 10.803810766601563)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(cls, test_loader, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-562.9708) 2.4522294998168945\n",
      "tensor(-239.5726) 2.507972478866577\n",
      "tensor(2.0767) 2.7254555225372314\n",
      "tensor(126.5036) 2.5243442058563232\n",
      "tensor(31.4626) 2.5457208156585693\n",
      "tensor(-653.7198) 2.2350082397460938\n",
      "tensor(-206.5633) 2.480289936065674\n",
      "tensor(-199.0407) 2.640108823776245\n",
      "tensor(173.3533) 2.641848087310791\n",
      "tensor(-129.2892) 2.6940395832061768\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "train_iter = iter(DataLoader(train_set, batch_size, num_workers=0, sampler=InfiniteSampler(len(train_set))))\n",
    "\n",
    "cls = CNN(dropout=True)\n",
    "set_framework_seed(1)\n",
    "cls.apply(weights_init)\n",
    "cls = cls.to(args.device)\n",
    "optimizer = Adam(list(cls.parameters()), lr=args.lr)\n",
    "\n",
    "cls.train()\n",
    "set_framework_seed(1)\n",
    "for it in range(10):\n",
    "    # batch norm need a fix seed\n",
    "    x, y = next(train_iter)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logit = cls(x.to(args.device))\n",
    "    loss = criterion(logit, y.to(device))\n",
    "    print(x.sum(), loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8973, 19.94179396057129)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(cls, test_loader, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
