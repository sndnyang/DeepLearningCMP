{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as t_func\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle as pickle\n",
    "\n",
    "from ExpUtils import *\n",
    "from theano_func.CostFunc import get_cost_type_semi\n",
    "from theano_func.source import optimizers\n",
    "from theano_func.source import costs\n",
    "from theano_func.models.fnn_mnist_semisup import FNN_MNIST\n",
    "from collections import OrderedDict\n",
    "import load_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 23:00:42,961 - <ipython-input-2-7791375fbe33>[line:97]: [Epoch] 0\n",
      "2019-03-08 23:00:43,076 - <ipython-input-2-7791375fbe33>[line:101]: nll_train: 0.113660 error_train : 0 nll_test : 0.988779 error_test : 2475 accuracy:0.752500\n",
      "2019-03-08 23:01:14,481 - <ipython-input-2-7791375fbe33>[line:97]: [Epoch] 1\n",
      "2019-03-08 23:01:14,482 - <ipython-input-2-7791375fbe33>[line:101]: nll_train: 0.059409 error_train : 0 nll_test : 0.908701 error_test : 2475 accuracy:0.752500\n"
     ]
    }
   ],
   "source": [
    "def train(args):\n",
    "\n",
    "    numpy.random.seed(int(args['seed']))\n",
    "\n",
    "    dataset = load_data.load_mnist_for_semi_sup(n_l=int(args['size']),\n",
    "                                                n_v=int(args['num_validation_samples']))\n",
    "\n",
    "    x_train, t_train, ul_x_train = dataset[0]\n",
    "    x_test, t_test = dataset[2]\n",
    "\n",
    "    numpy.random.seed(int(args['seed']))\n",
    "    layer_sizes = [int(layer_size) for layer_size in args['layer_sizes'].split('-')]\n",
    "    model = FNN_MNIST(layer_sizes=layer_sizes)\n",
    "\n",
    "    x = t_func.matrix()\n",
    "    ul_x = t_func.matrix()\n",
    "    t = t_func.ivector()\n",
    "\n",
    "    cost_semi = get_cost_type_semi(model, x, t, ul_x, args)\n",
    "    nll = costs.cross_entropy_loss(x=x, t=t, forward_func=model.forward_test)\n",
    "    error = costs.error(x=x, t=t, forward_func=model.forward_test)\n",
    "\n",
    "    optimizer = optimizers.ADAM(cost=cost_semi, params=model.params, alpha=float(args['lr']))\n",
    "\n",
    "    index = t_func.iscalar()\n",
    "    ul_index = t_func.iscalar()\n",
    "    batch_size = int(args['batch_size'])\n",
    "    ul_batch_size = int(args['ul_batch_size'])\n",
    "\n",
    "    f_train = theano.function(inputs=[index, ul_index], outputs=cost_semi, updates=optimizer.updates,\n",
    "                              givens={\n",
    "                                  x: x_train[batch_size * index:batch_size * (index + 1)],\n",
    "                                  t: t_train[batch_size * index:batch_size * (index + 1)],\n",
    "                                  ul_x: ul_x_train[ul_batch_size * ul_index:ul_batch_size * (ul_index + 1)]},\n",
    "                              on_unused_input='ignore')\n",
    "    f_nll_train = theano.function(inputs=[index], outputs=nll,\n",
    "                                  givens={\n",
    "                                      x: x_train[batch_size * index:batch_size * (index + 1)],\n",
    "                                      t: t_train[batch_size * index:batch_size * (index + 1)]})\n",
    "    f_nll_test = theano.function(inputs=[index], outputs=nll,\n",
    "                                 givens={\n",
    "                                     x: x_test[batch_size * index:batch_size * (index + 1)],\n",
    "                                     t: t_test[batch_size * index:batch_size * (index + 1)]})\n",
    "\n",
    "    f_error_train = theano.function(inputs=[index], outputs=error,\n",
    "                                    givens={\n",
    "                                        x: x_train[batch_size * index:batch_size * (index + 1)],\n",
    "                                        t: t_train[batch_size * index:batch_size * (index + 1)]})\n",
    "    f_error_test = theano.function(inputs=[index], outputs=error,\n",
    "                                   givens={\n",
    "                                       x: x_test[batch_size * index:batch_size * (index + 1)],\n",
    "                                       t: t_test[batch_size * index:batch_size * (index + 1)]})\n",
    "\n",
    "    f_lr_decay = theano.function(inputs=[], outputs=optimizer.alpha,\n",
    "                                 updates={optimizer.alpha: theano.shared(\n",
    "                                     numpy.array(args['lr_decay']).astype(\n",
    "                                         theano.config.floatX)) * optimizer.alpha})\n",
    "\n",
    "    # Shuffle training set\n",
    "    randix = RandomStreams(seed=numpy.random.randint(1234)).permutation(n=x_train.shape[0])\n",
    "    update_permutation = OrderedDict()\n",
    "    update_permutation[x_train] = x_train[randix]\n",
    "    update_permutation[t_train] = t_train[randix]\n",
    "    f_permute_train_set = theano.function(inputs=[], outputs=x_train, updates=update_permutation)\n",
    "\n",
    "    # Shuffle unlabeled training set\n",
    "    ul_randix = RandomStreams(seed=numpy.random.randint(1234)).permutation(n=ul_x_train.shape[0])\n",
    "    update_ul_permutation = OrderedDict()\n",
    "    update_ul_permutation[ul_x_train] = ul_x_train[ul_randix]\n",
    "    f_permute_ul_train_set = theano.function(inputs=[], outputs=ul_x_train, updates=update_ul_permutation)\n",
    "\n",
    "    statuses = {'nll_train': [], 'error_train': [], 'nll_test': [], 'error_test': []}\n",
    "\n",
    "    n_train = x_train.get_value().shape[0]\n",
    "    n_test = x_test.get_value().shape[0]\n",
    "    n_ul_train = ul_x_train.get_value().shape[0]\n",
    "\n",
    "    l_i = 0\n",
    "    ul_i = 0\n",
    "    for epoch in range(int(args['epochs'])):\n",
    "\n",
    "        f_permute_train_set()\n",
    "        f_permute_ul_train_set()\n",
    "        for it in range(int(args['iterations'])):\n",
    "            f_train(l_i, ul_i)\n",
    "            l_i = 0 if l_i >= n_train / batch_size - 1 else l_i + 1\n",
    "            ul_i = 0 if ul_i >= n_ul_train / ul_batch_size - 1 else ul_i + 1\n",
    "\n",
    "        sum_nll_train = numpy.sum(numpy.array([f_nll_train(i) for i in range(n_train // batch_size)])) * batch_size\n",
    "        sum_error_train = numpy.sum(numpy.array([f_error_train(i) for i in range(n_train // batch_size)]))\n",
    "        sum_nll_test = numpy.sum(numpy.array([f_nll_test(i) for i in range(n_test // batch_size)])) * batch_size\n",
    "        sum_error_test = numpy.sum(numpy.array([f_error_test(i) for i in range(n_test // batch_size)]))\n",
    "        statuses['nll_train'].append(sum_nll_train / n_train)\n",
    "        statuses['error_train'].append(sum_error_train)\n",
    "        statuses['nll_test'].append(sum_nll_test / n_test)\n",
    "        statuses['error_test'].append(sum_error_test)\n",
    "        wlog(\"[Epoch] %d\" % epoch)\n",
    "        acc = 1 - 1.0*statuses['error_test'][-1]/n_test\n",
    "        wlog(\"nll_train: %f error_train : %d nll_test : %f error_test : %d accuracy:%f\" % (\n",
    "            statuses['nll_train'][-1], statuses['error_train'][-1], statuses['nll_test'][-1],\n",
    "            statuses['error_test'][-1], acc))\n",
    "        if args[\"vis\"]:\n",
    "            saver.writer.add_scalar(\"Train/Loss\", statuses['nll_train'][-1], epoch)\n",
    "            saver.writer.add_scalar(\"Train/Error\", statuses['error_train'][-1], epoch)\n",
    "            saver.writer.add_scalar(\"Test/Loss\", statuses['nll_test'][-1], epoch)\n",
    "            saver.writer.add_scalar(\"Test/Acc\", acc, epoch)\n",
    "        f_lr_decay()\n",
    "    # fine_tune batch stat\n",
    "    f_fine_tune = theano.function(inputs=[ul_index], outputs=model.forward_for_finetuning_batch_stat(x),\n",
    "                                  givens={x: ul_x_train[ul_batch_size * ul_index:ul_batch_size * (ul_index + 1)]})\n",
    "    [f_fine_tune(i) for i in range(n_ul_train // ul_batch_size)]\n",
    "\n",
    "    sum_nll_train = numpy.sum(numpy.array([f_nll_train(i) for i in range(n_train // batch_size)])) * batch_size\n",
    "    sum_error_train = numpy.sum(numpy.array([f_error_train(i) for i in range(n_train // batch_size)]))\n",
    "    sum_nll_test = numpy.sum(numpy.array([f_nll_test(i) for i in range(n_test // batch_size)])) * batch_size\n",
    "    sum_error_test = numpy.sum(numpy.array([f_error_test(i) for i in range(n_test // batch_size)]))\n",
    "    statuses['nll_train'].append(sum_nll_train / n_train)\n",
    "    statuses['error_train'].append(sum_error_train)\n",
    "    statuses['nll_test'].append(sum_nll_test / n_test)\n",
    "    statuses['error_test'].append(sum_error_test)\n",
    "    wlog(\"final nll_train: %f error_train: %d nll_test: %f error_test: %d accuracy:%f\" % (\n",
    "        statuses['nll_train'][-1], statuses['error_train'][-1], statuses['nll_test'][-1],\n",
    "        statuses['error_test'][-1], 1 - 1.0*statuses['error_test'][-1]/n_test))\n",
    "    if args[\"vis\"]:\n",
    "        saver.writer.add_scalar(\"Train/Loss\", statuses['nll_train'][-1], epoch)\n",
    "        saver.writer.add_scalar(\"Train/Error\", statuses['error_train'][-1], epoch)\n",
    "        saver.writer.add_scalar(\"Test/Loss\", statuses['nll_test'][-1], epoch)\n",
    "        saver.writer.add_scalar(\"Test/Acc\", acc, epoch)\n",
    "\n",
    "    error_test = numpy.array(statuses['error_test'])\n",
    "    saver.save_npy(numpy.array(statuses['nll_train']), \"train_nll\")\n",
    "    saver.save_npy(numpy.array(statuses['error_train']), \"train_error\")\n",
    "    saver.save_npy(numpy.array(statuses['nll_test']), \"test_nll\")\n",
    "    saver.save_npy(error_test, \"test_error\")\n",
    "    saver.save_npy(1 - 1.0 * error_test / n_test, \"test_acc\")\n",
    "\n",
    "\n",
    "arg = {'seed': 1,\n",
    "       'size': 100,\n",
    "       'vis': False,\n",
    "       'epochs': 100,\n",
    "       'iterations': 400,\n",
    "       'lr_decay': 0.95,\n",
    "       'batch_size': 100,\n",
    "       'ul_batch_size': 250,\n",
    "       'num_validation_samples': 1000,\n",
    "       'layer_sizes': '784-1200-1200-10',\n",
    "       'dataset': 'mnist',\n",
    "       'epsilon': 0.3,\n",
    "       'cost_type': 'MLE',\n",
    "       'norm_constraint': 'L2',\n",
    "       'num_power_iter': 1,\n",
    "       'lamb': 1,\n",
    "       'lr': 0.001\n",
    "      }\n",
    "train(arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
