{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data by tensorflow\n",
    "\n",
    "import torch and tensorflow\n",
    "\n",
    "set memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "gpu = \"5\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "tf.enable_eager_execution(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tf_func import data_loader\n",
    "from tf_func import mnist_model\n",
    "from torch_func.load_dataset import load_dataset\n",
    "\n",
    "class ConfigDict(object):\n",
    "    \"\"\"MNIST configration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # List of tuples specify (kernel_size, number of filters) for each layer.\n",
    "        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\n",
    "        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\n",
    "        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\n",
    "        self.num_units_fc_layers = [512]\n",
    "        self.dropout_rate = 0\n",
    "        self.batch_norm = True\n",
    "        self.activation = None\n",
    "        self.regularizer = None\n",
    "        \n",
    "        \n",
    "config = ConfigDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    initialize normal distribution weight matrix\n",
    "    and set bias to 0\n",
    "    :param m:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_name = m.__class__.__name__\n",
    "    fan_in = 0\n",
    "    if class_name.find('Conv') != -1:\n",
    "        shape = m.weight.data.shape\n",
    "        fan_in = shape[1] * shape[2] * shape[3]\n",
    "    if class_name.find('Linear') != -1:\n",
    "        shape = m.weight.data.shape\n",
    "        fan_in = shape[1]\n",
    "    if fan_in:\n",
    "        s = 1.0 * np.sqrt(6.0 / fan_in)\n",
    "        transpose = np.random.uniform(-s, s, m.weight.data.shape).astype(\"float32\")\n",
    "        if debug:\n",
    "            print(shape, transpose.sum())\n",
    "        tensor = torch.from_numpy(transpose)\n",
    "        m.weight = Parameter(tensor, requires_grad=True)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "            \n",
    "def set_framework_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_iter, device):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_iter:\n",
    "            size += images.numpy().shape[0]\n",
    "            images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "            labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "            logits = model(images)\n",
    "            total_loss += criterion(logits, labels).item() * images.shape[0]\n",
    "            pred_y = torch.max(logits, dim=1)[1]        \n",
    "            total_acc += (pred_y == labels).sum().item()\n",
    "    model.train()\n",
    "    return total_acc / size, total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, update_batch_stats=True):\n",
    "    if bn.training is False:\n",
    "        return bn(x)\n",
    "    elif not update_batch_stats:\n",
    "        return nfunc.batch_norm(x, None, None, bn.weight, bn.bias, True, bn.momentum, bn.eps)\n",
    "    else:\n",
    "        return bn(x)\n",
    "    \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, affine=False, top_bn=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_len = 1 * 28 * 28\n",
    "        self.fc1 = nn.Linear(self.input_len, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1200, affine=affine)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1200, affine=affine)\n",
    "        self.top_bn = top_bn\n",
    "        if top_bn:\n",
    "            self.bn_fc3 = nn.BatchNorm1d(10, affine=affine)\n",
    "\n",
    "    def forward(self, x, update_batch_stats=True, return_h=False):\n",
    "        endpoints = {}\n",
    "        h = nfunc.relu(call_bn(self.bn_fc1, self.fc1(x.view(-1, self.input_len)), update_batch_stats))\n",
    "        endpoints[\"fc_layer0\"] = h\n",
    "        h = nfunc.relu(call_bn(self.bn_fc2, self.fc2(h), update_batch_stats))\n",
    "        endpoints[\"fc_layer1\"] = h\n",
    "        if self.top_bn:\n",
    "            h = call_bn(self.bn_fc3, self.fc3(h), update_batch_stats)\n",
    "        else:\n",
    "            h = self.fc3(h)\n",
    "        logits = h\n",
    "        if return_h:\n",
    "            return logits, endpoints\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_func.vat import VAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_labeled:100, N_train_unlabeled:59000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "set_framework_seed(1)\n",
    "device = torch.device(\"cpu\")\n",
    "train_l, train_ul, test_set = load_dataset(\"mnist\", valid=True, dataset_seed=1, size=100)\n",
    "print(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "\n",
    "test_set = TensorDataset(torch.FloatTensor(test_set.data), torch.LongTensor(test_set.label))\n",
    "test_loader = DataLoader(test_set, 128, False)\n",
    "\n",
    "# Define losses.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "debug = True\n",
    "\n",
    "layer_sizes = [784, 1200, 1200, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 784]) -13.070702\n",
      "torch.Size([1200, 1200]) -44.62378\n",
      "torch.Size([10, 1200]) -3.5299373\n",
      "x 10491.24609375\n",
      "ul x 25582.734375\n",
      "sup 2.523423433303833\n",
      "unsup tensor(0.1865, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 26232.44140625\n",
      "sup 0.5586448907852173\n",
      "unsup tensor(0.0317, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25147.70703125\n",
      "sup 0.45668429136276245\n",
      "unsup tensor(0.0203, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25282.7109375\n",
      "sup 0.41053539514541626\n",
      "unsup tensor(0.0171, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25813.4609375\n",
      "sup 0.3806508183479309\n",
      "unsup tensor(0.0167, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 24906.30859375\n",
      "sup 0.3595522344112396\n",
      "unsup tensor(0.0178, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25616.22265625\n",
      "sup 0.3443249762058258\n",
      "unsup tensor(0.0168, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25217.87109375\n",
      "sup 0.3329668343067169\n",
      "unsup tensor(0.0181, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25456.51953125\n",
      "sup 0.3242984712123871\n",
      "unsup tensor(0.0175, grad_fn=<MeanBackward1>)\n",
      "x 10491.24609375\n",
      "ul x 25974.90625\n",
      "sup 0.31758564710617065\n",
      "unsup tensor(0.0182, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(layer_sizes=layer_sizes, affine=False, top_bn=True)\n",
    "debug = True\n",
    "set_framework_seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(list(model.parameters()), lr=0.002)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1,\n",
    "                                      gamma=0.95)\n",
    "\n",
    "n_train = train_l.label.shape[0]\n",
    "n_ul_train = train_ul.label.shape[0]\n",
    "batch_size_l = 100\n",
    "batch_size_ul = 250\n",
    "eps = 0.3\n",
    "xi = 0.000001\n",
    "debug = False\n",
    "for epoch in range(10):\n",
    "    l_i, ul_i = 0, 0\n",
    "\n",
    "    rand_ind = np.random.permutation(train_l.label.shape[0])\n",
    "    train_images = train_l.data[rand_ind]\n",
    "    train_labels = train_l.label[rand_ind]\n",
    "    rand_ind = np.random.permutation(train_ul.data.shape[0])\n",
    "    train_ul_images = train_ul.data[rand_ind]\n",
    "    for i in range(1):\n",
    "\n",
    "        images = torch.FloatTensor(train_images[batch_size_l*l_i:batch_size_l*(l_i + 1)])\n",
    "        labels = torch.LongTensor(train_labels[batch_size_l*l_i:batch_size_l*(l_i + 1)])\n",
    "        print(\"x\", images.sum().item())\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_ind = np.random.choice(np.arange(train_ul_images.shape[0]), batch_size_ul)\n",
    "        ul_images = torch.FloatTensor(train_ul_images[batch_ind])\n",
    "        print(\"ul x\", ul_images.sum().item())\n",
    "        # ul_images = torch.FloatTensor(train_ul_images[batch_size_ul*l_i:batch_size_ul*(l_i + 1)])\n",
    "        ul_images = ul_images.to(device)\n",
    "\n",
    "        l_i = 0 if l_i >= n_train / batch_size_l - 1 else l_i + 1\n",
    "        ul_i = 0 if ul_i >= n_ul_train / batch_size_ul - 1 else ul_i + 1\n",
    "\n",
    "        logits = model(images)\n",
    "\n",
    "        total_loss = 0\n",
    "        sup_loss = 0\n",
    "\n",
    "        # supervised loss\n",
    "        xent_loss = criterion(logits, labels)\n",
    "        print(\"sup\", xent_loss.item())\n",
    "        sup_loss += xent_loss\n",
    "\n",
    "        vat_criterion = VAT(device, eps=eps, xi=xi, use_entmin=False, debug=debug)\n",
    "        unsup_loss = vat_criterion(model, ul_images)\n",
    "        print(\"unsup\", unsup_loss)\n",
    "        total_loss += sup_loss + unsup_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "It works fine for Chainer and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n",
      "logits 1279.9627685546875\n",
      "iter 0,  train loss 2.63721 vat loss 0.00843\n",
      "\n",
      "data 12953.98438\n",
      "logits 1279.959716796875\n",
      "iter 1,  train loss 2.44700 vat loss 0.00843\n",
      "\n",
      "data 12442.99316\n",
      "logits 1279.9593505859375\n",
      "iter 2,  train loss 2.37316 vat loss 0.00843\n",
      "\n",
      "data 12292.56152\n",
      "logits 1279.9571533203125\n",
      "iter 3,  train loss 2.68267 vat loss 0.00843\n",
      "\n",
      "data 12402.40137\n",
      "logits 1279.9608154296875\n",
      "iter 4,  train loss 2.61225 vat loss 0.00843\n",
      "\n",
      "test acc 0.11058,  loss 2.31515\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = True\n",
    "debug = False\n",
    "affine = False\n",
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 3\n",
    "lr = 0.002\n",
    "\n",
    "torch_optimizer = optim.Adam(list(model.parameters()), lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = iterator.get_next()\n",
    "    print(\"data %.5f\" % x.numpy().sum())\n",
    "    images = torch.FloatTensor(x.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(y.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits = model(images)\n",
    "    print(\"logits\", (logits ** 2).sum().item())\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "\n",
    "    # vat_criterion = VAT(device, 0.3, 1e-6, use_entmin=False)\n",
    "    # vat_loss = vat_criterion(model, images)\n",
    "    \n",
    "    torch_optimizer.zero_grad()\n",
    "#     total_loss.backward()\n",
    "#     torch_optimizer.step()\n",
    "#     torch_optimizer.zero_grad()\n",
    "    print(\"iter %d,  train loss %.5f vat loss %.5f\\n\" % (i, total_loss.item(), vat_loss.item()))\n",
    "    if (i+1) % 10 == 0:\n",
    "        debug = False\n",
    "        \n",
    "        acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "        print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n",
      "logits tensor(-3.8147e-06)\n",
      "iter 0,  train loss 2.63721 vat loss 0.39495\n",
      "\n",
      "data 12953.98438\n",
      "logits tensor(201.0817)\n",
      "iter 1,  train loss 1.51227 vat loss 0.02097\n",
      "\n",
      "data 12442.99316\n",
      "logits tensor(324.2178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyang2/software/dllib3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2,  train loss 0.91790 vat loss 0.02568\n",
      "\n",
      "data 12292.56152\n",
      "logits tensor(304.0911)\n",
      "iter 3,  train loss 0.88271 vat loss 0.03013\n",
      "\n",
      "data 12402.40137\n",
      "logits tensor(286.4545)\n",
      "iter 4,  train loss 0.95338 vat loss 0.04151\n",
      "\n",
      "test acc 0.77704,  loss 0.68860\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = True\n",
    "debug = False\n",
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 3\n",
    "lr = 0.002\n",
    "\n",
    "torch_optimizer = optim.Adam(list(model.parameters()), lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = iterator.get_next()\n",
    "    print(\"data %.5f\" % x.numpy().sum())\n",
    "    images = torch.FloatTensor(x.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(y.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits = model(images)\n",
    "    print(\"logits\", logits.data.sum())\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "\n",
    "    vat_criterion = VAT(device, 0.3, 1e-6, use_entmin=False)\n",
    "    vat_loss = vat_criterion(model, images)\n",
    "    \n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    print(\"iter %d,  train loss %.5f vat loss %.5f\\n\" % (i, total_loss.item(), vat_loss.item()))\n",
    "    if (i+1) % 10 == 0:\n",
    "        debug = False\n",
    "        \n",
    "        acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "        print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, update_batch_stats=True):\n",
    "    if bn.training is False:\n",
    "        return bn(x)\n",
    "    elif not update_batch_stats:\n",
    "        return nfunc.batch_norm(x, None, None, bn.weight, bn.bias, True, bn.momentum, bn.eps)\n",
    "    else:\n",
    "        return bn(x)\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_len = 1 * 28 * 28\n",
    "        self.fc1 = nn.Linear(self.input_len, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(10, eps=2e-5, affine=False)\n",
    "\n",
    "    def forward(self, x, update_batch_stats=True):\n",
    "\n",
    "        x = x.view(-1, self.input_len)\n",
    "        endpoints = {}\n",
    "        h = nfunc.relu(call_bn(self.bn_fc1, self.fc1(x), update_batch_stats))\n",
    "        endpoints[\"fc_layer0\"] = h\n",
    "        h = nfunc.relu(call_bn(self.bn_fc2, self.fc2(h), update_batch_stats))\n",
    "        endpoints[\"fc_layer1\"] = h\n",
    "        h = call_bn(self.bn_fc3, self.fc3(h), update_batch_stats)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12442.9932)\n",
      "tensor(1.1444e-05, grad_fn=<SumBackward0>)\n",
      "tensor(1.1444e-05, grad_fn=<SumBackward0>)\n",
      "tensor(68.5641, grad_fn=<SumBackward0>)\n",
      "tensor(68.5641, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "print(images.sum())\n",
    "model.train()\n",
    "p = model(images, update_batch_stats=True)\n",
    "print(p.sum())\n",
    "p = model(images, update_batch_stats=False)\n",
    "print(p.sum())\n",
    "model.eval()\n",
    "p = model(images, update_batch_stats=True)\n",
    "print(p.sum())\n",
    "model.eval()\n",
    "p = model(images, update_batch_stats=False)\n",
    "print(p.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n",
      "d tensor(3101.4385) tensor(128.)\n",
      "input 12790.1474609375\n",
      "output 60.19732666015625\n",
      "loss 0.2745497226715088\n",
      "grad d tensor(0.1649)\n",
      "d tensor(51.4371)\n",
      "post cost 0.3949525058269501\n",
      "iter 0,  train loss 2.63721 vat loss 0.39495\n",
      "\n",
      "data 12953.98438\n",
      "d tensor(3108.1763) tensor(128.)\n",
      "input 12953.9853515625\n",
      "output 79.67488098144531\n",
      "loss 3.4834282125473237e-09\n",
      "grad d tensor(7.3648e-09)\n",
      "d tensor(15.5153)\n",
      "post cost 0.008067242801189423\n",
      "iter 1,  train loss 2.27209 vat loss 0.00807\n",
      "\n",
      "data 12442.99316\n",
      "d tensor(3103.8330) tensor(128.)\n",
      "input 12442.994140625\n",
      "output 70.6865234375\n",
      "loss 4.737283454403496e-09\n",
      "grad d tensor(1.4721e-09)\n",
      "d tensor(8.9609)\n",
      "post cost 0.008078992366790771\n",
      "iter 2,  train loss 2.28871 vat loss 0.00808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyang2/software/dllib3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc 0.12029,  loss 2.31698\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = True\n",
    "debug = False\n",
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 3\n",
    "lr = 0.002\n",
    "\n",
    "torch_optimizer = optim.Adam(list(model.parameters()), lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "debug = True\n",
    "\n",
    "class VAT(object):\n",
    "\n",
    "    def __init__(self, device, eps, xi, k=1, use_entmin=False):\n",
    "        self.device = device\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.k = k\n",
    "        self.kl_div = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.use_entmin = use_entmin\n",
    "\n",
    "    def __call__(self, model, X):\n",
    "        logits = model(X, update_batch_stats=False)\n",
    "        # logits = model(X)\n",
    "        prob_logits = nfunc.softmax(logits.detach(), dim=1)\n",
    "        d = VAT.approx_power_iter(model, X, prob_logits, self.xi, self.k, self.device)\n",
    "        \n",
    "        \n",
    "        logits_hat = model(X + self.eps * d, update_batch_stats=False)\n",
    "        LDS = torch.mean(self.kl_div(\n",
    "            nfunc.log_softmax(logits_hat, dim=1), prob_logits).sum(dim=1))\n",
    "        if debug:\n",
    "            print(\"post cost\", LDS.item())\n",
    "        if self.use_entmin:\n",
    "            LDS += _entropy(logits_hat)\n",
    "\n",
    "        return LDS\n",
    "\n",
    "    @staticmethod\n",
    "    def approx_power_iter(model, X, prob_logits, xi, k, device):\n",
    "        kl_div = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        r = torch.FloatTensor(np.random.random(X.shape))\n",
    "        d = _l2_normalize(r).to(device)\n",
    "        if debug:\n",
    "            print(\"d\", d.sum(), (d ** 2).sum())\n",
    "            \n",
    "        for ip in range(k):\n",
    "            X_hat = X + d * xi\n",
    "            if debug:\n",
    "                print(\"input\", X_hat.data.sum().item())\n",
    "            X_hat.requires_grad = True\n",
    "            model.eval()\n",
    "            logits_hat = model(X_hat, update_batch_stats=False)\n",
    "            # logits_hat = model(X_hat)\n",
    "            if debug:\n",
    "                print(\"output\", logits_hat.data.sum().item())\n",
    "            adv_distance = torch.mean(kl_div(nfunc.log_softmax(logits_hat, dim=1), prob_logits).sum(dim=1))\n",
    "            if debug:\n",
    "                print(\"loss\", adv_distance.item())\n",
    "            adv_distance.backward()\n",
    "            if debug:\n",
    "                print(\"grad d\", X_hat.grad.sum())\n",
    "            d = _l2_normalize(X_hat.grad).to(device)\n",
    "            if debug:\n",
    "                print(\"d\", d.sum())\n",
    "        return d\n",
    "    \n",
    "for i in range(3):\n",
    "    x, y = iterator.get_next()\n",
    "    print(\"data %.5f\" % x.numpy().sum())\n",
    "    images = torch.FloatTensor(x.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(y.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits = model(images)\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "\n",
    "    vat_criterion = VAT(device, 0.3, 1e-6, use_entmin=False)\n",
    "    vat_loss = vat_criterion(model, images)\n",
    "    \n",
    "#     torch_optimizer.zero_grad()\n",
    "#     total_loss.backward()\n",
    "#     torch_optimizer.step()\n",
    "#     torch_optimizer.zero_grad()\n",
    "    print(\"iter %d,  train loss %.5f vat loss %.5f\\n\" % (i, total_loss.item(), vat_loss.item()))\n",
    "    if (i+1) % 10 == 0:\n",
    "        debug = False\n",
    "        \n",
    "        acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "        print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _l2_normalize(d):\n",
    "    d = d.cpu().numpy()\n",
    "    axis = tuple(range(1, len(d.shape)))\n",
    "    reshape = tuple([-1] + [1] * (len(d.shape) -1))\n",
    "    d /= (np.sqrt(np.sum(d ** 2, axis=axis)).reshape(\n",
    "        reshape) + 1e-16)\n",
    "    return torch.from_numpy(d)\n",
    "\n",
    "\n",
    "def _entropy(logits):\n",
    "    p = nfunc.softmax(logits, dim=1)\n",
    "    return -torch.mean(torch.sum(p * F.log_softmax(logits, dim=1), dim=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n",
      "iter 0,  train loss 2.63721 vat loss 0.17757\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 2.44700 vat loss 0.19656\n",
      "\n",
      "data 12442.99316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyang2/software/dllib3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2,  train loss 2.37316 vat loss 0.19594\n",
      "\n",
      "data 12292.56152\n",
      "iter 3,  train loss 2.68267 vat loss 0.20597\n",
      "\n",
      "data 12402.40137\n",
      "iter 4,  train loss 2.61225 vat loss 0.18760\n",
      "\n",
      "test acc 0.12340,  loss 2.41112\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = True\n",
    "debug = False\n",
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 3\n",
    "lr = 0.002\n",
    "\n",
    "torch_optimizer = optim.Adam(list(model.parameters()), lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = iterator.get_next()\n",
    "    print(\"data %.5f\" % x.numpy().sum())\n",
    "    images = torch.FloatTensor(x.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(y.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits = model(images)\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "\n",
    "    vat_criterion = VAT(device, 0.3, 1e-6, use_entmin=False)\n",
    "    vat_loss = vat_criterion(model, images)\n",
    "    \n",
    "#     torch_optimizer.zero_grad()\n",
    "#     total_loss.backward()\n",
    "#     torch_optimizer.step()\n",
    "#     torch_optimizer.zero_grad()\n",
    "    print(\"iter %d,  train loss %.5f vat loss %.5f\\n\" % (i, total_loss.item(), vat_loss.item()))\n",
    "    if (i+1) % 10 == 0:\n",
    "        debug = False\n",
    "        \n",
    "        acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "        print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "data 12790.14453\n",
      "iter 0,  train loss 2.38092\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 1.61463\n",
      "\n",
      "data 12442.99316\n",
      "iter 2,  train loss 1.55225\n",
      "\n",
      "test acc 0.45202,  loss 1.90216\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = True\n",
    "debug = False\n",
    "model = MLP(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 3\n",
    "lr = 0.002\n",
    "\n",
    "torch_optimizer = optim.Adam(list(model.parameters()), lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits = model(images)\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "    \n",
    "    print(\"iter %d,  train loss %.5f\\n\" % (i, total_loss))\n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
