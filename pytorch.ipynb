{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data by tensorflow\n",
    "\n",
    "import torch and tensorflow\n",
    "\n",
    "set memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "gpu = \"\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "tf.enable_eager_execution(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tf_func import data_loader\n",
    "from tf_func import mnist_model\n",
    "\n",
    "\n",
    "class ConfigDict(object):\n",
    "    \"\"\"MNIST configration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # List of tuples specify (kernel_size, number of filters) for each layer.\n",
    "        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\n",
    "        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\n",
    "        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\n",
    "        self.num_units_fc_layers = [512]\n",
    "        self.dropout_rate = 0\n",
    "        self.batch_norm = True\n",
    "        self.activation = None\n",
    "        self.regularizer = None\n",
    "        \n",
    "        \n",
    "config = ConfigDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"train\",\n",
    "    batch_size=128,\n",
    "    is_training=False)\n",
    "\n",
    "test_dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"test\",\n",
    "    batch_size=128,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12790.145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels, num_examples, num_classes = (dataset.images, dataset.labels, dataset.num_examples, dataset.num_classes)\n",
    "images, labels = dataset.get_next()\n",
    "images.numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    initialize kaimin uniform distribution weight matrix\n",
    "    and set bias to 0\n",
    "    :param m:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_name = m.__class__.__name__\n",
    "    fan_in = 0\n",
    "    if class_name.find('Conv') != -1:\n",
    "        shape = m.weight.data.shape\n",
    "        fan_in = shape[1] * shape[2] * shape[3]\n",
    "    if class_name.find('Linear') != -1:\n",
    "        shape = m.weight.data.shape\n",
    "        fan_in = shape[1]\n",
    "    if fan_in:\n",
    "        s = 1.0 * np.sqrt(6.0 / fan_in)\n",
    "        transpose = np.random.uniform(-s, s, m.weight.data.shape).astype(\"float32\")\n",
    "        if debug:\n",
    "            print(shape, transpose.sum())\n",
    "        tensor = torch.from_numpy(transpose)\n",
    "        m.weight = Parameter(tensor, requires_grad=True)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_iter, device):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_iter:\n",
    "            size += images.numpy().shape[0]\n",
    "            images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "            labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "            logits, _ = model(images)\n",
    "            total_loss += criterion(logits, labels).item() * images.shape[0]\n",
    "            pred_y = torch.max(logits, dim=1)[1]        \n",
    "            total_acc += (pred_y == labels).sum().item()\n",
    "    model.train()\n",
    "    return total_acc / size, total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "class CNN3(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.config = config\n",
    "        self.batch_norm = config.batch_norm\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=(1, 1), \n",
    "                               padding=2, bias=not self.batch_norm)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=(1, 1), \n",
    "                               padding=2, bias=not self.batch_norm)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        if self.config.dropout_rate > 0:\n",
    "            self.drop_conv1 = nn.Dropout2d(self.config.dropout_rate)\n",
    "            self.drop_conv2 = nn.Dropout2d(self.config.dropout_rate)\n",
    "            self.drop_fc1 = nn.Dropout(0.1)\n",
    "            \n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(32, eps=EPS, momentum=1-MOMENTUM, affine=False)\n",
    "            self.bn2 = nn.BatchNorm2d(64, eps=EPS, momentum=1-MOMENTUM, affine=False)\n",
    "\n",
    "    def forward(self, images):\n",
    "        endpoints = {}\n",
    "        x = images\n",
    "\n",
    "        # Conv Layer 1\n",
    "        x = nfunc.relu(self.conv1(x))\n",
    "        x = nfunc.max_pool2d(x, 2, stride=2)\n",
    "        if debug:\n",
    "            print(\"after pool\", \"%.4f\" % x.sum().item())\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.drop_conv1(x)\n",
    "        if self.batch_norm:\n",
    "            if debug:\n",
    "                print(\"before batech norm %.4f\" % (x ** 2).sum().item())\n",
    "            x = self.bn1(x)\n",
    "            if debug:\n",
    "                print(\"after batech norm %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"conv_layer0\"] = x\n",
    "\n",
    "        # Conv Layer 2\n",
    "        x = nfunc.max_pool2d(nfunc.relu(self.conv2(x)), 2, stride=2)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.drop_conv2(x)\n",
    "        if self.batch_norm:\n",
    "            if debug:\n",
    "                print(\"before batech norm %.4f\" % (x ** 2).sum().item())\n",
    "            x = self.bn2(x)\n",
    "            if debug:\n",
    "                print(\"after batech norm %.4f\" % (x ** 2).sum().item())\n",
    "        if debug:\n",
    "            print(\"After two conv %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"conv_layer1\"] = x\n",
    "        x = x.permute(0, 2, 3, 1).contiguous().view(x.shape[0], -1)\n",
    "\n",
    "        # fully connect layer 1\n",
    "        x = nfunc.relu(self.fc1(x))\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.drop_fc1(x)\n",
    "        if debug:\n",
    "            print(\"logits %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"fc_layer0\"] = x\n",
    "\n",
    "        # fully connect layer logit\n",
    "        x = self.fc2(x)\n",
    "        if debug:\n",
    "            print(\"logits %.4f\" % (x ** 2).sum().item())\n",
    "        endpoints[\"logits\"] = x\n",
    "        return x, endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "It works fine for TF and PyTorch\n",
    "\n",
    "without BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy\n",
      "torch.Size([32, 1, 5, 5]) 4.01563\n",
      "torch.Size([64, 32, 5, 5]) -4.576685\n",
      "torch.Size([512, 3136]) -19.584167\n",
      "torch.Size([10, 512]) 1.3087051\n",
      "data 12790.14453\n",
      "iter 0,  train loss 2.61532\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 2.45566\n",
      "\n",
      "data 12442.99316\n",
      "iter 2,  train loss 2.16393\n",
      "\n",
      "data 12292.56152\n",
      "iter 3,  train loss 1.94132\n",
      "\n",
      "data 12402.40137\n",
      "iter 4,  train loss 1.68689\n",
      "\n",
      "data 13600.25098\n",
      "iter 5,  train loss 1.51943\n",
      "\n",
      "data 13450.91211\n",
      "iter 6,  train loss 1.19969\n",
      "\n",
      "data 12739.25195\n",
      "iter 7,  train loss 1.10128\n",
      "\n",
      "data 11608.33594\n",
      "iter 8,  train loss 1.11012\n",
      "\n",
      "data 14801.60449\n",
      "iter 9,  train loss 0.83975\n",
      "\n",
      "data 15733.70703\n",
      "iter 10,  train loss 0.82863\n",
      "\n",
      "data 12621.97754\n",
      "iter 11,  train loss 0.66821\n",
      "\n",
      "data 12366.39551\n",
      "iter 12,  train loss 0.54461\n",
      "\n",
      "data 12358.09375\n",
      "iter 13,  train loss 0.44116\n",
      "\n",
      "data 12827.04590\n",
      "iter 14,  train loss 0.43031\n",
      "\n",
      "data 14613.22363\n",
      "iter 15,  train loss 0.49216\n",
      "\n",
      "data 13533.24316\n",
      "iter 16,  train loss 0.40719\n",
      "\n",
      "data 13346.56543\n",
      "iter 17,  train loss 0.39415\n",
      "\n",
      "data 12810.09473\n",
      "iter 18,  train loss 0.48474\n",
      "\n",
      "data 13308.19238\n",
      "iter 19,  train loss 0.29287\n",
      "\n",
      "data 13648.76465\n",
      "iter 20,  train loss 0.48887\n",
      "\n",
      "data 12919.14941\n",
      "iter 21,  train loss 0.47165\n",
      "\n",
      "data 13686.84766\n",
      "iter 22,  train loss 0.35224\n",
      "\n",
      "data 12311.84375\n",
      "iter 23,  train loss 0.39082\n",
      "\n",
      "data 12001.96875\n",
      "iter 24,  train loss 0.21566\n",
      "\n",
      "data 11975.05176\n",
      "iter 25,  train loss 0.25613\n",
      "\n",
      "data 12390.50586\n",
      "iter 26,  train loss 0.30682\n",
      "\n",
      "data 12357.15332\n",
      "iter 27,  train loss 0.34740\n",
      "\n",
      "data 14262.87109\n",
      "iter 28,  train loss 0.27277\n",
      "\n",
      "data 12710.52051\n",
      "iter 29,  train loss 0.34510\n",
      "\n",
      "test acc 0.89293,  loss 0.36160\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy\")\n",
    "config.batch_norm = False\n",
    "debug = True\n",
    "model = CNN3(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "max_iters = 30\n",
    "debug = False\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "torch_optimizer = optim.SGD(list(model.parameters()), lr, momentum=momentum, nesterov=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits, endpoints = model(images)\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "    \n",
    "    print(\"iter %d,  train loss %.5f\\n\" % (i, total_loss))\n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "with Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data 12790.14453\n",
      "sum of output**2\n",
      " 49262.445\n",
      "sum of output**2\n",
      " 100341.16\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "print(\"input data %.5f\" % images.numpy().sum())\n",
    "with torch.no_grad():\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    bn_layer = nn.BatchNorm2d(1, eps=EPS, momentum=MOMENTUM).to(device)\n",
    "    p = bn_layer(images)\n",
    "    print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())\n",
    "    \n",
    "    # if want to match tf batch norm, set affine=False\n",
    "    bn_layer = nn.BatchNorm2d(1, eps=EPS, momentum=MOMENTUM, affine=False).to(device)\n",
    "    p = bn_layer(images)\n",
    "    print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy, batch norm\n",
      "data 12790.14453\n",
      "after pool 145856.7188\n",
      "before batech norm 142664.3438\n",
      "after batech norm 802661.1250\n",
      "before batech norm 907971.3125\n",
      "after batech norm 401403.5000\n",
      "After two conv 401403.5000\n",
      "logits 62439.2461\n",
      "logits 2373.4065\n",
      "-203.366943359375\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy, batch norm\")\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "config.batch_norm = True\n",
    "\n",
    "model = CNN3(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "debug = True\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "images, labels = iterator.get_next()\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "# Build model.\n",
    "logits, endpoints = model(images)\n",
    "print(logits.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy with model contains batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy, batch norm\n",
      "torch.Size([32, 1, 5, 5]) 4.01563\n",
      "torch.Size([64, 32, 5, 5]) -4.576685\n",
      "torch.Size([512, 3136]) -19.584167\n",
      "torch.Size([10, 512]) 1.3087051\n",
      "test acc 0.12790,  loss 2.59772\n",
      "data 12790.14453\n",
      "iter 0,  train loss 3.03514\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 2.31430\n",
      "\n",
      "data 12442.99316\n",
      "iter 2,  train loss 1.12174\n",
      "\n",
      "data 12292.56152\n",
      "iter 3,  train loss 0.81504\n",
      "\n",
      "data 12402.40137\n",
      "iter 4,  train loss 0.87731\n",
      "\n",
      "data 13600.25098\n",
      "iter 5,  train loss 0.68717\n",
      "\n",
      "data 13450.91211\n",
      "iter 6,  train loss 0.56344\n",
      "\n",
      "data 12739.25195\n",
      "iter 7,  train loss 0.48809\n",
      "\n",
      "data 11608.33594\n",
      "iter 8,  train loss 0.60272\n",
      "\n",
      "data 14801.60449\n",
      "iter 9,  train loss 0.37551\n",
      "\n",
      "data 15733.70703\n",
      "iter 10,  train loss 0.46466\n",
      "\n",
      "data 12621.97754\n",
      "iter 11,  train loss 0.24003\n",
      "\n",
      "data 12366.39551\n",
      "iter 12,  train loss 0.26288\n",
      "\n",
      "data 12358.09375\n",
      "iter 13,  train loss 0.28607\n",
      "\n",
      "data 12827.04590\n",
      "iter 14,  train loss 0.15964\n",
      "\n",
      "data 14613.22363\n",
      "iter 15,  train loss 0.26158\n",
      "\n",
      "data 13533.24316\n",
      "iter 16,  train loss 0.23480\n",
      "\n",
      "data 13346.56543\n",
      "iter 17,  train loss 0.19895\n",
      "\n",
      "data 12810.09473\n",
      "iter 18,  train loss 0.30735\n",
      "\n",
      "data 13308.19238\n",
      "iter 19,  train loss 0.15193\n",
      "\n",
      "data 13648.76465\n",
      "iter 20,  train loss 0.30180\n",
      "\n",
      "data 12919.14941\n",
      "iter 21,  train loss 0.26112\n",
      "\n",
      "data 13686.84766\n",
      "iter 22,  train loss 0.19350\n",
      "\n",
      "data 12311.84375\n",
      "iter 23,  train loss 0.18223\n",
      "\n",
      "data 12001.96875\n",
      "iter 24,  train loss 0.12950\n",
      "\n",
      "data 11975.05176\n",
      "iter 25,  train loss 0.18375\n",
      "\n",
      "data 12390.50586\n",
      "iter 26,  train loss 0.15724\n",
      "\n",
      "data 12357.15332\n",
      "iter 27,  train loss 0.22774\n",
      "\n",
      "data 14262.87109\n",
      "iter 28,  train loss 0.24606\n",
      "\n",
      "data 12710.52051\n",
      "iter 29,  train loss 0.26198\n",
      "\n",
      "test acc 0.93009,  loss 0.22634\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy, batch norm\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "config.batch_norm = True\n",
    "model = CNN3(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "torch_optimizer = optim.SGD(list(model.parameters()), lr, momentum=momentum, nesterov=True)\n",
    "max_iters = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits, endpoints = model(images)\n",
    "\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "    \n",
    "    print(\"iter %d,  train loss %.5f\\n\" % (i, total_loss))\n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "single dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 12790.14453\n",
      "sum of output**2\n",
      " 22012.12\n",
      "sum of output**2\n",
      " 22012.12\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "drop_layer = nn.Dropout2d(p=0.5)\n",
    "p = drop_layer(images)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())\n",
    "\n",
    "torch.manual_seed(1)\n",
    "p = nfunc.dropout2d(images, p=0.5)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy, dropout\n",
      "test acc 0.12790,  loss 2.59773\n",
      "data 12790.14453\n",
      "iter 0,  train loss 3.05590\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 2.85969\n",
      "\n",
      "data 12442.99316\n",
      "iter 2,  train loss 2.33658\n",
      "\n",
      "data 12292.56152\n",
      "iter 3,  train loss 2.31932\n",
      "\n",
      "data 12402.40137\n",
      "iter 4,  train loss 2.06880\n",
      "\n",
      "data 13600.25098\n",
      "iter 5,  train loss 1.89253\n",
      "\n",
      "data 13450.91211\n",
      "iter 6,  train loss 1.69860\n",
      "\n",
      "data 12739.25195\n",
      "iter 7,  train loss 1.63169\n",
      "\n",
      "data 11608.33594\n",
      "iter 8,  train loss 1.53965\n",
      "\n",
      "data 14801.60449\n",
      "iter 9,  train loss 1.38021\n",
      "\n",
      "data 15733.70703\n",
      "iter 10,  train loss 1.36266\n",
      "\n",
      "data 12621.97754\n",
      "iter 11,  train loss 1.18539\n",
      "\n",
      "data 12366.39551\n",
      "iter 12,  train loss 1.02797\n",
      "\n",
      "data 12358.09375\n",
      "iter 13,  train loss 0.82358\n",
      "\n",
      "data 12827.04590\n",
      "iter 14,  train loss 0.79694\n",
      "\n",
      "data 14613.22363\n",
      "iter 15,  train loss 0.86224\n",
      "\n",
      "data 13533.24316\n",
      "iter 16,  train loss 0.70383\n",
      "\n",
      "data 13346.56543\n",
      "iter 17,  train loss 0.63417\n",
      "\n",
      "data 12810.09473\n",
      "iter 18,  train loss 0.70275\n",
      "\n",
      "data 13308.19238\n",
      "iter 19,  train loss 0.62492\n",
      "\n",
      "data 13648.76465\n",
      "iter 20,  train loss 0.68052\n",
      "\n",
      "data 12919.14941\n",
      "iter 21,  train loss 0.64686\n",
      "\n",
      "data 13686.84766\n",
      "iter 22,  train loss 0.63076\n",
      "\n",
      "data 12311.84375\n",
      "iter 23,  train loss 0.51695\n",
      "\n",
      "data 12001.96875\n",
      "iter 24,  train loss 0.55937\n",
      "\n",
      "data 11975.05176\n",
      "iter 25,  train loss 0.46521\n",
      "\n",
      "data 12390.50586\n",
      "iter 26,  train loss 0.52004\n",
      "\n",
      "data 12357.15332\n",
      "iter 27,  train loss 0.60729\n",
      "\n",
      "data 14262.87109\n",
      "iter 28,  train loss 0.37934\n",
      "\n",
      "data 12710.52051\n",
      "iter 29,  train loss 0.58319\n",
      "\n",
      "test acc 0.88361,  loss 0.38820\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy, dropout\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "config.batch_norm = False\n",
    "config.dropout_rate = 0.3\n",
    "\n",
    "model = CNN3(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "torch_optimizer = optim.SGD(list(model.parameters()), lr, momentum=momentum, nesterov=True)\n",
    "max_iters = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits, endpoints = model(images)\n",
    "\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "    \n",
    "    print(\"iter %d,  train loss %.5f\\n\" % (i, total_loss))\n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch, cross entropy, batch norm and dropout\n",
      "test acc 0.12790,  loss 2.59772\n",
      "data 12790.14453\n",
      "iter 0,  train loss 3.23084\n",
      "\n",
      "data 12953.98438\n",
      "iter 1,  train loss 2.54534\n",
      "\n",
      "data 12442.99316\n",
      "iter 2,  train loss 2.26509\n",
      "\n",
      "data 12292.56152\n",
      "iter 3,  train loss 1.87439\n",
      "\n",
      "data 12402.40137\n",
      "iter 4,  train loss 1.86594\n",
      "\n",
      "data 13600.25098\n",
      "iter 5,  train loss 1.56637\n",
      "\n",
      "data 13450.91211\n",
      "iter 6,  train loss 1.40208\n",
      "\n",
      "data 12739.25195\n",
      "iter 7,  train loss 1.21994\n",
      "\n",
      "data 11608.33594\n",
      "iter 8,  train loss 1.15521\n",
      "\n",
      "data 14801.60449\n",
      "iter 9,  train loss 0.96120\n",
      "\n",
      "data 15733.70703\n",
      "iter 10,  train loss 1.05848\n",
      "\n",
      "data 12621.97754\n",
      "iter 11,  train loss 0.67963\n",
      "\n",
      "data 12366.39551\n",
      "iter 12,  train loss 0.68146\n",
      "\n",
      "data 12358.09375\n",
      "iter 13,  train loss 0.56572\n",
      "\n",
      "data 12827.04590\n",
      "iter 14,  train loss 0.55992\n",
      "\n",
      "data 14613.22363\n",
      "iter 15,  train loss 0.57170\n",
      "\n",
      "data 13533.24316\n",
      "iter 16,  train loss 0.48861\n",
      "\n",
      "data 13346.56543\n",
      "iter 17,  train loss 0.49517\n",
      "\n",
      "data 12810.09473\n",
      "iter 18,  train loss 0.56066\n",
      "\n",
      "data 13308.19238\n",
      "iter 19,  train loss 0.48829\n",
      "\n",
      "data 13648.76465\n",
      "iter 20,  train loss 0.49601\n",
      "\n",
      "data 12919.14941\n",
      "iter 21,  train loss 0.56681\n",
      "\n",
      "data 13686.84766\n",
      "iter 22,  train loss 0.42190\n",
      "\n",
      "data 12311.84375\n",
      "iter 23,  train loss 0.52490\n",
      "\n",
      "data 12001.96875\n",
      "iter 24,  train loss 0.33208\n",
      "\n",
      "data 11975.05176\n",
      "iter 25,  train loss 0.36742\n",
      "\n",
      "data 12390.50586\n",
      "iter 26,  train loss 0.44744\n",
      "\n",
      "data 12357.15332\n",
      "iter 27,  train loss 0.44592\n",
      "\n",
      "data 14262.87109\n",
      "iter 28,  train loss 0.40995\n",
      "\n",
      "data 12710.52051\n",
      "iter 29,  train loss 0.45645\n",
      "\n",
      "test acc 0.89864,  loss 0.33027\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch, cross entropy, batch norm and dropout\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "config.batch_norm = True\n",
    "config.dropout_rate = 0.3\n",
    "\n",
    "model = CNN3(config)\n",
    "np.random.seed(1)\n",
    "model.apply(weights_init)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "torch_optimizer = optim.SGD(list(model.parameters()), lr, momentum=momentum, nesterov=True)\n",
    "max_iters = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    images = torch.FloatTensor(images.numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    labels = torch.LongTensor(labels.numpy()).to(device)\n",
    "\n",
    "    # Build model.\n",
    "    logits, endpoints = model(images)\n",
    "\n",
    "    total_loss = 0\n",
    "    loss_list = criterion(logits, labels)\n",
    "    xent_loss = torch.mean(loss_list)\n",
    "    total_loss = xent_loss\n",
    "    \n",
    "    print(\"iter %d,  train loss %.5f\\n\" % (i, total_loss))\n",
    "    torch_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch_optimizer.step()\n",
    "    torch_optimizer.zero_grad()\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator(), device)\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
