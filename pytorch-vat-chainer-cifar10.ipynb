{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import sys, os, time, argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as nfunc\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "except ImportError:\n",
    "    cp = None\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Variable, optimizers, cuda, serializers\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "gpu = \"\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ExpUtils import *\n",
    "\n",
    "from torch_func.utils import set_framework_seed, weights_init_normal, adjust_learning_rate\n",
    "from torch_func.evaluate import evaluate_classifier\n",
    "from torch_func.load_dataset import load_dataset\n",
    "import torch_func.CNN as CNN\n",
    "from torch_func.vat import VAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    set_framework_seed(args.seed, args.debug)\n",
    "\n",
    "    train_l, train_ul, test_set = load_dataset(\"dataset/%s\" % args.dataset, valid=False, dataset_seed=args.seed)\n",
    "    wlog(\"N_train_labeled:{}, N_train_unlabeled:{}\".format(train_l.N, train_ul.N))\n",
    "\n",
    "    test_set = TensorDataset(torch.FloatTensor(test_set.data), torch.LongTensor(test_set.label))\n",
    "    test_loader = DataLoader(test_set, 128, False)\n",
    "\n",
    "    arch = getattr(CNN, args.arch)\n",
    "    model = arch(args)\n",
    "    if args.debug:\n",
    "        # weights init is based on numpy, so only need np.random.seed()\n",
    "        np.random.seed(args.seed)\n",
    "        model.apply(weights_init_normal)\n",
    "\n",
    "    optimizer = optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    model = model.to(args.device)\n",
    "    model.train()\n",
    "\n",
    "    # Define losses.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    vat_criterion = VAT(args.device, eps=args.eps, xi=args.xi, use_ent_min=args.ent_min, debug=args.debug)\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(args.num_epochs):\n",
    "\n",
    "        sum_loss_l = 0\n",
    "        sum_loss_ul = 0\n",
    "        for it in range(args.num_iter_per_epoch):\n",
    "            x, t = train_l.get(args.batchsize, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            x_u, _ = train_ul.get(args.batchsize_ul, aug_trans=args.aug_trans, aug_flip=args.aug_flip)\n",
    "            \n",
    "            images = torch.FloatTensor(x).to(args.device)\n",
    "            labels = torch.LongTensor(t).to(args.device)\n",
    "            ul_images = torch.FloatTensor(x_u).to(args.device)\n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            sup_loss = 0\n",
    "            ul_loss = 0\n",
    "\n",
    "            # supervised loss\n",
    "            ce_loss = criterion(logits, labels)\n",
    "            sup_loss += ce_loss\n",
    "\n",
    "            if args.trainer == \"mle\":\n",
    "                total_loss = sup_loss\n",
    "            else:\n",
    "                ul_loss = vat_criterion(model, ul_images)\n",
    "                total_loss = sup_loss + ul_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((epoch % args.log_interval) == 0 and it == args.num_batch_it - 1) or (args.debug and it < 50):\n",
    "                n_err, test_loss = evaluate_classifier(model, test_loader, args.device)\n",
    "                acc = 1 - n_err / len(test_set)\n",
    "                wlog(\"Epoch: %d Train Loss: %.4f ce: %.5f, vat: %.5f, test loss: %.5f, test acc: %.4f\" % (epoch, total_loss, ce_loss, ul_loss, test_loss, acc))\n",
    "\n",
    "                pred_y = torch.max(logits, dim=1)[1]\n",
    "                train_acc = 1.0 * torch.sum(pred_y == labels).item() / pred_y.shape[0]\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, args)\n",
    "        if (epoch % args.log_interval) == 0:\n",
    "            wlog(\"learning rate %f\" % lr)\n",
    "            if args.vis:\n",
    "                args.writer.add_scalar(\"optimizer/learning_rate\", lr, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 21:53:03,540 - <ipython-input-19-9922df79d12a>[line:67]: run time marker C:\\Users\\34639\\project\\results\\VATSemi\\cifar10\\vat-epsilon=10-dropout_rate=0.5-20190509215303_running\n",
      "2019-05-09 21:53:03,542 - <ipython-input-19-9922df79d12a>[line:68]: args in this experiment ('affine', False)\n",
      "('alpha', 1)\n",
      "('arch', 'CNN9c')\n",
      "('aug_flip', False)\n",
      "('aug_trans', False)\n",
      "('batchsize', 32)\n",
      "('batchsize_eval', 100)\n",
      "('batchsize_ul', 128)\n",
      "('cuda', False)\n",
      "('data_dir', './dataset/cifar10')\n",
      "('dataset', 'cifar10')\n",
      "('dataset_seed', 1)\n",
      "('debug', False)\n",
      "('device', device(type='cpu'))\n",
      "('dropout_rate', 0.5)\n",
      "('ent_min', False)\n",
      "('epoch_decay_start', 0)\n",
      "('eps', 10)\n",
      "('epsilon', 10)\n",
      "('eval_freq', 5)\n",
      "('exp', 'avg')\n",
      "('extra_lamb', 1)\n",
      "('gpu_id', '')\n",
      "('iterations', 1000)\n",
      "('k', 1)\n",
      "('log_dir', 'C:\\\\Users\\\\34639\\\\project\\\\results\\\\VATSemi\\\\cifar10\\\\vat-epsilon=10-dropout_rate=0.5-20190509215303_running')\n",
      "('lr', 0.001)\n",
      "('method', 'vat')\n",
      "('mom1', 0.9)\n",
      "('mom2', 0.5)\n",
      "('n_categories', 10)\n",
      "('no_cuda', False)\n",
      "('num_epochs', 120)\n",
      "('num_iter_per_epoch', 500)\n",
      "('reg_lamb', 1)\n",
      "('seed', 1)\n",
      "('size', 100)\n",
      "('snapshot_freq', 20)\n",
      "('task_solver', 'VATSemi-vat')\n",
      "('top_bn', False)\n",
      "('trainer', 'vat')\n",
      "('use_entmin', False)\n",
      "('validation', False)\n",
      "('vis', False)\n",
      "('xi', 1e-06)\n",
      "2019-05-09 21:53:05,203 - <ipython-input-18-bc0c6bd04444>[line:5]: N_train_labeled:4000, N_train_unlabeled:50000\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = \"cifar10\"\n",
    "    args.trainer = \"vat\"\n",
    "    args.lr = 0.001\n",
    "    args.arch = \"CNN9c\"\n",
    "    args.iterations = 1000\n",
    "    args.seed = 1\n",
    "    args.size = 100\n",
    "    args.no_cuda = False\n",
    "    \n",
    "    args.xi = 1e-6\n",
    "    args.eps = 10\n",
    "    args.k = 1\n",
    "    args.use_entmin = False\n",
    "    args.alpha = 1\n",
    "    args.mom1 = 0.9\n",
    "    args.mom2 = 0.5\n",
    "    args.reg_lamb = 1\n",
    "    args.affine = False\n",
    "    args.ent_min = False\n",
    "    \n",
    "    args.gpu_id = \"\"\n",
    "    args.data_dir = \"./dataset/cifar10/\"\n",
    "    args.log_dir = \"log\"\n",
    "    args.n_categories = 10\n",
    "    args.eval_freq = 5\n",
    "    args.snapshot_freq = 20\n",
    "    args.aug_flip = False\n",
    "    args.aug_trans = False\n",
    "    args.validation = False\n",
    "    args.dataset_seed = 1\n",
    "    args.batchsize = 32\n",
    "    args.batchsize_ul = 128\n",
    "    args.batchsize_eval = 100\n",
    "    args.num_epochs = 120\n",
    "    args.num_iter_per_epoch = 500\n",
    "    args.epoch_decay_start = 0\n",
    "    args.method = \"vat\"\n",
    "    args.epsilon = 10\n",
    "    args.extra_lamb = 1\n",
    "    args.dropout_rate = 0.5\n",
    "    args.top_bn = False\n",
    "    args.vis = False\n",
    "    args.debug = False\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "    args.data_dir = os.path.join(\"./dataset/%s\" % args.dataset)\n",
    "    \n",
    "    chainer.global_config.cudnn_deterministic = True\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "arg = parse_args()\n",
    "arg.exp = \"avg\"\n",
    "saver = ExpSaver(\"VATSemi-%s\" % arg.method, arg, [\"epsilon\", \"dropout_rate\"], None)\n",
    "if arg.vis:\n",
    "    saver.init_writer([\"epsilon\", \"dropout_rate\"])\n",
    "arg.log_dir = saver.log_dir\n",
    "run_time = saver.log_dir\n",
    "wlog(\"run time marker %s\" % run_time)\n",
    "wlog(\"args in this experiment %s\", '\\n'.join(str(e) for e in sorted(vars(arg).items())))\n",
    "# noinspection PyBroadException\n",
    "\n",
    "train(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits 1279.9625\n",
      "True\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.3949525)\n",
      "logits 1279.9597\n",
      "True\n",
      "it 1 ce loss variable(2.447002) vat loss variable(0.59222496)\n",
      "logits 1279.9594\n",
      "True\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(0.60971195)\n",
      "logits 1279.9573\n",
      "True\n",
      "it 3 ce loss variable(2.6826663) vat loss variable(0.62293935)\n",
      "logits 1279.9609\n",
      "True\n",
      "it 4 ce loss variable(2.6122522) vat loss variable(0.6236712)\n",
      "test loss 2.315154103132395 test acc 0.11057692307692307\n"
     ]
    }
   ],
   "source": [
    "def loss_labeled(forward, x, t):\n",
    "    y = forward(x, update_batch_stats=True)\n",
    "    print(\"logits\", (y.data ** 2).sum())\n",
    "    L = F.softmax_cross_entropy(y, t)\n",
    "    return L\n",
    "\n",
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "\n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits 6.67572e-06\n",
      "it 0 ce loss variable(2.6372051) vat loss 0\n",
      "logits -6.389618e-05\n",
      "it 1 ce loss variable(1.0207927) vat loss 0\n",
      "logits 0.0028562546\n",
      "it 2 ce loss variable(0.9196422) vat loss 0\n",
      "logits 0.026527405\n",
      "it 3 ce loss variable(0.9217887) vat loss 0\n",
      "logits 0.05603504\n",
      "it 4 ce loss variable(0.94327796) vat loss 0\n",
      "test loss 0.9446233698190787 test acc 0.7655248397435898\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul = 0\n",
    "        #loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 3101.001574004157 127.99999951050698\n",
      "input 12790.146\n",
      "output 1.7166138e-05\n",
      "loss variable(-1.1133411e-08)\n",
      "d -0.9175085\n",
      "post cost variable(0.18286765)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18286765)\n",
      "d 3104.2961100598077 127.99999951136697\n",
      "input 12953.986\n",
      "output 1.9073486e-06\n",
      "loss variable(0.77622306)\n",
      "d -0.025210619\n",
      "post cost variable(1.2601826)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(1.2601826)\n",
      "d 3103.1908931663115 127.99999951057141\n",
      "input 12442.994\n",
      "output -1.4781952e-05\n",
      "loss variable(0.8542405)\n",
      "d 9.233387\n",
      "post cost variable(1.3496399)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(1.3496399)\n",
      "test loss 2.316648061458881 test acc 0.1141826923076923\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "debug = True\n",
    "def vat_loss(forward, distance, x, y=None, train=True, epsilon=8.0,\n",
    "             xi=1e-6, num_iter=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = np.random.random(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    if debug:\n",
    "        print(\"d\", d.sum(), (d ** 2).sum())\n",
    "        \n",
    "    for ip in range(num_iter):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        if debug:\n",
    "            print(\"input\", x_d.data.sum())\n",
    "        p_d_logit = forward(x_d, train=train, update_batch_stats=True)\n",
    "        if debug:\n",
    "            print(\"output\", p_d_logit.data.sum())\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        if debug:\n",
    "            print(\"loss\", kl_loss)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        if debug:\n",
    "            print(\"d\", d.sum())\n",
    "    x_adv = x + epsilon * d\n",
    "\n",
    "    p_adv_logit = forward(x_adv, train=train, update_batch_stats=False)\n",
    "    pos_cost = distance(p_logit, p_adv_logit)\n",
    "    if debug:\n",
    "        print(\"post cost\", pos_cost)\n",
    "    return pos_cost, p_d_logit\n",
    "\n",
    "for it in range(3):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x, test=False, update_batch_stats=True):\n",
    "    if test:\n",
    "        return F.fixed_batch_normalization(x, bn.gamma, bn.beta, bn.avg_mean, bn.avg_var)\n",
    "    elif not update_batch_stats:\n",
    "        return F.batch_normalization(x, bn.gamma, bn.beta)\n",
    "    else:\n",
    "        return bn(x)\n",
    "    \n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1)\n",
    "        super(MLP, self).__init__(\n",
    "            c1=L.Linear(784, 1200, initialW=initializer),\n",
    "            c2=L.Linear(1200, 1200, initialW=initializer),\n",
    "            l_cl=L.Linear(1200, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(1200),\n",
    "            bn2=L.BatchNormalization(1200),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        h = self.c2(h)\n",
    "        h = F.relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats))\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12442.993\n",
      "2.4318695e-05\n",
      "2.4318695e-05\n",
      "68.56414\n",
      "68.56414\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "\n",
    "model = enc\n",
    "print(images.numpy().sum())\n",
    "x = images.numpy()\n",
    "p = model(x, train=True, update_batch_stats=True)\n",
    "print(p.data.sum())\n",
    "p = model(x, train=True, update_batch_stats=False)\n",
    "print(p.data.sum())\n",
    "p = model(x, train=False, update_batch_stats=True)\n",
    "print(p.data.sum())\n",
    "\n",
    "p = model(x, train=False, update_batch_stats=False)\n",
    "print(p.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 3101.001574004157 127.99999951050698\n",
      "True\n",
      "input 12790.146\n",
      "output 60.197327\n",
      "loss variable(0.27454972)\n",
      "grad d 0.16488276\n",
      "d 51.43709\n",
      "post cost variable(0.3949525)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.3949525)\n",
      "d 3104.2961100598077 127.99999951136697\n",
      "True\n",
      "input 12953.986\n",
      "output 68.14215\n",
      "loss variable(0.43572783)\n",
      "grad d 0.07118307\n",
      "d 14.237335\n",
      "post cost variable(0.59222496)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(0.59222496)\n",
      "d 3103.1908931663115 127.99999951057141\n",
      "True\n",
      "input 12442.994\n",
      "output 47.62139\n",
      "loss variable(0.43799925)\n",
      "grad d 0.0547363\n",
      "d 14.975924\n",
      "post cost variable(0.60971195)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(0.60971195)\n",
      "test loss 2.3108466466267905 test acc 0.11207932692307693\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "debug = True\n",
    "\n",
    "def vat_loss(forward, distance, x, y=None, train=True, epsilon=8.0,\n",
    "             xi=1e-6, num_iter=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = np.random.random(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    if debug:\n",
    "        print(\"d\", d.sum(), (d ** 2).sum())\n",
    "    print(train)\n",
    "    for ip in range(num_iter):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        if debug:\n",
    "            print(\"input\", x_d.data.sum())\n",
    "        p_d_logit = forward(x_d, train=False, update_batch_stats=False)\n",
    "        if debug:\n",
    "            print(\"output\", p_d_logit.data.sum())\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        if debug:\n",
    "            print(\"loss\", kl_loss)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        if debug:\n",
    "            print(\"grad d\", d.sum())\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        if debug:\n",
    "            print(\"d\", d.sum())\n",
    "    x_adv = x + epsilon * d\n",
    "\n",
    "    p_adv_logit = forward(x_adv, train=False, update_batch_stats=False)\n",
    "    pos_cost = distance(p_logit, p_adv_logit)\n",
    "    if debug:\n",
    "        print(\"post cost\", pos_cost)\n",
    "    return pos_cost, p_d_logit\n",
    "\n",
    "for it in range(3):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "   \n",
    "acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_categorical(p_logit, q_logit):\n",
    "    if isinstance(p_logit, chainer.Variable):\n",
    "        xp = cuda.get_array_module(p_logit.data)\n",
    "    else:\n",
    "        xp = cuda.get_array_module(p_logit)\n",
    "    p = F.softmax(p_logit)\n",
    "    # print(p_logit.sum())\n",
    "    # print(q_logit.data.sum())\n",
    "    _kl = F.sum(p * (F.log_softmax(p_logit) - F.log_softmax(q_logit)), 1)\n",
    "    # print(_kl)\n",
    "    return F.sum(_kl) / xp.prod(xp.array(_kl.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18520069)\n",
      "it 1 ce loss variable(2.447002) vat loss variable(1.2601826)\n",
      "it 2 ce loss variable(2.3731635) vat loss variable(1.3496399)\n",
      "it 3 ce loss variable(2.6826663) vat loss variable(1.3082993)\n",
      "it 4 ce loss variable(2.6122522) vat loss variable(1.2677813)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(5):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "#         enc.cleargrads()\n",
    "#         loss_l.backward()\n",
    "#         optimizer.update()\n",
    "    if (it+1) % 10 == 0:\n",
    "        \n",
    "        acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "        print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 9 ce loss variable(0.34254467) vat loss variable(1.3673117)\n",
      "test loss 1.2030614102498078 test acc 0.7214543269230769\n",
      "it 19 ce loss variable(0.2949698) vat loss variable(1.4227189)\n",
      "test loss 1.136353354423474 test acc 0.7433894230769231\n",
      "it 29 ce loss variable(0.27102554) vat loss variable(1.4771286)\n",
      "test loss 1.1068920233310797 test acc 0.7440905448717948\n",
      "it 39 ce loss variable(0.25327155) vat loss variable(1.5224514)\n",
      "test loss 1.1016949224166381 test acc 0.7450921474358975\n",
      "it 49 ce loss variable(0.23831564) vat loss variable(1.565478)\n",
      "test loss 1.1002999788675554 test acc 0.7425881410256411\n",
      "it 59 ce loss variable(0.22491816) vat loss variable(1.6105677)\n",
      "test loss 1.0964220601778765 test acc 0.7416866987179487\n",
      "it 69 ce loss variable(0.21271741) vat loss variable(1.6542144)\n",
      "test loss 1.0899064196990087 test acc 0.7415865384615384\n",
      "it 79 ce loss variable(0.20149495) vat loss variable(1.6965376)\n",
      "test loss 1.0825705895057092 test acc 0.7407852564102564\n",
      "it 89 ce loss variable(0.19111884) vat loss variable(1.7388303)\n",
      "test loss 1.0754560790000818 test acc 0.7403846153846154\n",
      "it 99 ce loss variable(0.1815022) vat loss variable(1.7804842)\n",
      "test loss 1.0681669826690967 test acc 0.7398838141025641\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "debug = False\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "for it in range(100):\n",
    "    \n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        \n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()\n",
    "    if (it+1) % 10 == 0:\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        acc, loss = evaluate_classifier(enc, test_dataset.dataset.make_one_shot_iterator())\n",
    "        print(\"test loss\", loss, \"test acc\", acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d 18.809333635120456\n",
      "loss variable(-1.0035267e-08)\n",
      "d 2.8182023\n",
      "variable(0.18520069)\n",
      "it 0 ce loss variable(2.6372051) vat loss variable(0.18520069)\n",
      "d 13.625716652791654\n",
      "loss variable(1.0147533)\n",
      "d -10.247404\n",
      "variable(1.2203554)\n",
      "it 1 ce loss variable(1.0207927) vat loss variable(1.2203554)\n",
      "d -6.073275889670836\n",
      "loss variable(1.0948114)\n",
      "d 2.128046\n",
      "variable(1.2472886)\n",
      "it 2 ce loss variable(0.9196422) vat loss variable(1.2472886)\n",
      "d -5.657291669940053\n",
      "loss variable(1.1145282)\n",
      "d -7.6926003\n",
      "variable(1.2436831)\n",
      "it 3 ce loss variable(0.9217887) vat loss variable(1.2436831)\n",
      "d -0.31942830946959505\n",
      "loss variable(1.0268389)\n",
      "d 17.752928\n",
      "variable(1.159715)\n",
      "it 4 ce loss variable(0.94327796) vat loss variable(1.159715)\n",
      "d 15.941850424374195\n",
      "loss variable(1.0886321)\n",
      "d -1.6491866\n",
      "variable(1.1983008)\n",
      "it 5 ce loss variable(0.84185266) vat loss variable(1.1983008)\n",
      "d -1.8059021547236176\n",
      "loss variable(1.1284835)\n",
      "d 12.763174\n",
      "variable(1.232116)\n",
      "it 6 ce loss variable(0.8465243) vat loss variable(1.232116)\n",
      "d 12.150667692702767\n",
      "loss variable(1.0905163)\n",
      "d 11.111301\n",
      "variable(1.1997862)\n",
      "it 7 ce loss variable(0.8774015) vat loss variable(1.1997862)\n",
      "d -22.19119122219938\n",
      "loss variable(1.0985621)\n",
      "d 1.4018455\n",
      "variable(1.2154684)\n",
      "it 8 ce loss variable(0.95440537) vat loss variable(1.2154684)\n",
      "d -1.1151227363142457\n",
      "loss variable(1.0837042)\n",
      "d -3.3026528\n",
      "variable(1.1895165)\n",
      "it 9 ce loss variable(0.79499936) vat loss variable(1.1895165)\n",
      "d 19.537457862010598\n",
      "loss variable(1.1167276)\n",
      "d 6.0228558\n",
      "variable(1.2159164)\n",
      "it 10 ce loss variable(0.7709754) vat loss variable(1.2159164)\n",
      "d -28.764608738084966\n",
      "loss variable(1.0599291)\n",
      "d -4.6187477\n",
      "variable(1.170081)\n",
      "it 11 ce loss variable(0.7052435) vat loss variable(1.170081)\n",
      "d 8.82450542634338\n",
      "loss variable(1.1599612)\n",
      "d 4.174144\n",
      "variable(1.2607242)\n",
      "it 12 ce loss variable(0.7232995) vat loss variable(1.2607242)\n",
      "d 17.07257253366464\n",
      "loss variable(1.1669801)\n",
      "d -10.064225\n",
      "variable(1.2728804)\n",
      "it 13 ce loss variable(0.64415395) vat loss variable(1.2728804)\n",
      "d -18.55460323395467\n",
      "loss variable(1.1778262)\n",
      "d -12.064077\n",
      "variable(1.2870444)\n",
      "it 14 ce loss variable(0.6375556) vat loss variable(1.2870444)\n",
      "d -12.81132006292529\n",
      "loss variable(1.1236259)\n",
      "d -9.917589\n",
      "variable(1.2267137)\n",
      "it 15 ce loss variable(0.68250287) vat loss variable(1.2267137)\n",
      "d -12.5480754497891\n",
      "loss variable(1.2604878)\n",
      "d -1.2017266\n",
      "variable(1.3578336)\n",
      "it 16 ce loss variable(0.6229531) vat loss variable(1.3578336)\n",
      "d 11.990174416589168\n",
      "loss variable(1.1660719)\n",
      "d -9.589942\n",
      "variable(1.2682521)\n",
      "it 17 ce loss variable(0.66972756) vat loss variable(1.2682521)\n",
      "d -10.407844440895506\n",
      "loss variable(1.1450057)\n",
      "d 12.380405\n",
      "variable(1.2499914)\n",
      "it 18 ce loss variable(0.669995) vat loss variable(1.2499914)\n",
      "d -3.8609322673812807\n",
      "loss variable(1.2059662)\n",
      "d -1.9027166\n",
      "variable(1.3109951)\n",
      "it 19 ce loss variable(0.5574555) vat loss variable(1.3109951)\n",
      "d -3.3537872066015533\n",
      "loss variable(1.262367)\n",
      "d 4.4025707\n",
      "variable(1.3641775)\n",
      "it 20 ce loss variable(0.5962399) vat loss variable(1.3641775)\n",
      "d -5.536906575310836\n",
      "loss variable(1.1905057)\n",
      "d 0.22041392\n",
      "variable(1.3089786)\n",
      "it 21 ce loss variable(0.6798394) vat loss variable(1.3089786)\n",
      "d 9.461606045093934\n",
      "loss variable(1.199542)\n",
      "d -0.82267714\n",
      "variable(1.3103716)\n",
      "it 22 ce loss variable(0.65722597) vat loss variable(1.3103716)\n",
      "d 1.659321208207567\n",
      "loss variable(1.2956293)\n",
      "d -14.656862\n",
      "variable(1.4014994)\n",
      "it 23 ce loss variable(0.62663084) vat loss variable(1.4014994)\n",
      "d 2.1031809384112496\n",
      "loss variable(1.2720475)\n",
      "d 1.4196811\n",
      "variable(1.3819842)\n",
      "it 24 ce loss variable(0.52777994) vat loss variable(1.3819842)\n",
      "d 1.9628915927641861\n",
      "loss variable(1.235307)\n",
      "d -9.867787\n",
      "variable(1.3473048)\n",
      "it 25 ce loss variable(0.5819466) vat loss variable(1.3473048)\n",
      "d 0.7717617320859329\n",
      "loss variable(1.2398214)\n",
      "d -26.134771\n",
      "variable(1.3555927)\n",
      "it 26 ce loss variable(0.5324574) vat loss variable(1.3555927)\n",
      "d -9.484701186795872\n",
      "loss variable(1.2564429)\n",
      "d -12.015231\n",
      "variable(1.3745458)\n",
      "it 27 ce loss variable(0.5595169) vat loss variable(1.3745458)\n",
      "d 3.6070528373359263\n",
      "loss variable(1.24495)\n",
      "d -23.010424\n",
      "variable(1.3467615)\n",
      "it 28 ce loss variable(0.5578137) vat loss variable(1.3467615)\n",
      "d -0.4210237207167622\n",
      "loss variable(1.2669747)\n",
      "d -9.834547\n",
      "variable(1.3829197)\n",
      "it 29 ce loss variable(0.6134574) vat loss variable(1.3829197)\n",
      "d 0.6141587798513892\n",
      "loss variable(1.2770869)\n",
      "d -16.520443\n",
      "variable(1.387226)\n",
      "it 30 ce loss variable(0.52135503) vat loss variable(1.387226)\n",
      "d 11.642277595008917\n",
      "loss variable(1.2927568)\n",
      "d -0.32007957\n",
      "variable(1.3948996)\n",
      "it 31 ce loss variable(0.61205536) vat loss variable(1.3948996)\n",
      "d 11.742239405125495\n",
      "loss variable(1.3124548)\n",
      "d -19.97419\n",
      "variable(1.4259231)\n",
      "it 32 ce loss variable(0.54090494) vat loss variable(1.4259231)\n",
      "d -5.521902693695445\n",
      "loss variable(1.2701559)\n",
      "d -17.54828\n",
      "variable(1.3852599)\n",
      "it 33 ce loss variable(0.6153326) vat loss variable(1.3852599)\n",
      "d 11.443393270560357\n",
      "loss variable(1.2957718)\n",
      "d 10.306329\n",
      "variable(1.4033513)\n",
      "it 34 ce loss variable(0.55727744) vat loss variable(1.4033513)\n",
      "d 0.1089651630876809\n",
      "loss variable(1.2646873)\n",
      "d -32.211845\n",
      "variable(1.3706245)\n",
      "it 35 ce loss variable(0.46945387) vat loss variable(1.3706245)\n",
      "d -18.958932883439566\n",
      "loss variable(1.3405955)\n",
      "d -21.759275\n",
      "variable(1.4391092)\n",
      "it 36 ce loss variable(0.5755157) vat loss variable(1.4391092)\n",
      "d 17.834838563985212\n",
      "loss variable(1.3036685)\n",
      "d -11.244947\n",
      "variable(1.4127519)\n",
      "it 37 ce loss variable(0.5539928) vat loss variable(1.4127519)\n",
      "d -1.9762978829981006\n",
      "loss variable(1.2770425)\n",
      "d -15.831317\n",
      "variable(1.3949046)\n",
      "it 38 ce loss variable(0.52981627) vat loss variable(1.3949046)\n",
      "d 5.711908568132681\n",
      "loss variable(1.2900443)\n",
      "d 7.8729\n",
      "variable(1.4052365)\n",
      "it 39 ce loss variable(0.48810005) vat loss variable(1.4052365)\n",
      "d -1.4692007664838331\n",
      "loss variable(1.341851)\n",
      "d -17.915264\n",
      "variable(1.4504914)\n",
      "it 40 ce loss variable(0.55558777) vat loss variable(1.4504914)\n",
      "d -5.126474138727224\n",
      "loss variable(1.3140737)\n",
      "d 10.415051\n",
      "variable(1.4239393)\n",
      "it 41 ce loss variable(0.55384386) vat loss variable(1.4239393)\n",
      "d 9.56829916398193\n",
      "loss variable(1.335239)\n",
      "d -5.7372437\n",
      "variable(1.4276927)\n",
      "it 42 ce loss variable(0.46935156) vat loss variable(1.4276927)\n",
      "d -20.339647875375555\n",
      "loss variable(1.3320531)\n",
      "d -11.940778\n",
      "variable(1.443888)\n",
      "it 43 ce loss variable(0.5271072) vat loss variable(1.443888)\n",
      "d 6.838921258408536\n",
      "loss variable(1.3308517)\n",
      "d -11.9447975\n",
      "variable(1.4417572)\n",
      "it 44 ce loss variable(0.5176019) vat loss variable(1.4417572)\n",
      "d -20.882921617519422\n",
      "loss variable(1.3548068)\n",
      "d -8.516261\n",
      "variable(1.4687951)\n",
      "it 45 ce loss variable(0.46282414) vat loss variable(1.4687951)\n",
      "d -7.4476275107837155\n",
      "loss variable(1.3583913)\n",
      "d -14.767474\n",
      "variable(1.4662354)\n",
      "it 46 ce loss variable(0.44224188) vat loss variable(1.4662354)\n",
      "d -4.8765588240176045\n",
      "loss variable(1.4045091)\n",
      "d -31.569254\n",
      "variable(1.5023803)\n",
      "it 47 ce loss variable(0.4666009) vat loss variable(1.5023803)\n",
      "d 6.198011766313341\n",
      "loss variable(1.3791254)\n",
      "d -14.198257\n",
      "variable(1.4844203)\n",
      "it 48 ce loss variable(0.4652521) vat loss variable(1.4844203)\n",
      "d -18.417532164672053\n",
      "loss variable(1.3293316)\n",
      "d -19.668766\n",
      "variable(1.441889)\n",
      "it 49 ce loss variable(0.4740532) vat loss variable(1.441889)\n",
      "d 11.499870444884642\n",
      "loss variable(1.3680856)\n",
      "d -18.667166\n",
      "variable(1.4715075)\n",
      "it 50 ce loss variable(0.5043552) vat loss variable(1.4715075)\n",
      "d -3.916152708342042\n",
      "loss variable(1.4374003)\n",
      "d -0.6954527\n",
      "variable(1.5389137)\n",
      "it 51 ce loss variable(0.427392) vat loss variable(1.5389137)\n",
      "d 10.01712975610236\n",
      "loss variable(1.4190614)\n",
      "d -20.789139\n",
      "variable(1.5397143)\n",
      "it 52 ce loss variable(0.4457674) vat loss variable(1.5397143)\n",
      "d 7.301845633936739\n",
      "loss variable(1.3485345)\n",
      "d -18.79001\n",
      "variable(1.4802843)\n",
      "it 53 ce loss variable(0.585494) vat loss variable(1.4802843)\n",
      "d 15.86757225070745\n",
      "loss variable(1.343765)\n",
      "d -23.600498\n",
      "variable(1.4597104)\n",
      "it 54 ce loss variable(0.59236884) vat loss variable(1.4597104)\n",
      "d 6.545725992832868\n",
      "loss variable(1.4197702)\n",
      "d -2.1002972\n",
      "variable(1.530483)\n",
      "it 55 ce loss variable(0.4669355) vat loss variable(1.530483)\n",
      "d -0.9768616916579125\n",
      "loss variable(1.358917)\n",
      "d -6.005143\n",
      "variable(1.4934279)\n",
      "it 56 ce loss variable(0.5691479) vat loss variable(1.4934279)\n",
      "d -11.935087234972634\n",
      "loss variable(1.3851652)\n",
      "d -10.979122\n",
      "variable(1.511116)\n",
      "it 57 ce loss variable(0.49034953) vat loss variable(1.511116)\n",
      "d -20.651292250502454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss variable(1.3605834)\n",
      "d -5.204693\n",
      "variable(1.4866124)\n",
      "it 58 ce loss variable(0.4668119) vat loss variable(1.4866124)\n",
      "d 5.930736741602792\n",
      "loss variable(1.4217685)\n",
      "d -2.96503\n",
      "variable(1.5363104)\n",
      "it 59 ce loss variable(0.48291457) vat loss variable(1.5363104)\n",
      "d -2.0687157650696317\n",
      "loss variable(1.3750725)\n",
      "d -20.187246\n",
      "variable(1.5088575)\n",
      "it 60 ce loss variable(0.4801722) vat loss variable(1.5088575)\n",
      "d 1.8124146704867552\n",
      "loss variable(1.4250214)\n",
      "d -31.683117\n",
      "variable(1.5315392)\n",
      "it 61 ce loss variable(0.50071263) vat loss variable(1.5315392)\n",
      "d -2.850726191424384\n",
      "loss variable(1.4569461)\n",
      "d -34.39248\n",
      "variable(1.5637801)\n",
      "it 62 ce loss variable(0.42511448) vat loss variable(1.5637801)\n",
      "d -11.751381348113794\n",
      "loss variable(1.3885074)\n",
      "d -24.86853\n",
      "variable(1.5128889)\n",
      "it 63 ce loss variable(0.49216902) vat loss variable(1.5128889)\n",
      "d 6.378625198477101\n",
      "loss variable(1.4517913)\n",
      "d -32.447678\n",
      "variable(1.5701138)\n",
      "it 64 ce loss variable(0.5427566) vat loss variable(1.5701138)\n",
      "d 2.7153277303448657\n",
      "loss variable(1.465215)\n",
      "d -22.916714\n",
      "variable(1.5846007)\n",
      "it 65 ce loss variable(0.40772316) vat loss variable(1.5846007)\n",
      "d -22.308087885109458\n",
      "loss variable(1.4969636)\n",
      "d -36.372955\n",
      "variable(1.6092582)\n",
      "it 66 ce loss variable(0.50262094) vat loss variable(1.6092582)\n",
      "d -2.4192262486470897\n",
      "loss variable(1.4375935)\n",
      "d -14.680861\n",
      "variable(1.5536546)\n",
      "it 67 ce loss variable(0.5135555) vat loss variable(1.5536546)\n",
      "d 6.019773782497798\n",
      "loss variable(1.3961962)\n",
      "d -63.45828\n",
      "variable(1.527636)\n",
      "it 68 ce loss variable(0.55315363) vat loss variable(1.527636)\n",
      "d -13.050808521321562\n",
      "loss variable(1.4348187)\n",
      "d -35.248444\n",
      "variable(1.5626605)\n",
      "it 69 ce loss variable(0.5986991) vat loss variable(1.5626605)\n",
      "d 12.242728546553089\n",
      "loss variable(1.4967582)\n",
      "d -0.91281223\n",
      "variable(1.6048044)\n",
      "it 70 ce loss variable(0.39243954) vat loss variable(1.6048044)\n",
      "d -1.0065886980648036\n",
      "loss variable(1.4938748)\n",
      "d -26.504574\n",
      "variable(1.6141081)\n",
      "it 71 ce loss variable(0.425413) vat loss variable(1.6141081)\n",
      "d -5.898650847588845\n",
      "loss variable(1.4512051)\n",
      "d -26.741228\n",
      "variable(1.5761062)\n",
      "it 72 ce loss variable(0.46639082) vat loss variable(1.5761062)\n",
      "d 0.20407054445150008\n",
      "loss variable(1.5079597)\n",
      "d -9.618024\n",
      "variable(1.6198823)\n",
      "it 73 ce loss variable(0.5143717) vat loss variable(1.6198823)\n",
      "d -1.8879851720624448\n",
      "loss variable(1.5007646)\n",
      "d -18.218437\n",
      "variable(1.6331649)\n",
      "it 74 ce loss variable(0.48229313) vat loss variable(1.6331649)\n",
      "d -17.765654704610878\n",
      "loss variable(1.4432151)\n",
      "d -23.259878\n",
      "variable(1.5719182)\n",
      "it 75 ce loss variable(0.40724617) vat loss variable(1.5719182)\n",
      "d 8.99825119708387\n",
      "loss variable(1.4926869)\n",
      "d 6.604527\n",
      "variable(1.5989352)\n",
      "it 76 ce loss variable(0.43710768) vat loss variable(1.5989352)\n",
      "d -12.127983976464943\n",
      "loss variable(1.5324087)\n",
      "d -6.8169055\n",
      "variable(1.6448643)\n",
      "it 77 ce loss variable(0.36148173) vat loss variable(1.6448643)\n",
      "d 22.194063711211534\n",
      "loss variable(1.4740313)\n",
      "d -32.723717\n",
      "variable(1.5895956)\n",
      "it 78 ce loss variable(0.40229177) vat loss variable(1.5895956)\n",
      "d 1.0636439482504092\n",
      "loss variable(1.501802)\n",
      "d -26.298784\n",
      "variable(1.6283778)\n",
      "it 79 ce loss variable(0.4478165) vat loss variable(1.6283778)\n",
      "d -2.360875721615897\n",
      "loss variable(1.4691436)\n",
      "d -10.845854\n",
      "variable(1.5909395)\n",
      "it 80 ce loss variable(0.4349839) vat loss variable(1.5909395)\n",
      "d -16.35444721518794\n",
      "loss variable(1.5467818)\n",
      "d -22.867163\n",
      "variable(1.6699708)\n",
      "it 81 ce loss variable(0.38690618) vat loss variable(1.6699708)\n",
      "d -5.459936451380143\n",
      "loss variable(1.5207355)\n",
      "d 0.65002143\n",
      "variable(1.6383059)\n",
      "it 82 ce loss variable(0.31777406) vat loss variable(1.6383059)\n",
      "d 4.842541580840611\n",
      "loss variable(1.5121353)\n",
      "d -16.626282\n",
      "variable(1.6369443)\n",
      "it 83 ce loss variable(0.42345172) vat loss variable(1.6369443)\n",
      "d 5.4152817471985975\n",
      "loss variable(1.5143564)\n",
      "d -5.9943886\n",
      "variable(1.6413108)\n",
      "it 84 ce loss variable(0.4236464) vat loss variable(1.6413108)\n",
      "d -5.754446120699502\n",
      "loss variable(1.5209382)\n",
      "d -24.78445\n",
      "variable(1.6328276)\n",
      "it 85 ce loss variable(0.38971806) vat loss variable(1.6328276)\n",
      "d 23.01949145393792\n",
      "loss variable(1.5336045)\n",
      "d -2.1896753\n",
      "variable(1.6568484)\n",
      "it 86 ce loss variable(0.41371632) vat loss variable(1.6568484)\n",
      "d 1.026650813650571\n",
      "loss variable(1.5694077)\n",
      "d 0.43878627\n",
      "variable(1.6916659)\n",
      "it 87 ce loss variable(0.43581235) vat loss variable(1.6916659)\n",
      "d -3.7272299351401204\n",
      "loss variable(1.5322535)\n",
      "d -12.44911\n",
      "variable(1.6510043)\n",
      "it 88 ce loss variable(0.39081147) vat loss variable(1.6510043)\n",
      "d 2.4539012455792886\n",
      "loss variable(1.49538)\n",
      "d -42.64389\n",
      "variable(1.6265726)\n",
      "it 89 ce loss variable(0.36579055) vat loss variable(1.6265726)\n",
      "d -9.077089979175865\n",
      "loss variable(1.515259)\n",
      "d -32.392242\n",
      "variable(1.6441574)\n",
      "it 90 ce loss variable(0.47298357) vat loss variable(1.6441574)\n",
      "d 1.5298456952706845\n",
      "loss variable(1.5651612)\n",
      "d -38.877937\n",
      "variable(1.6898088)\n",
      "it 91 ce loss variable(0.42935228) vat loss variable(1.6898088)\n",
      "d 7.189722053016781\n",
      "loss variable(1.5272)\n",
      "d -53.860912\n",
      "variable(1.6615038)\n",
      "it 92 ce loss variable(0.42192256) vat loss variable(1.6615038)\n",
      "d 9.480157833083993\n",
      "loss variable(1.5183927)\n",
      "d -49.79727\n",
      "variable(1.652884)\n",
      "it 93 ce loss variable(0.4143789) vat loss variable(1.652884)\n",
      "d 3.1108946017806796\n",
      "loss variable(1.5771916)\n",
      "d -59.646393\n",
      "variable(1.6935525)\n",
      "it 94 ce loss variable(0.37112278) vat loss variable(1.6935525)\n",
      "d 2.823493722115363\n",
      "loss variable(1.5676838)\n",
      "d -20.721746\n",
      "variable(1.6882787)\n",
      "it 95 ce loss variable(0.39960742) vat loss variable(1.6882787)\n",
      "d -12.189618523958252\n",
      "loss variable(1.5682818)\n",
      "d -16.407381\n",
      "variable(1.697522)\n",
      "it 96 ce loss variable(0.42520842) vat loss variable(1.697522)\n",
      "d 1.818478622741708\n",
      "loss variable(1.575295)\n",
      "d 11.22592\n",
      "variable(1.7041317)\n",
      "it 97 ce loss variable(0.40007377) vat loss variable(1.7041317)\n",
      "d 0.1806178413765278\n",
      "loss variable(1.5444249)\n",
      "d -46.634815\n",
      "variable(1.670377)\n",
      "it 98 ce loss variable(0.5384443) vat loss variable(1.670377)\n",
      "d 1.1534798487339804\n",
      "loss variable(1.5304189)\n",
      "d -47.21165\n",
      "variable(1.6656125)\n",
      "it 99 ce loss variable(0.46958965) vat loss variable(1.6656125)\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "set_framework_seed(1)\n",
    "enc = MLP(n_outputs=args.n_categories, dropout_rate=0, top_bn=True)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "optimizer = optimizers.Adam(alpha=args.lr)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "\n",
    "for it in range(100):\n",
    "    images, labels = iterator.get_next()\n",
    "    with chainer.using_config(\"train\", True):\n",
    "        x = images.numpy()\n",
    "        t = labels.numpy()\n",
    "        loss_l = loss_labeled(enc, Variable(x), Variable(t))\n",
    "        x_u = x\n",
    "        loss_ul, _ = vat_loss(enc, kl_categorical, Variable(x_u), None, epsilon=args.epsilon, xi=1e-6, p_logit=logit.data)\n",
    "        loss_total = loss_l + loss_ul\n",
    "        print(\"it\", it, \"ce loss\", loss_l, \"vat loss\", loss_ul)\n",
    "        enc.cleargrads()\n",
    "        loss_l.backward()\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=args.top_bn)\n",
    "if args.gpu:\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "    \n",
    "with chainer.using_config(\"train\", False):\n",
    "    logits = enc(Variable(x), train=False, update_batch_stats=False)\n",
    "    print(logits.shape)\n",
    "    print(logits.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_loss(forward, distance, x, train=True, epsilon=8.0, xi=1e-6, Ip=1, p_logit=None):\n",
    "    if p_logit is None:\n",
    "        p_logit = forward(x, train=train, update_batch_stats=False).data  # unchain\n",
    "    else:\n",
    "        assert not isinstance(p_logit, Variable)\n",
    "\n",
    "    xp = cuda.get_array_module(x.data)\n",
    "    d = xp.random.normal(size=x.shape)\n",
    "    d = get_normalized_vector(d, xp)\n",
    "    for ip in range(Ip):\n",
    "        x_d = Variable(x.data + xi * d.astype(xp.float32))\n",
    "        p_d_logit = forward(x_d, train=train, update_batch_stats=False)\n",
    "        kl_loss = distance(p_logit, p_d_logit)\n",
    "        kl_loss.backward()\n",
    "        d = x_d.grad\n",
    "        d = d / xp.sqrt(xp.sum(d ** 2, axis=tuple(range(1, len(d.shape))), keepdims=True))\n",
    "    x_adv = x + epsilon * d \n",
    "    p_adv_logit = forward(x_adv, train=train, update_batch_stats=False)\n",
    "    return distance(p_logit, p_adv_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_framework_seed(args.seed)\n",
    "device = args.device\n",
    "\n",
    "train_all, test, shape, num_classes = data_set_name(args.dataset)\n",
    "if args.size == 0:\n",
    "    args.size = len(train_all)\n",
    "test_loader = DataLoader(test, 1000, num_workers=3)\n",
    "\n",
    "train_l = SubsetDataset(train_all, list(range(args.size)))\n",
    "train_ul = SubsetDataset(train_all, list(range(len(train_all) - 1000)))\n",
    "\n",
    "print(len(train_l), len(train_ul))\n",
    "\n",
    "batch_size_l = 32\n",
    "batch_size_ul = 128\n",
    "\n",
    "Arch = getattr(models, args.arch)\n",
    "api_criterion = None\n",
    "\n",
    "if args.trainer != \"none\":\n",
    "    api_criterion = getattr(SemiMode, args.trainer)(args)\n",
    "\n",
    "set_framework_seed(args.seed)\n",
    "l_train_iter = iter(DataLoader(train_l, batch_size_l, num_workers=0, sampler=InfiniteSampler(len(train_l))))\n",
    "ul_train_iter = iter(DataLoader(train_ul, batch_size_ul, num_workers=0, sampler=InfiniteSampler(len(train_ul))))\n",
    "\n",
    "l_x, l_y = next(l_train_iter)\n",
    "print(\"l_y\", l_y[:5])\n",
    "ul_x, ul_y = next(ul_train_iter)\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "print(\"ul_y\", ul_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_outputs=10, dropout_rate=0.5, top_bn=False, dropout=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.dropout = dropout\n",
    "        initializer = chainer.initializers.HeUniform(1)\n",
    "        super(CNN, self).__init__(\n",
    "            c1=L.Convolution2D(3, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c2=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c3=L.Convolution2D(128, 128, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c4=L.Convolution2D(128, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c5=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c6=L.Convolution2D(256, 256, ksize=3, stride=1, pad=1, initialW=initializer),\n",
    "            c7=L.Convolution2D(256, 512, ksize=3, stride=1, pad=0, initialW=initializer),\n",
    "            c8=L.Convolution2D(512, 256, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            c9=L.Convolution2D(256, 128, ksize=1, stride=1, pad=0, initialW=initializer),\n",
    "            l_cl=L.Linear(128, n_outputs, initialW=initializer),\n",
    "            bn1=L.BatchNormalization(128),\n",
    "            bn2=L.BatchNormalization(128),\n",
    "            bn3=L.BatchNormalization(128),\n",
    "            bn4=L.BatchNormalization(256),\n",
    "            bn5=L.BatchNormalization(256),\n",
    "            bn6=L.BatchNormalization(256),\n",
    "            bn7=L.BatchNormalization(512),\n",
    "            bn8=L.BatchNormalization(256),\n",
    "            bn9=L.BatchNormalization(128),\n",
    "        )\n",
    "        if top_bn:\n",
    "            self.add_link('bn_cl', L.BatchNormalization(n_outputs))\n",
    "\n",
    "    def __call__(self, x, train=True, update_batch_stats=True):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        if self.dropout:\n",
    "            h = F.dropout(h, ratio=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h, test=not train, update_batch_stats=update_batch_stats), slope=0.1)\n",
    "        h = F.average_pooling_2d(h, ksize=h.data.shape[2])\n",
    "        logit = self.l_cl(h)\n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_cl, logit, test=not train, update_batch_stats=update_batch_stats)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-562.97064 40.848587\n"
     ]
    }
   ],
   "source": [
    "set_framework_seed(1)\n",
    "# enc = MLP(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False)\n",
    "enc = CNN(n_outputs=args.n_categories, dropout_rate=args.dropout_rate, top_bn=False, dropout=False)\n",
    "if args.gpu > -1:\n",
    "    print(\"gpu\")\n",
    "    chainer.cuda.get_device(args.gpu).use()\n",
    "    enc.to_gpu()\n",
    "set_framework_seed(1)\n",
    "out = enc(Variable(x), update_batch_stats=True)\n",
    "print(x.sum(), out.data.sum())\n",
    "optimizer = optimizers.Adam(alpha=args.lr, beta1=args.mom1)\n",
    "optimizer.setup(enc)\n",
    "optimizer.use_cleargrads()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
