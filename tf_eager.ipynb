{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/xyang2/project/data/dataset/mnist/seed1/labeled_train.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-aee9fc8ac08c>:99: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Processing validation data\n",
      "Processing sample 5000 of 5000\n",
      "Processing train data\n",
      "Processing sample 9244 of 100000"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def _data_path(data_directory:str, name:str) -> str:\n",
    "    \"\"\"Construct a full path to a TFRecord file to be stored in the \n",
    "    data_directory. Will also ensure the data directory exists\n",
    "    \n",
    "    Args:\n",
    "        data_directory: The directory where the records will be stored\n",
    "        name:           The name of the TFRecord\n",
    "    \n",
    "    Returns:\n",
    "        The full path to the TFRecord file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    return os.path.join(data_directory, f'{name}.tfrecords')\n",
    "\n",
    "def _int64_feature(value:int) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a Int64List Feature\n",
    "    \n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "    \n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value:str) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a BytesList Feature\n",
    "    \n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "    \n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def convert_to(data_set, name:str, data_directory:str, num_shards:int=1):\n",
    "    \"\"\"Convert the dataset into TFRecords on disk\n",
    "    \n",
    "    Args:\n",
    "        data_set:       The MNIST data set to convert\n",
    "        name:           The name of the data set\n",
    "        data_directory: The directory where records will be stored\n",
    "        num_shards:     The number of files on disk to separate records into\n",
    "    \"\"\"\n",
    "    print(f'Processing {name} data')\n",
    "\n",
    "    images = data_set.images\n",
    "    labels = data_set.labels\n",
    "    \n",
    "    num_examples, rows, cols, depth = data_set.images.shape\n",
    "\n",
    "    def _process_examples(start_idx:int, end_index:int, filename:str):\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for index in range(start_idx, end_index):\n",
    "                sys.stdout.write(f\"\\rProcessing sample {index+1} of {num_examples}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                image_raw = images[index].tostring()\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'height': _int64_feature(rows),\n",
    "                    'width': _int64_feature(cols),\n",
    "                    'depth': _int64_feature(depth),\n",
    "                    'label': _int64_feature(int(labels[index])),\n",
    "                    'image': _bytes_feature(image_raw)\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())\n",
    "    \n",
    "    if num_shards == 1:\n",
    "        _process_examples(0, data_set.num_examples, _data_path(data_directory, name))\n",
    "    else:\n",
    "        total_examples = data_set.num_examples\n",
    "        samples_per_shard = total_examples // num_shards\n",
    "\n",
    "        for shard in range(num_shards):\n",
    "            start_index = shard * samples_per_shard\n",
    "            end_index = start_index + samples_per_shard\n",
    "            _process_examples(start_index, end_index, _data_path(data_directory, f'{name}-{shard+1}'))\n",
    "\n",
    "    print()\n",
    "\n",
    "def convert_to_tf_record(data_directory:str):\n",
    "    \"\"\"Convert the TF MNIST Dataset to TFRecord formats\n",
    "    \n",
    "    Args:\n",
    "        data_directory: The directory where the TFRecord files should be stored\n",
    "    \"\"\"\n",
    "\n",
    "    mnist = input_data.read_data_sets(\n",
    "        \"/tmp/tensorflow/mnist/input_data\", \n",
    "        reshape=False\n",
    "    )\n",
    "    \n",
    "    convert_to(mnist.validation, 'validation', data_directory)\n",
    "    convert_to(mnist.train, 'train', data_directory, num_shards=10)\n",
    "    convert_to(mnist.test, 'test', data_directory)\n",
    "convert_to_tf_record(\"./mnist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input_fn(filenames, batch_size=1000, shuffle=False):\n",
    "    \n",
    "    def _parser(record):\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        parsed_record = tf.parse_single_example(record, features)\n",
    "        image = tf.decode_raw(parsed_record['image'], tf.float32)\n",
    "\n",
    "        label = tf.cast(parsed_record['label'], tf.int32)\n",
    "\n",
    "        return image, tf.one_hot(label, depth=10)\n",
    "    \n",
    "    def _iter():\n",
    "        dataset = (tf.data.TFRecordDataset(filenames)\n",
    "            .map(_parser))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10_000)\n",
    "\n",
    "        dataset = dataset.repeat(None) # Infinite iterations: let experiment determine num_epochs\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator\n",
    "    \n",
    "    def _input_fn():        \n",
    "        iterator = _iter()\n",
    "        features, labels = iterator.get_next()\n",
    "        \n",
    "        return features, labels\n",
    "    return _iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = data_input_fn(\"./mnist/train-1.tfrecords\", batch_size=100)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(x, dim, is_training=True, update_batch_stats=True, collections=None, name=\"bn\"):\n",
    "    params_shape = (dim,)\n",
    "    n = tf.to_float(tf.reduce_prod(tf.shape(x)[:-1]))\n",
    "    axis = list(range(int(tf.shape(x).get_shape().as_list()[0]) - 1))\n",
    "    mean = tf.reduce_mean(x, axis)\n",
    "    var = tf.reduce_mean(tf.pow(x - mean, 2.0), axis)\n",
    "    avg_mean = tf.get_variable(\n",
    "        name=name + \"_mean\",\n",
    "        shape=params_shape,\n",
    "        initializer=tf.constant_initializer(0.0),\n",
    "        collections=collections,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    avg_var = tf.get_variable(\n",
    "        name=name + \"_var\",\n",
    "        shape=params_shape,\n",
    "        initializer=tf.constant_initializer(1.0),\n",
    "        collections=collections,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    gamma = tf.get_variable(\n",
    "        name=name + \"_gamma\",\n",
    "        shape=params_shape,\n",
    "        initializer=tf.constant_initializer(1.0),\n",
    "        collections=collections\n",
    "    )\n",
    "\n",
    "    beta = tf.get_variable(\n",
    "        name=name + \"_beta\",\n",
    "        shape=params_shape,\n",
    "        initializer=tf.constant_initializer(0.0),\n",
    "        collections=collections,\n",
    "    )\n",
    "\n",
    "    if is_training:\n",
    "        avg_mean_assign_op = tf.no_op()\n",
    "        avg_var_assign_op = tf.no_op()\n",
    "        if update_batch_stats:\n",
    "            avg_mean_assign_op = tf.assign(\n",
    "                avg_mean,\n",
    "                FLAGS.bn_stats_decay_factor * avg_mean + (1 - FLAGS.bn_stats_decay_factor) * mean)\n",
    "            avg_var_assign_op = tf.assign(\n",
    "                avg_var,\n",
    "                FLAGS.bn_stats_decay_factor * avg_var + (n / (n - 1))\n",
    "                * (1 - FLAGS.bn_stats_decay_factor) * var)\n",
    "\n",
    "        with tf.control_dependencies([avg_mean_assign_op, avg_var_assign_op]):\n",
    "            z = (x - mean) / tf.sqrt(1e-6 + var)\n",
    "    else:\n",
    "        z = (x - avg_mean) / tf.sqrt(1e-6 + avg_var)\n",
    "\n",
    "    return gamma * z + beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \"\"\"MLP\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.num_classes = 10\n",
    "        self.var_list = []\n",
    "        self.init_ops = None\n",
    "        self.activation = \"relu\" # config.activation\n",
    "        self.num_units_fc_layers = [1200, 1200] # config.num_units_fc_layers\n",
    "        self.batch_norm = True\n",
    "\n",
    "    def __call__(self, images, is_training=False):\n",
    "        \"\"\"Builds model.\"\"\"\n",
    "        endpoints = {}\n",
    "        net = images\n",
    "        print(net.shape)\n",
    "        reuse = tf.AUTO_REUSE\n",
    "\n",
    "        net = tf.layers.flatten(net)\n",
    "\n",
    "        for i, num_units in enumerate(self.num_units_fc_layers):\n",
    "            layer_suffix = \"layer%d\" % i\n",
    "            with tf.variable_scope(os.path.join(\"mnist_network\", \"fc_\" + layer_suffix), reuse=reuse):\n",
    "                net = tf.layers.dense(\n",
    "                    net,\n",
    "                    num_units,\n",
    "                    activation=self.activation,\n",
    "                    use_bias=True)\n",
    "\n",
    "            endpoints[\"fc_\" + layer_suffix] = net\n",
    "\n",
    "        with tf.variable_scope(os.path.join(\"mnist_network\", \"output_layer\"), reuse=reuse):\n",
    "            logits = tf.layers.dense(\n",
    "                net,\n",
    "                self.num_classes,\n",
    "                activation=None)\n",
    "        endpoints[\"logits\"] = net\n",
    "\n",
    "        return logits, endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "def softmax_model(image_batch):\n",
    "    model = MLP(None)\n",
    "    logits, endpoints = model(image_batch, is_training=True)\n",
    "    model_output = tf.nn.softmax(logits)\n",
    "    return model_output\n",
    "\n",
    "\n",
    "def cross_entropy(model_output, label_batch):\n",
    "    loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(label_batch * tf.log(model_output),\n",
    "        reduction_indices=[1]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tfe.implicit_value_and_gradients\n",
    "def cal_gradient(image_batch, label_batch):\n",
    "    return cross_entropy(softmax_model(image_batch), label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(None)\n",
    "logits, endpoints = model(image_batch, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.18038413 0.22659564 0.         ... 0.         0.         0.3759577 ]\n",
      " [0.36179423 0.36371958 0.         ... 0.         0.         0.        ]\n",
      " [0.20473713 0.3723094  0.         ... 0.         0.1492198  0.        ]\n",
      " ...\n",
      " [0.25645143 0.6003772  0.         ... 0.         0.13306507 0.        ]\n",
      " [0.01401306 0.30898386 0.         ... 0.         0.         0.        ]\n",
      " [0.05506128 0.23868443 0.00085331 ... 0.         0.         0.29849178]], shape=(100, 1200), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.06070615 0.04257288 0.05172792 ... 0.29537678 0.         0.1104888 ]\n",
      " [0.         0.         0.25382587 ... 0.         0.14778559 0.        ]\n",
      " [0.         0.04479885 0.07401633 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.16355065 0.         0.         ... 0.26173037 0.         0.        ]\n",
      " [0.02450862 0.         0.         ... 0.05797724 0.         0.04237015]\n",
      " [0.         0.10344929 0.         ... 0.4595394  0.00108536 0.09780887]], shape=(100, 1200), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.06070615 0.04257288 0.05172792 ... 0.29537678 0.         0.1104888 ]\n",
      " [0.         0.         0.25382587 ... 0.         0.14778559 0.        ]\n",
      " [0.         0.04479885 0.07401633 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.16355065 0.         0.         ... 0.26173037 0.         0.        ]\n",
      " [0.02450862 0.         0.         ... 0.05797724 0.         0.04237015]\n",
      " [0.         0.10344929 0.         ... 0.4595394  0.00108536 0.09780887]], shape=(100, 1200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for layer in layers_list:\n",
    "    print(layer)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer flatten_9 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [1200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-902972ac91e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         difference_prob_grad = [\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m<ipython-input-18-902972ac91e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         difference_prob_grad = [\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m   \"\"\"\n\u001b[1;32m    315\u001b[0m   \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                            \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m                            \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                            str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m   1441\u001b[0m       \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer flatten_9 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [1200]"
     ]
    }
   ],
   "source": [
    "model = MLP(None)\n",
    "\n",
    "one_hot_labels = label_batch\n",
    "top_k = 1\n",
    "with tf.GradientTape() as t:\n",
    "    logits, endpoints = model(image_batch, is_training=True)\n",
    "    layers_list = [endpoints[e] for e in endpoints]\n",
    "    class_prob = tf.nn.softmax(logits)\n",
    "    # Pick the correct class probability.\n",
    "    correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "    # Class probabilities except the correct.\n",
    "    other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "    if top_k > 1:\n",
    "        # Pick the top k class probabilities other than the correct.\n",
    "        top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "    else:\n",
    "        top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "    # Difference between correct class probailities and top_k probabilities.\n",
    "    difference_prob = correct_class_prob - top_k_class_prob\n",
    "    losses_list = []\n",
    "    for layer in layers_list:\n",
    "        difference_prob_grad = [\n",
    "            tf.layers.flatten(t.gradient(difference_prob[:, i], layer)[0])\n",
    "            for i in range(top_k)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-803edc17e53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlarge_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-e9ce10671bfc>\u001b[0m in \u001b[0;36mlarge_margin\u001b[0;34m(_sentinel, logits, one_hot_labels, layers_list, gamma, alpha_factor, top_k, dist_norm, epsilon, use_approximation, loss_type, loss_collection)\u001b[0m\n\u001b[1;32m     90\u001b[0m             difference_prob_grad = [\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             ]\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e9ce10671bfc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m             difference_prob_grad = [\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             ]\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 596\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    611\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    612\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
     ]
    }
   ],
   "source": [
    "large_margin(logits=logits, one_hot_labels=label_batch, layers_list=endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_norm_fn(norm_type):\n",
    "    norm_fn = lambda x: tf.norm(x, ord=norm_type)\n",
    "    return norm_fn\n",
    "\n",
    "\n",
    "def maximum_with_relu(a, b):\n",
    "    return a + tf.nn.relu(b - a)\n",
    "\n",
    "\n",
    "def _ensure_large_margin_args(name, sentinel, one_hot_labels, logits, layers_list, dist_norm, loss_type):\n",
    "    \"\"\"Ensures arguments are correct.\"\"\"\n",
    "    # Make sure that all arguments were passed as named arguments.\n",
    "    if sentinel is not None:\n",
    "        raise ValueError(\n",
    "            \"Only call `%s` with \"\n",
    "            \"named arguments (one_hot_labels=..., logits=..., ...)\" % name)\n",
    "    if one_hot_labels is None or logits is None or not layers_list:\n",
    "        raise ValueError(\"logits, one_hot_labels and layers_list must be provided.\")\n",
    "\n",
    "    if dist_norm not in {1, 2, np.inf}:\n",
    "        raise ValueError(\"dist_norm must be 1, 2, or np.inf.\")\n",
    "\n",
    "    if loss_type not in {\"all_top_k\", \"average_top_k\", \"worst_top_k\"}:\n",
    "        raise ValueError(\n",
    "            \"loss_type must be 'all_top_k', 'average_top_k', or 'worst_top_k'.\")\n",
    "\n",
    "\n",
    "# pylint: disable=invalid-name\n",
    "def large_margin(_sentinel=None, logits=None, one_hot_labels=None, layers_list=None, gamma=10000, alpha_factor=2, top_k=1, dist_norm=2,\n",
    "                 epsilon=1e-8, use_approximation=True, loss_type=\"all_top_k\", loss_collection=tf.GraphKeys.LOSSES):\n",
    "    \"\"\"Creates a large margin loss.\n",
    "\n",
    "    Args:\n",
    "        _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
    "        logits: Float `[batch_size, num_classes]` logits outputs of the network.\n",
    "        one_hot_labels: `[batch_size, num_classes]` Target integer labels in `{0,\n",
    "            1}`.\n",
    "        layers_list: List of network Tensors at different layers. The large margin\n",
    "            is enforced at the layers specified.\n",
    "        gamma: Desired margin, and distance to boundary above the margin will be\n",
    "            clipped.\n",
    "        alpha_factor: Factor to determine the lower bound of margin. Both gamma and\n",
    "            alpha_factor determine points to include in training the margin these\n",
    "            points lie with distance to boundary of [gamma * (1 - alpha), gamma]\n",
    "        top_k: Number of top classes to include in the margin loss.\n",
    "        dist_norm: Distance to boundary defined on norm (options: be 1, 2, np.inf).\n",
    "        epsilon: Small number to avoid division by 0.\n",
    "        use_approximation: If true, use approximation of the margin gradient for\n",
    "            less computationally expensive training.\n",
    "        loss_type: 'worst_top_k', 'average_top_k', or 'all_top_k'. If 'worst_top_k'\n",
    "            only consider the minimum distance to boundary of the top_k classes. If\n",
    "            'average_top_k' consider average distance to boundary. If 'all_top_k'\n",
    "            consider all top_k. When top_k = 1, these choices are equivalent.\n",
    "        loss_collection: Collection to which the loss will be added.\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar `Tensor` of the same type as `logits`.\n",
    "    Raises:\n",
    "        ValueError: If the shape of `logits` doesn't match that of\n",
    "            `one_hot_labels`.    Also if `one_hot_labels` or `logits` is None.\n",
    "    \"\"\"\n",
    "\n",
    "    _ensure_large_margin_args(\"large_margin\", _sentinel, one_hot_labels, logits, layers_list, dist_norm, loss_type)\n",
    "    logits = tf.convert_to_tensor(logits)\n",
    "    one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n",
    "    logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n",
    "    assert top_k > 0\n",
    "    assert top_k <= logits.get_shape()[1]\n",
    "\n",
    "    dual_norm = {1: np.inf, 2: 2, np.inf: 1}\n",
    "    norm_fn = get_norm_fn(dual_norm[dist_norm])\n",
    "    with tf.name_scope(\"large_margin_loss\"):\n",
    "        class_prob = tf.nn.softmax(logits)\n",
    "        # Pick the correct class probability.\n",
    "        correct_class_prob = tf.reduce_sum(class_prob * one_hot_labels, axis=1, keepdims=True)\n",
    "\n",
    "        # Class probabilities except the correct.\n",
    "        other_class_prob = class_prob * (1. - one_hot_labels)\n",
    "        if top_k > 1:\n",
    "            # Pick the top k class probabilities other than the correct.\n",
    "            top_k_class_prob, _ = tf.nn.top_k(other_class_prob, k=top_k)\n",
    "        else:\n",
    "            top_k_class_prob = tf.reduce_max(other_class_prob, axis=1, keepdims=True)\n",
    "\n",
    "        # Difference between correct class probailities and top_k probabilities.\n",
    "        difference_prob = correct_class_prob - top_k_class_prob\n",
    "        losses_list = []\n",
    "        for layer in layers_list:\n",
    "            difference_prob_grad = [\n",
    "                tf.layers.flatten(tf.gradients(difference_prob[:, i], layer)[0])\n",
    "                for i in range(top_k)\n",
    "            ]\n",
    "\n",
    "            difference_prob_gradnorm = tf.concat([\n",
    "                tf.map_fn(norm_fn, difference_prob_grad[i])[:, tf.newaxis]\n",
    "                for i in range(top_k)\n",
    "            ], axis=1)\n",
    "\n",
    "            if use_approximation:\n",
    "                difference_prob_gradnorm = tf.stop_gradient(difference_prob_gradnorm)\n",
    "\n",
    "            distance_to_boundary = difference_prob / (difference_prob_gradnorm + epsilon)\n",
    "\n",
    "            if loss_type == \"worst_top_k\":\n",
    "                # Only consider worst distance to boundary.\n",
    "                distance_to_boundary = tf.reduce_min(distance_to_boundary, axis=1)\n",
    "\n",
    "            elif loss_type == \"average_top_k\":\n",
    "                # Only consider average distance to boundary.\n",
    "                distance_to_boundary = tf.reduce_mean(distance_to_boundary, axis=1)\n",
    "\n",
    "            # Distances to consider between distance_upper and distance_lower bounds\n",
    "            distance_upper = gamma\n",
    "            distance_lower = gamma * (1 - alpha_factor)\n",
    "\n",
    "            # Enforce lower bound.\n",
    "            loss_layer = maximum_with_relu(distance_to_boundary, distance_lower)\n",
    "\n",
    "            # Enforce upper bound.\n",
    "            loss_layer = maximum_with_relu(\n",
    "                0, distance_upper - loss_layer) - distance_upper\n",
    "\n",
    "            losses_list.append(tf.reduce_mean(loss_layer))\n",
    "\n",
    "        loss = tf.reduce_mean(losses_list)\n",
    "        # Add loss to loss_collection.\n",
    "        tf.losses.add_loss(loss, loss_collection)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/xyang2/project/data/dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xyang2/software/dllib3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/xyang2/project/data/dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "(100, 784)\n",
      "(100, 784)\n",
      "(10000, 784)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e480e7e6bc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_test_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel_test_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_test_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_test_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a4f77f724512>\u001b[0m in \u001b[0;36msoftmax_model\u001b[0;34m(image_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b69d9b734326>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, is_training)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_units_fc_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m   \"\"\"\n\u001b[1;32m    315\u001b[0m   \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/dllib3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m           spec.max_ndim is not None):\n\u001b[0;32m-> 1410\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m   1412\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = input_data.read_data_sets(\"/home/xyang2/project/data/dataset/mnist/\", one_hot=True)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels))\\\n",
    "    .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "    .shuffle(buffer_size=1000)\\\n",
    "    .batch(100)\\\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_ds)):\n",
    "    print(image_batch.shape)\n",
    "    loss, grads_and_vars = cal_gradient(image_batch, label_batch)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    if step % 100:\n",
    "        print(\"step: {}  loss: {}\".format(step, loss.numpy()))\n",
    "    break\n",
    "\n",
    "model_test_output = softmax_model(data.test.images)\n",
    "model_test_label = data.test.labels\n",
    "correct_prediction = tf.equal(tf.argmax(model_test_output, 1), tf.argmax(model_test_label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"test accuracy = {}\".format(accuracy.numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
