{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data by tensorflow\n",
    "\n",
    "import torch and tensorflow\n",
    "\n",
    "set memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu = \"\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "\n",
    "tf.enable_eager_execution(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tf_func import data_loader\n",
    "from tf_func import mnist_model\n",
    "\n",
    "\n",
    "class ConfigDict(object):\n",
    "    \"\"\"MNIST configration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # List of tuples specify (kernel_size, number of filters) for each layer.\n",
    "        self.filter_sizes_conv_layers = [(5, 32), (5, 64)]\n",
    "        # Dictionary of pooling type (\"max\"/\"average\", size and stride).\n",
    "        self.pool_params = {\"type\": \"max\", \"size\": 2, \"stride\": 2}\n",
    "        self.num_units_fc_layers = [512]\n",
    "        self.dropout_rate = 0\n",
    "        self.batch_norm = True\n",
    "        self.activation = None\n",
    "        self.regularizer = None\n",
    "        \n",
    "        \n",
    "config = ConfigDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"train\",\n",
    "    batch_size=128,\n",
    "    is_training=False)\n",
    "\n",
    "test_dataset = data_loader.MNIST(\n",
    "    data_dir=\"./data/mnist\",\n",
    "    subset=\"test\",\n",
    "    batch_size=128,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12790.145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels, num_examples, num_classes = (dataset.images, dataset.labels, dataset.num_examples, dataset.num_classes)\n",
    "images, labels = dataset.get_next()\n",
    "images.numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow (eager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_tf(shape):\n",
    "    fan_in = 0\n",
    "    if len(shape) == 4:\n",
    "        fan_in = shape[1] * shape[2] * shape[3]\n",
    "    if len(shape) == 2:\n",
    "        fan_in = shape[1]\n",
    "    if fan_in:\n",
    "        s = 1.0 * np.sqrt(6.0 / fan_in)\n",
    "        transpose = np.random.uniform(-s, s, shape).astype(\"float32\")\n",
    "    if len(shape) == 2:\n",
    "        transpose = transpose.T\n",
    "    if len(shape) == 4:\n",
    "        transpose = np.transpose(transpose, axes=(2, 3, 1, 0))\n",
    "    if debug:\n",
    "        print(shape, transpose.sum())\n",
    "    return transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_iter):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    for images, labels in test_iter:\n",
    "        size += images.numpy().shape[0]\n",
    "        logits, _ = model(images, is_training=False)\n",
    "        total_loss += tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)).numpy()\n",
    "        top1_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "        total_acc += tf.reduce_sum(tf.cast(top1_op, dtype=tf.float32)).numpy()\n",
    "    return total_acc / size, total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "def pool2d_layer(inputs, pool_type, pool_size=2, pool_stride=2):\n",
    "    \"\"\"Pooling layer.\n",
    "\n",
    "    Args:\n",
    "      inputs: Tensor of size [batch, H, W, channels].\n",
    "      pool_type: String (\"max\", or \"average\"), specifying pooling type.\n",
    "      pool_size: Integer > 1 pooling size.\n",
    "      pool_stride: Integer > 1 pooling stride.\n",
    "\n",
    "    Returns:\n",
    "      Pooling result.\n",
    "    \"\"\"\n",
    "    if pool_type == \"max\":\n",
    "        # Max pooling layer\n",
    "        return tf.layers.max_pooling2d(\n",
    "            inputs, pool_size=[pool_size] * 2, strides=pool_stride)\n",
    "\n",
    "    elif pool_type == \"average\":\n",
    "        # Average pooling layer\n",
    "        return tf.layers.average_pooling2d(\n",
    "            inputs, pool_size=[pool_size] * 2, strides=pool_stride)\n",
    "    \n",
    "\n",
    "class MNISTNetwork(tf.keras.Model):\n",
    "    \"\"\"MNIST model. \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MNISTNetwork, self).__init__()\n",
    "        self.num_classes = config.num_classes\n",
    "        self.var_list = []\n",
    "        self.init_ops = None\n",
    "        self.regularizer = config.regularizer\n",
    "        self.activation = config.activation\n",
    "        self.filter_sizes_conv_layers = config.filter_sizes_conv_layers\n",
    "        self.num_units_fc_layers = config.num_units_fc_layers\n",
    "        self.pool_params = config.pool_params\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.batch_norm = config.batch_norm\n",
    "        self.conv_layers = []\n",
    "        self.bn_layers = []\n",
    "        self.drop_layers = []\n",
    "        in_channel = 1\n",
    "        for i, filter_size in enumerate(self.filter_sizes_conv_layers):\n",
    "            f_size = filter_size[0]\n",
    "            conv_layer = tf.layers.Conv2D(kernel_size=filter_size[0], filters=filter_size[1], \n",
    "                                          strides=(1, 1), padding=\"same\",\n",
    "                                          use_bias=not self.batch_norm,\n",
    "                                          kernel_initializer=tf.constant_initializer(\n",
    "                                              (weight_init_tf((filter_size[1], in_channel, f_size, f_size)))))\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            \n",
    "            batch_norm_layer = tf.layers.BatchNormalization(momentum=MOMENTUM, epsilon=EPS)\n",
    "            self.bn_layers.append(batch_norm_layer)\n",
    "            in_channel = filter_size[1]\n",
    "            if self.dropout_rate > 0:\n",
    "                drop_layer = tf.layers.Dropout(self.dropout_rate)\n",
    "                self.drop_layers.append(drop_layer)\n",
    "            \n",
    "        self.fc_layers = []\n",
    "        in_shape = 64 * 7 * 7\n",
    "        for i, num_units in enumerate(self.num_units_fc_layers):\n",
    "            fc_layer = tf.layers.Dense(num_units,\n",
    "                                       kernel_initializer=tf.constant_initializer((weight_init_tf((num_units, in_shape)))),)\n",
    "            self.fc_layers.append(fc_layer)\n",
    "            in_shape = num_units\n",
    "            if self.dropout_rate > 0:\n",
    "                self.drop_layers.append(tf.layers.Dropout(0.1))\n",
    "        self.output_layer = tf.layers.Dense(self.num_classes, activation=None,\n",
    "                                            kernel_initializer=tf.constant_initializer((weight_init_tf((self.num_classes, in_shape)))),)\n",
    "\n",
    "    def __call__(self, images, is_training=False):\n",
    "        \"\"\"Builds model.\"\"\"\n",
    "        endpoints = {}\n",
    "        net = images\n",
    "        for i in range(len(self.filter_sizes_conv_layers)):\n",
    "            layer_suffix = \"layer%d\" % i\n",
    "            net = self.conv_layers[i](net)\n",
    "            net = tf.nn.relu(net)\n",
    "            if self.pool_params:\n",
    "                net = pool2d_layer(net, pool_type=self.pool_params[\"type\"], pool_size=self.pool_params[\"size\"]\n",
    "                                   , pool_stride=self.pool_params[\"stride\"])\n",
    "            if debug: print(\"after pool\", \"%.4f\" % net.numpy().sum())\n",
    "            if self.dropout_rate > 0:\n",
    "                net = tf.layers.dropout(net, rate=self.dropout_rate, training=is_training)\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                if debug:\n",
    "                    print(\"before batech norm %.4f\" % (net.numpy() ** 2).sum())\n",
    "                # net = tf.layers.batch_normalization(net, training=is_training, epsilon=EPS, momentum=MOMENTUM)\n",
    "                net = self.bn_layers[i](net, training=is_training)\n",
    "                if debug:\n",
    "                    print(\"after batech norm %.4f\" % (net.numpy() ** 2).sum())\n",
    "\n",
    "            endpoints[\"conv_\" + layer_suffix] = net\n",
    "        if debug:\n",
    "            print(\"After two conv %.4f\" % (net ** 2).numpy().sum())\n",
    "        net = tf.layers.flatten(net)\n",
    "\n",
    "        for i in range(len(self.num_units_fc_layers)):\n",
    "            layer_suffix = \"layer%d\" % i\n",
    "            net = self.fc_layers[i](net)\n",
    "            net = tf.nn.relu(net)\n",
    "            endpoints[\"fc_\" + layer_suffix] = net\n",
    "\n",
    "        logits = self.output_layer(net)\n",
    "        endpoints[\"logits\"] = logits\n",
    "        if is_training and debug:\n",
    "            print(\"logits %.4f\" % (logits.numpy() ** 2).sum())\n",
    "        return logits, endpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "It works fine for TF and PyTorch\n",
    "\n",
    "without BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf, cross entropy, without batch norm\n",
      "(32, 1, 5, 5) 4.01563\n",
      "(64, 32, 5, 5) -4.576685\n",
      "(512, 3136) -19.584167\n",
      "(10, 512) 1.3087051\n",
      "data 12790.14453\n",
      "iter 0, train loss 2.61532\n",
      "\n",
      "data 12953.98438\n",
      "iter 1, train loss 2.45566\n",
      "\n",
      "data 12442.99316\n",
      "iter 2, train loss 2.16393\n",
      "\n",
      "data 12292.56152\n",
      "iter 3, train loss 1.94132\n",
      "\n",
      "data 12402.40137\n",
      "iter 4, train loss 1.68689\n",
      "\n",
      "data 13600.25098\n",
      "iter 5, train loss 1.51943\n",
      "\n",
      "data 13450.91211\n",
      "iter 6, train loss 1.19969\n",
      "\n",
      "data 12739.25195\n",
      "iter 7, train loss 1.10128\n",
      "\n",
      "data 11608.33594\n",
      "iter 8, train loss 1.11012\n",
      "\n",
      "data 14801.60449\n",
      "iter 9, train loss 0.83975\n",
      "\n",
      "data 15733.70703\n",
      "iter 10, train loss 0.82863\n",
      "\n",
      "data 12621.97754\n",
      "iter 11, train loss 0.66821\n",
      "\n",
      "data 12366.39551\n",
      "iter 12, train loss 0.54460\n",
      "\n",
      "data 12358.09375\n",
      "iter 13, train loss 0.44114\n",
      "\n",
      "data 12827.04590\n",
      "iter 14, train loss 0.43033\n",
      "\n",
      "data 14613.22363\n",
      "iter 15, train loss 0.49218\n",
      "\n",
      "data 13533.24316\n",
      "iter 16, train loss 0.40717\n",
      "\n",
      "data 13346.56543\n",
      "iter 17, train loss 0.39431\n",
      "\n",
      "data 12810.09473\n",
      "iter 18, train loss 0.48483\n",
      "\n",
      "data 13308.19238\n",
      "iter 19, train loss 0.29288\n",
      "\n",
      "data 13648.76465\n",
      "iter 20, train loss 0.48885\n",
      "\n",
      "data 12919.14941\n",
      "iter 21, train loss 0.47191\n",
      "\n",
      "data 13686.84766\n",
      "iter 22, train loss 0.35254\n",
      "\n",
      "data 12311.84375\n",
      "iter 23, train loss 0.39079\n",
      "\n",
      "data 12001.96875\n",
      "iter 24, train loss 0.21570\n",
      "\n",
      "data 11975.05176\n",
      "iter 25, train loss 0.25628\n",
      "\n",
      "data 12390.50586\n",
      "iter 26, train loss 0.30685\n",
      "\n",
      "data 12357.15332\n",
      "iter 27, train loss 0.34734\n",
      "\n",
      "data 14262.87109\n",
      "iter 28, train loss 0.27275\n",
      "\n",
      "data 12710.52051\n",
      "iter 29, train loss 0.34521\n",
      "\n",
      "test acc 0.89303,  loss 0.36152\n"
     ]
    }
   ],
   "source": [
    "print(\"tf, cross entropy, without batch norm\")\n",
    "debug = True\n",
    "config.batch_norm = False\n",
    "config.dropout_rate = 0\n",
    "np.random.seed(1)\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "debug = False\n",
    "max_iters = 30\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "optimizer = tf.train.MomentumOptimizer(lr, momentum=momentum, use_nesterov=True)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    with tf.GradientTape(persistent=True) as tp:\n",
    "        # Build model.\n",
    "        logits, endpoints = model(images, is_training=True)\n",
    "        loss_list = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        xent_loss = tf.reduce_mean(loss_list)\n",
    "        total_loss = xent_loss\n",
    "    grads = tp.gradient(total_loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(\"iter %d, train loss %.5f\\n\" % (i, total_loss))\n",
    "    \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy\n",
    "\n",
    "It works fine for TF and PyTorch\n",
    "\n",
    "with BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single layer batch norm\n",
    "\n",
    "data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 12790.14453\n",
      "first images\n",
      " 107.94118\n",
      "sum of output**2\n",
      " 100341.695\n",
      "sum of output**2\n",
      " 100341.695\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "bn_layer = tf.layers.BatchNormalization(axis=-1, momentum=MOMENTUM, epsilon=EPS)\n",
    "print(\"first images\\n\", images[0].numpy().sum())\n",
    "\n",
    "p = bn_layer(images, training=True)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())\n",
    "\n",
    "p = tf.layers.batch_normalization(images, training=True, momentum=MOMENTUM, epsilon=EPS)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 12790.14453\n",
      "after pool 145856.7031\n",
      "before batech norm 142664.3594\n",
      "after batech norm 802636.1250\n",
      "after pool 404656.3438\n",
      "before batech norm 907941.3125\n",
      "after batech norm 401403.4688\n",
      "After two conv 401403.4688\n",
      "logits 2373.4243\n",
      "-203.36768\n",
      "14\n",
      "[-1253.7639, -39865.3, -0.0032806396, 62.726154, -203.36748, -241.92593, -9245.387, -173.44833, 359090.5, 1280.0]\n",
      "(128, 28, 28, 1) -41.07355\n"
     ]
    }
   ],
   "source": [
    "config.batch_norm = True\n",
    "np.random.seed(1)\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "images, labels = iterator.get_next()\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "debug = True\n",
    "# Build model.\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tp:\n",
    "    tp.watch(images)\n",
    "    logits, endpoints = model(images, is_training=True)\n",
    "    m = tf.reduce_sum(logits)\n",
    "print(logits.numpy().sum())\n",
    "\n",
    "grads = tp.gradient(m, model.variables)\n",
    "print(len(grads))\n",
    "print([e.numpy().sum() for e in grads if e is not None])\n",
    "grads = tp.gradient(m, images)\n",
    "print(grads.numpy().shape, grads.numpy().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy with model contains batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf, cross entropy, batch norm\n",
      "test acc 0.12790,  loss 2.59772\n",
      "data 12790.14453\n",
      "iter 0, train loss 3.03516\n",
      "\n",
      "data 12953.98438\n",
      "iter 1, train loss 2.31661\n",
      "\n",
      "data 12442.99316\n",
      "iter 2, train loss 1.12291\n",
      "\n",
      "data 12292.56152\n",
      "iter 3, train loss 0.81643\n",
      "\n",
      "data 12402.40137\n",
      "iter 4, train loss 0.87677\n",
      "\n",
      "data 13600.25098\n",
      "iter 5, train loss 0.68687\n",
      "\n",
      "data 13450.91211\n",
      "iter 6, train loss 0.56323\n",
      "\n",
      "data 12739.25195\n",
      "iter 7, train loss 0.48896\n",
      "\n",
      "data 11608.33594\n",
      "iter 8, train loss 0.60257\n",
      "\n",
      "data 14801.60449\n",
      "iter 9, train loss 0.37499\n",
      "\n",
      "data 15733.70703\n",
      "iter 10, train loss 0.46338\n",
      "\n",
      "data 12621.97754\n",
      "iter 11, train loss 0.24045\n",
      "\n",
      "data 12366.39551\n",
      "iter 12, train loss 0.26338\n",
      "\n",
      "data 12358.09375\n",
      "iter 13, train loss 0.28679\n",
      "\n",
      "data 12827.04590\n",
      "iter 14, train loss 0.15868\n",
      "\n",
      "data 14613.22363\n",
      "iter 15, train loss 0.26080\n",
      "\n",
      "data 13533.24316\n",
      "iter 16, train loss 0.23541\n",
      "\n",
      "data 13346.56543\n",
      "iter 17, train loss 0.19799\n",
      "\n",
      "data 12810.09473\n",
      "iter 18, train loss 0.30681\n",
      "\n",
      "data 13308.19238\n",
      "iter 19, train loss 0.15206\n",
      "\n",
      "data 13648.76465\n",
      "iter 20, train loss 0.30062\n",
      "\n",
      "data 12919.14941\n",
      "iter 21, train loss 0.26311\n",
      "\n",
      "data 13686.84766\n",
      "iter 22, train loss 0.19258\n",
      "\n",
      "data 12311.84375\n",
      "iter 23, train loss 0.18489\n",
      "\n",
      "data 12001.96875\n",
      "iter 24, train loss 0.12751\n",
      "\n",
      "data 11975.05176\n",
      "iter 25, train loss 0.18268\n",
      "\n",
      "data 12390.50586\n",
      "iter 26, train loss 0.15792\n",
      "\n",
      "data 12357.15332\n",
      "iter 27, train loss 0.22728\n",
      "\n",
      "data 14262.87109\n",
      "iter 28, train loss 0.24531\n",
      "\n",
      "data 12710.52051\n",
      "iter 29, train loss 0.25865\n",
      "\n",
      "test acc 0.93019,  loss 0.22696\n"
     ]
    }
   ],
   "source": [
    "print(\"tf, cross entropy, batch norm\")\n",
    "\n",
    "config.batch_norm = True\n",
    "np.random.seed(1)\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "debug = False\n",
    "max_iters = 30\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "optimizer = tf.train.MomentumOptimizer(lr, momentum=momentum, use_nesterov=True)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    with tf.GradientTape() as tp:\n",
    "        # Build model.\n",
    "        logits, endpoints = model(images, is_training=True)\n",
    "        loss_list = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        xent_loss = tf.reduce_mean(loss_list)\n",
    "        total_loss = xent_loss\n",
    "    grads = tp.gradient(total_loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(\"iter %d, train loss %.5f\\n\" % (i, total_loss))\n",
    "        \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "single dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 12790.14453\n",
      "sum of output**2\n",
      " 21480.93\n",
      "sum of output**2\n",
      " 21480.93\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "drop_layer = tf.layers.Dropout(rate=0.5)\n",
    "\n",
    "p = drop_layer(images, training=True)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "p = tf.layers.dropout(images, rate=0.5, training=True)\n",
    "print(\"sum of output**2\\n\", (p.cpu().numpy() ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf, cross entropy, batch norm\n",
      "test acc 0.12790,  loss 2.59773\n",
      "data 12790.14453\n",
      "iter 0, train loss 3.08468\n",
      "\n",
      "data 12953.98438\n",
      "iter 1, train loss 3.24825\n",
      "\n",
      "data 12442.99316\n",
      "iter 2, train loss 2.94694\n",
      "\n",
      "data 12292.56152\n",
      "iter 3, train loss 2.90814\n",
      "\n",
      "data 12402.40137\n",
      "iter 4, train loss 2.30756\n",
      "\n",
      "data 13600.25098\n",
      "iter 5, train loss 1.95330\n",
      "\n",
      "data 13450.91211\n",
      "iter 6, train loss 1.74396\n",
      "\n",
      "data 12739.25195\n",
      "iter 7, train loss 1.60965\n",
      "\n",
      "data 11608.33594\n",
      "iter 8, train loss 1.65559\n",
      "\n",
      "data 14801.60449\n",
      "iter 9, train loss 1.57181\n",
      "\n",
      "data 15733.70703\n",
      "iter 10, train loss 1.53753\n",
      "\n",
      "data 12621.97754\n",
      "iter 11, train loss 1.34168\n",
      "\n",
      "data 12366.39551\n",
      "iter 12, train loss 1.17563\n",
      "\n",
      "data 12358.09375\n",
      "iter 13, train loss 0.94621\n",
      "\n",
      "data 12827.04590\n",
      "iter 14, train loss 0.92047\n",
      "\n",
      "data 14613.22363\n",
      "iter 15, train loss 0.99282\n",
      "\n",
      "data 13533.24316\n",
      "iter 16, train loss 0.75663\n",
      "\n",
      "data 13346.56543\n",
      "iter 17, train loss 0.72970\n",
      "\n",
      "data 12810.09473\n",
      "iter 18, train loss 0.75371\n",
      "\n",
      "data 13308.19238\n",
      "iter 19, train loss 0.64677\n",
      "\n",
      "data 13648.76465\n",
      "iter 20, train loss 0.69303\n",
      "\n",
      "data 12919.14941\n",
      "iter 21, train loss 0.71901\n",
      "\n",
      "data 13686.84766\n",
      "iter 22, train loss 0.60080\n",
      "\n",
      "data 12311.84375\n",
      "iter 23, train loss 0.55873\n",
      "\n",
      "data 12001.96875\n",
      "iter 24, train loss 0.44615\n",
      "\n",
      "data 11975.05176\n",
      "iter 25, train loss 0.49760\n",
      "\n",
      "data 12390.50586\n",
      "iter 26, train loss 0.45117\n",
      "\n",
      "data 12357.15332\n",
      "iter 27, train loss 0.60835\n",
      "\n",
      "data 14262.87109\n",
      "iter 28, train loss 0.34800\n",
      "\n",
      "data 12710.52051\n",
      "iter 29, train loss 0.54277\n",
      "\n",
      "test acc 0.87410,  loss 0.42123\n"
     ]
    }
   ],
   "source": [
    "print(\"tf, cross entropy, batch norm\")\n",
    "\n",
    "config.batch_norm = False\n",
    "config.dropout_rate = 0.3\n",
    "np.random.seed(1)\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "debug = False\n",
    "max_iters = 30\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "optimizer = tf.train.MomentumOptimizer(lr, momentum=momentum, use_nesterov=True)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    with tf.GradientTape() as tp:\n",
    "        # Build model.\n",
    "        logits, endpoints = model(images, is_training=True)\n",
    "        loss_list = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        xent_loss = tf.reduce_mean(loss_list)\n",
    "        total_loss = xent_loss\n",
    "    grads = tp.gradient(total_loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(\"iter %d, train loss %.5f\\n\" % (i, total_loss))\n",
    "        \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf, cross entropy, batch norm\n",
      "test acc 0.12790,  loss 2.59772\n",
      "data 12790.14453\n",
      "iter 0, train loss 3.23462\n",
      "\n",
      "data 12953.98438\n",
      "iter 1, train loss 2.21660\n",
      "\n",
      "data 12442.99316\n",
      "iter 2, train loss 2.09295\n",
      "\n",
      "data 12292.56152\n",
      "iter 3, train loss 1.86393\n",
      "\n",
      "data 12402.40137\n",
      "iter 4, train loss 1.70674\n",
      "\n",
      "data 13600.25098\n",
      "iter 5, train loss 1.34170\n",
      "\n",
      "data 13450.91211\n",
      "iter 6, train loss 1.11725\n",
      "\n",
      "data 12739.25195\n",
      "iter 7, train loss 1.02028\n",
      "\n",
      "data 11608.33594\n",
      "iter 8, train loss 1.07337\n",
      "\n",
      "data 14801.60449\n",
      "iter 9, train loss 0.92933\n",
      "\n",
      "data 15733.70703\n",
      "iter 10, train loss 0.88584\n",
      "\n",
      "data 12621.97754\n",
      "iter 11, train loss 0.68412\n",
      "\n",
      "data 12366.39551\n",
      "iter 12, train loss 0.54143\n",
      "\n",
      "data 12358.09375\n",
      "iter 13, train loss 0.47160\n",
      "\n",
      "data 12827.04590\n",
      "iter 14, train loss 0.40853\n",
      "\n",
      "data 14613.22363\n",
      "iter 15, train loss 0.52695\n",
      "\n",
      "data 13533.24316\n",
      "iter 16, train loss 0.33151\n",
      "\n",
      "data 13346.56543\n",
      "iter 17, train loss 0.48184\n",
      "\n",
      "data 12810.09473\n",
      "iter 18, train loss 0.53470\n",
      "\n",
      "data 13308.19238\n",
      "iter 19, train loss 0.36797\n",
      "\n",
      "data 13648.76465\n",
      "iter 20, train loss 0.45924\n",
      "\n",
      "data 12919.14941\n",
      "iter 21, train loss 0.43735\n",
      "\n",
      "data 13686.84766\n",
      "iter 22, train loss 0.39355\n",
      "\n",
      "data 12311.84375\n",
      "iter 23, train loss 0.37789\n",
      "\n",
      "data 12001.96875\n",
      "iter 24, train loss 0.37591\n",
      "\n",
      "data 11975.05176\n",
      "iter 25, train loss 0.41594\n",
      "\n",
      "data 12390.50586\n",
      "iter 26, train loss 0.40256\n",
      "\n",
      "data 12357.15332\n",
      "iter 27, train loss 0.35776\n",
      "\n",
      "data 14262.87109\n",
      "iter 28, train loss 0.36261\n",
      "\n",
      "data 12710.52051\n",
      "iter 29, train loss 0.40711\n",
      "\n",
      "test acc 0.88512,  loss 0.36535\n"
     ]
    }
   ],
   "source": [
    "print(\"tf, cross entropy, batch norm\")\n",
    "\n",
    "config.batch_norm = True\n",
    "config.dropout_rate = 0.7\n",
    "np.random.seed(1)\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "debug = False\n",
    "max_iters = 30\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))\n",
    "\n",
    "iterator = dataset.dataset.make_one_shot_iterator()\n",
    "optimizer = tf.train.MomentumOptimizer(lr, momentum=momentum, use_nesterov=True)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    images, labels = iterator.get_next()\n",
    "    print(\"data %.5f\" % images.numpy().sum())\n",
    "    with tf.GradientTape() as tp:\n",
    "        # Build model.\n",
    "        logits, endpoints = model(images, is_training=True)\n",
    "        loss_list = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        xent_loss = tf.reduce_mean(loss_list)\n",
    "        total_loss = xent_loss\n",
    "    grads = tp.gradient(total_loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(\"iter %d, train loss %.5f\\n\" % (i, total_loss))\n",
    "        \n",
    "debug = False\n",
    "acc, loss = evaluate_classifier(model, test_dataset.dataset.make_one_shot_iterator())\n",
    "print(\"test acc %.5f,  loss %.5f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bug in TF 0.12.1 and 0.10.1\n",
    "\n",
    "[issue report](https://github.com/tensorflow/tensorflow/issues/27043)\n",
    "\n",
    "In Eager mode, batch norm doesn't support Second order derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 1182.03564\n",
      "-5.686387\n",
      "first -66.77962\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cea52213d76a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdp_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp_dx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0md2y_dx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_dx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"second order\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2y_dx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormGradGrad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    938\u001b[0m   \u001b[0mgrad_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrad_grad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_grad_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_grad_offset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m   grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\n\u001b[0;32m--> 940\u001b[0;31m       [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\n\u001b[0m\u001b[1;32m    941\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_grad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 630\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/dllib3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    645\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    646\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
     ]
    }
   ],
   "source": [
    "config = ConfigDict()\n",
    "# enable/disable batch norm\n",
    "config.batch_norm = True\n",
    "\n",
    "model = MNISTNetwork(config)\n",
    "\n",
    "# Build model.\n",
    "\n",
    "images = np.random.uniform(0, 1, (3, 28, 28, 1))\n",
    "images = tf.convert_to_tensor(images, dtype=np.float32)\n",
    "# images = tf.Variable(images)\n",
    "print(\"data %.5f\" % images.numpy().sum())\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    with tf.GradientTape(persistent=True) as t2:\n",
    "        logits = model(images, is_training=True)\n",
    "        m = tf.reduce_sum(logits)\n",
    "        print(logits.numpy().sum())\n",
    "        dp_dx = t2.gradient(m, model.variables)\n",
    "    print(\"first\", dp_dx[0].numpy().sum())\n",
    "    d2y_dx2 = t.gradient(dp_dx[0], model.variables)\n",
    "    print(\"second order\", d2y_dx2[0].numpy().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
